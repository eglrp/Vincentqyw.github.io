<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Vincent Qin</title>
  
  <subtitle>Keep Your Curiosity</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.vincentqin.tech/"/>
  <updated>2018-06-19T12:34:46.043Z</updated>
  <id>https://www.vincentqin.tech/</id>
  
  <author>
    <name>Vincent Qin</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NEWS</title>
    <link href="https://www.vincentqin.tech/posts/news/"/>
    <id>https://www.vincentqin.tech/posts/news/</id>
    <published>2018-06-19T12:15:27.000Z</published>
    <updated>2018-06-19T12:34:46.043Z</updated>
    
    <content type="html"><![CDATA[<div class="note danger"><p>大家好，最近评论系统<a href="https://www.hypercomments.com" target="_blank" rel="external">HyperComments</a>竟然开始收费了，于是我不得不改用新的评论系统<a href="https://valine.js.org/" target="_blank" rel="external">valine</a>。这样一来，原来的评论都看不到了，由此给大家带来的不便，特此道歉！</p><p>Hello everyone, the comment system <a href="https://www.hypercomments.com" target="_blank" rel="external">HyperComments</a> is charging recently, so I had to switch to the new comment system <a href="https://valine.js.org/" target="_blank" rel="external">valine</a>. As a result, the original comments are invisible. I deeply apologize for this inconveniences!</p></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div class=&quot;note danger&quot;&gt;&lt;p&gt;大家好，最近评论系统&lt;a href=&quot;https://www.hypercomments.com&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;HyperComments&lt;/a&gt;竟然开始收费了，于是我不得不
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Light Field Depth Estimation</title>
    <link href="https://www.vincentqin.tech/posts/light-field-depth-estimation/"/>
    <id>https://www.vincentqin.tech/posts/light-field-depth-estimation/</id>
    <published>2018-05-16T05:35:54.000Z</published>
    <updated>2018-06-04T15:02:08.883Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/street.jpg" alt=""></p><div class="note success"><p>本文将介绍光场领域进行深度估计的相关研究。<br>In this post, I’ll introduce some depth estimation algorithms using Light field information. Here is some of the <a href="https://github.com/Vincentqyw/Depth-Estimation-Light-Field" target="_blank" rel="external">code</a>.<br>研究生阶段的研究方向是光场深度信息的恢复。再此做一些总结，以便于让大家了解光场数据处理的一般步骤以及深度估计的相关的知识，光场可视化部分代码见<a href="https://github.com/Vincentqyw/light-field-Processing" target="_blank" rel="external">light-field-Processing</a>。如有任何疑问或者建议，请大家在评论区提出。</p></div><a id="more"></a><h1 id="什么是光场？"><a href="#什么是光场？" class="headerlink" title="什么是光场？"></a>什么是光场？</h1><p>提到光场，很多人对它的解释模糊不清，在此我对它的概念进行统一表述。它的故事可以追溯到1936年，那是一个春天，Gershun写了一本名为<strong>The Light Field</strong><sup><a href="#fn_1" id="reffn_1">1</a></sup>的鸿篇巨著（感兴趣的同学可以看看那个年代的论文），于是光场的概念就此诞生，但它并没有因此被世人熟知。经过了近六十年的沉寂，1991年Adelson<sup><a href="#fn_2" id="reffn_2">2</a></sup>等一帮帅小伙将光场表示成了如下的7维函数：</p><script type="math/tex; mode=display">P(\theta,\phi,\lambda,t,V_x,V_y,V_z). \tag{1}</script><p>其中$(\theta,\phi)$表示球面坐标，$\lambda$表示光线的波长，$t$表示时间，$(V_x,V_y,V_z)$表示观察者的位置。<br>可以想象假如有这样一张由针孔相机拍摄的黑白照片，它表示：我们从<strong>某个时刻</strong>、<strong>单一视角</strong>观察到的<strong>可见光谱</strong>中某个<strong>波长</strong>的光线的平均。也就是说，它记录了通过$P$点的光强分布，光线方向可以由球面坐标$P(\theta,\phi)$或者笛卡尔坐标$P(x,y)$来表示。对于彩色图片而已，我们要添加光线的波长$\lambda$信息即变为$P(\theta,\phi,\lambda)$。按照同样的思路，彩色电影也就是增加了时间维度$t$，因此$P(\theta,\phi,\lambda,t)$。对于彩色全息电影而言，我们可以从任意空间位置$(V_x,V_y,V_z)$进行观看，于是其可以表达为最终的形式$P(\theta,\phi,\lambda,t,V_x,V_y,V_z)$。这个函数又被成为全光函数（Plenoptic Function）。<br>但是以上的七维的全光函数过于复杂，难以记录以及编程实现。所以在实际应用中我们对其进行简化处理。第一个简化是单色光以及时不变。可分别记录3原色以简化掉波长$\lambda$，可以通过记录不同帧以简化$t$，这样全光函数就变成了5D。第二个简化是Levoy<sup><a href="#fn_3" id="reffn_3">3</a></sup>等人（1996年）认为5D光场中还有一定的冗余，可以在自由空间（光线在传播过程中能量保持不变）中简化成4D。</p><h2 id="光场参数化表示"><a href="#光场参数化表示" class="headerlink" title="光场参数化表示"></a>光场参数化表示</h2><p>参数化表示要解决的问题包括：1. 计算高效；2. 光线可控；3. 光线均匀采样。目前比较常用的表示方式是双平面法（$2PP$）<sup><a href="#fn_3" id="reffn_3">3</a></sup>，利用双平面法可以将光场表示为：$L(u,v,s,t)$。其中$(u,v)$为第一个平面，$(s,t)$是第二个平面。那么一条有方向的直线可以表示为连接$uv$以及$st$平面上任意两点确定的线，如下图所示：</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/2pp-v1.png" alt=""></p><p>【注】：Levoy<sup><a href="#fn_3" id="reffn_3">3</a></sup>首先利用双平面法对光场进行表示，光线首先通过$uv$平面，然后再通过$st$平面。但是后来（同年）Gortler<sup><a href="#fn_4" id="reffn_4">4</a></sup>等人将其传播方向反了过来，导致后续研究者对此表述并不一致。与此同时，也有不少文献中也引入了$xy$坐标，例如著名的光场相机的缔造者N.G.博士的毕业论文。通常情况下，这指的是像平面坐标，即指的是由传感器得到的图像中像素的位置坐标。由于后续处理中都是针对图像而言，而对于光学结构以及光线的传播过程并不感兴趣。所以为了方便起见，我们在本文中统一采用Levoy<sup><a href="#fn_3" id="reffn_3">3</a></sup>的方式对<strong>光场图像</strong>进行表示，即$uv$表示角度分辨率，$xy$表示空间分辨率，即$L(u,v,x,y)$。同时在表示<strong>光场</strong>时用$L(u,v,s,t)$。有时候二者不做区分，注意即可。</p><h2 id="光场的可视化"><a href="#光场的可视化" class="headerlink" title="光场的可视化"></a>光场的可视化</h2><p>虽然光场由$7D$全光函数降维到$4D$，但是其结构还是很难直观想象。通过固定4D光场参数化表示$L(u,v,s,t)$中的某些变量，我们可以很容易地对光场进行可视化。我们通常认为$(u,v)$控制着某个视角的位置，即相机平面；而$(s,t)$控制着从某个视角观察到的图像。说简单点：$uv$控制角度分辨率，$st$控制空间分辨率（视野）。注意上式描述的是光线的表示方法，并没有涉及图像处理，所以没有$xy$。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/uvst2images.png" width="600" alt="uvst2images"></p><p>接下来讲解，几种常见的可视化方式（图片来源<sup><a href="#fn_5" id="reffn_5">5</a></sup>）。首先是<strong>多视图法</strong>。很容易理解，对于最简单的情况，首先固定$u=u^*,v=v^*$，我们可以得到多视角的某个视图$L(u^*,v^*,s,t)$，如下图所示：</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/allviews.png" alt=""></p><p>第二种表示方法是<strong>角度域法</strong>，通过固定$s=s^*,t=t^*$可以得到某个宏像素$L(u,v,s^*,t^*)$，如下图所示：</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/angular-patch.png" alt=""></p><p>第三种表示方法是<strong>极线图法</strong>，通过固定$v=v^*,t=t^*$可以得到极线图：$L(u,v^*,s,t^*)$，如下图中水平方向的图所示；同理固定$u=u^*,s=s^*$可以得到极线图：$L(u^*,v,s^*,t)$，如下图中竖直方向的图所示：</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/epi.png" alt=""></p><p>最后，给出这几种方式的对应关系图（注意图中，$xy$对应于以上$st$，$st$对应于$uv$）。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/lf-all-view.png" alt=""></p><h1 id="光场的获取"><a href="#光场的获取" class="headerlink" title="光场的获取"></a>光场的获取</h1><p>我们知道传统的相机只能采集来自场景某个方向的$2D$信息，那怎么才能够采集到光场信息呢？试想一下，当多个相机在多个不同视角同时拍摄时，这样我们就可以得到一个光场的采样（多视角图像）了。当然，这是容易想到的方法，目前已有多种获得光场的方式，如下表格中列举了其中具有代表性的方式<sup><a href="#fn_5" id="reffn_5">5</a></sup>。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/lf-acquisition.png" alt=""></p><h1 id="光场深度估计算法分类"><a href="#光场深度估计算法分类" class="headerlink" title="光场深度估计算法分类"></a>光场深度估计算法分类</h1><p>由上可知，光场图像中包含来自场景的多视角信息，这使得深度估计成为可能。相较于传统的多视角深度估计算法而言，基于光场的深度估计算法无需进行相机标定，这大大简化的深度估计的流程。但是由于光场图像巨大导致了深度估计过程占用大量的计算资源。同时这些所谓的多个视角之间虚拟相机的基线过短，从而可能导致误匹配的问题。以下将对多种深度估计算法进行分类并挑选具有代表性的算法进行介绍。</p><h2 id="多视角立体匹配"><a href="#多视角立体匹配" class="headerlink" title="多视角立体匹配"></a>多视角立体匹配</h2><p>根据光场相机的成像原理，我们可以将光场图像想像成为多个虚拟相机在多个不同视角拍摄同一场景得到图像的集合，那么此时的深度估计问题就转换成为多视角立体匹配问题。以下列举几种基于多视角立体匹配算法的深度估计算法<sup><a href="#fn_8" id="reffn_8">8</a></sup> <sup><a href="#fn_9" id="reffn_9">9</a></sup> <sup><a href="#fn_10" id="reffn_10">10</a></sup> <sup><a href="#fn_20" id="reffn_20">20</a></sup> <sup><a href="#fn_21" id="reffn_21">21</a></sup>。</p><table>    <tr>        <td rowspan="6"> MVS-based<br>        </td><td><b>Approach</b></td>        <td><b>Main Feature</b></td>    </tr>    <tr>        <td>Jeon et al. <sup><a href="#fn_8" id="reffn_8">8</a></sup></td>        <td>Phase shift Sub-pixel</td>    </tr>    <tr>        <td>Yu et al. <sup><a href="#fn_9" id="reffn_9">9</a></sup></td>        <td>Line-assisted graph cut</td>    </tr>    <tr>        <td>Heber et al. <sup><a href="#fn_10" id="reffn_10">10</a></sup> <sup><a href="#fn_2" id="reffn_20">20</a></sup></td>        <td>PCA matching term</td>    </tr>    <tr>        <td>Chen et al. <sup><a href="#fn_21" id="reffn_21">21</a></sup></td>        <td>Scam: Bilateral Consistency Metric</td>    </tr></table><p>在这里介绍Jeon等人<sup><a href="#fn_8" id="reffn_8">8</a></sup>提出的基于相移的亚像素多视角立体匹配算法。</p><h3 id="相移理论"><a href="#相移理论" class="headerlink" title="相移理论"></a>相移理论</h3><p>该算法的核心就是用到了相移理论，即空域的一个小的位移在频域为原始信号的频域表达与位移的指数的幂乘积，即如下公式：</p><script type="math/tex; mode=display">\mathcal{F}\left\{I(x+\Delta x)\right\} = \mathcal{F}\left\{I(x)\right\}\exp^{2\pi j\Delta x}. \tag{2}</script><p>所以，经过位移后图像可以表示为：</p><script type="math/tex; mode=display">I'(x)=I(x+\Delta x)={\mathcal{F}^{-1}\left\{\mathcal{F}\left\{I(x)\right\}\exp^{2 \pi j \Delta x}\right\}},\tag{3}</script><p>面对Lytro相机窄基线的难点，通过相移的思想能够实现亚像素精度的匹配，在一定程度上解决了基线短的问题。那么大家可能好奇的是，如何将这个理论用在多视角立体匹配中呢？带着这样的疑问，继续介绍该算法。</p><h3 id="匹配代价构建"><a href="#匹配代价构建" class="headerlink" title="匹配代价构建"></a>匹配代价构建</h3><p>为了能够使子视角图像之间进行匹配，作者设计了2中不同的代价量：Sum of Absolute Differences (SAD)以及Sum of Gradient Differences (GRAD)，最终通过加权的方式获得最终的匹配量$C$，它是位点$x$以及损失编号（可以理解成深度/视差层）$l$的函数，具体形式如下公式所示：</p><script type="math/tex; mode=display">C(x,l) = \alpha C_A(x,l)+(1-\alpha)C_G(x,l),\tag{4}</script><p>其中$\alpha \in [0,1]$表示SAD损失量$C_A$以及SGD损失量$C_G$之间的权重。同时其中的$C_A$被定义为如下形式：</p><script type="math/tex; mode=display">C_A(x,l) = \sum_{u \in V}\sum_{x \in R_x}{\min\left( | I(u_c,x)-I(u,x+\Delta x(u,l))|,\tau _1\right)},\tag{5}</script><p>其中的$R_x$表示在$x$点邻域的矩形区域；$\tau _1$是代价的截断值（为了增加算法鲁棒性）；$V$表示除了中心视角$u_c$之外的其余视角。上述公式通过比较中心视角图像$I(u_c,x)$与其余视角$I(u,x)$的差异来构建损失量，具体而言就是通过不断地在某个视角$I(u_i,x)$上$x$点的周围移动一个<strong>小的距离</strong>并于中心视角做差；重复这个过程直到比较完所有的视角(i=1…视角数目N)为止。此时会用到上节提及的相移理论以得到移动后的像素强度，注意上面提到的<strong>小的距离</strong>实际上就是公式中的$\Delta x$，它被定义为如下形式：</p><script type="math/tex; mode=display">\Delta x(u,l) = lk(u-u_c),\tag{6}</script><p>其中k表示深度/视差层的单位（像素），$\Delta x$会随着任意视角与中心视角之间距离的增大而线性增加。同理，可以构造出第二个匹配代价量SGD，其基本形式如下所示：</p><script type="math/tex; mode=display">C_G(x,l) = \sum_{u \in V}\sum_{x \in R_x}\beta (u){\min\left( Diff_x(u_c,u,x,l),\tau _2\right)}+ \\ \ \ \ (1-\beta (u)){\min\left( Diff_y(u_c,u,x,l),\tau _2\right)},\tag{7}</script><p>其中的$Diff_x(u_c,u,x,l)=|I_x(u_c,x)-I_x(u,x+\Delta x(u,l))|$表示子视角图像在x方向的上的梯度，同理$Diff_y$表示子孔径图像在y方向上的梯度；$\beta (u)$控制着这两个方向代价量的权重，它由任意视角与中心视角之间的相对距离表示：</p><script type="math/tex; mode=display">\beta (u) = \frac{|u-u_c|}{|u-u_c|+|v-v_c|}.\tag{8}</script><p>至此，代价函数构建完毕。随后对于该代价函数利用边缘保持滤波器进行损失聚合，得到优化后的代价量。紧接着作者建立了一个多标签优化模型（GC求解）以及迭代优化模型对深度图进行优化，再此不做详细介绍。下面是其算法的分部结果：</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/jeon-depth-step.png" alt=""></p><h2 id="基于EPI的方法"><a href="#基于EPI的方法" class="headerlink" title="基于EPI的方法"></a>基于EPI的方法</h2><p><img src="http://oofx6tpf6.bkt.clouddn.com/2pp-epi-depth.png" width="100%"></p><p>不同于多视角立体匹配的方式，EPI的方式是通过分析光场数据结构的从而进行深度估计的方式。EPI图像中斜线的斜率就能够反映出场景的深度。上图中点P为空间点，平面$\Pi$为相机平面，平面$\Omega$为像平面。图中$\Delta u$与$\Delta x$的关系可以表示为如下公式<sup><a href="#fn_6" id="reffn_6">6</a></sup>：</p><script type="math/tex; mode=display">\Delta x=- \frac{f}{Z}\Delta u,\tag{9}</script><p>假如固定相同的$\Delta u$，水平方向位移较大的EPI图中斜线所对应的视差就越大，即深度就越小。如下图所示，$\Delta x_2$&gt;$\Delta x_1$，那么绿色线所对应的空间点要比红色线所对应的空间点深度小。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/epi-depth.png" width="100%"></p><p>以下列举几种基于EPI的深度估计算法<sup><a href="#fn_11" id="reffn_11">11</a></sup> <sup><a href="#fn_12" id="reffn_12">12</a></sup> <sup><a href="#fn_13" id="reffn_13">13</a></sup> <sup><a href="#fn_14" id="reffn_14">14</a></sup> <sup><a href="#fn_15" id="reffn_15">15</a></sup> <sup><a href="#fn_24" id="reffn_24">24</a></sup>。</p><table>    <tr>        <td rowspan="9"> EPI-based<br>        </td><td><b>Approach</b></td>        <td><b>Main Feature</b></td>    </tr>    <tr>        <td>Kim et al. <sup><a href="#fn_11" id="reffn_11">11</a></sup></td>        <td>Large scene reconstruction</td>    </tr>    <tr>        <td>Li et al. <sup><a href="#fn_12" id="reffn_12">12</a></sup></td>        <td>Sparse linear optimization</td>    </tr>    <tr>        <td>Krolla et al. <sup><a href="#fn_13" id="reffn_13">13</a></sup></td>        <td>Spherical light field</td>    </tr>    <tr>        <td>Wanner et al. <sup><a href="#fn_14" id="reffn_14">14</a></sup> <sup><a href="#fn_26" id="reffn_26">26</a></sup></td>        <td>Total variation(TV)</td>    </tr>    <tr>        <td>Diebode et al. <sup><a href="#fn_15" id="reffn_15">15</a></sup></td>        <td>Modified structure tensor</td>    </tr>    <tr>        <td>Zhang et al. <sup><a href="#fn_24" id="reffn_24">24</a></sup></td>        <td>Spinning Parallelogram Operator(SPO)</td>    </tr></table><p>在以上表格中最具代表性的算法是由wanner<sup><a href="#fn_14" id="reffn_14">14</a></sup>提出的结构张量法得到EPI图中线的斜率，如下公式所示：</p><script type="math/tex; mode=display"> J= \left[ \begin{matrix}   G_{\sigma}*(S_xS_x) & G_{\sigma}*(S_xS_y)  \\   G_{\sigma}*(S_xS_y) & G_{\sigma}*(S_yS_y)  \end{matrix}  \right]=  \left[ \begin{matrix}   J_{xx} & J_{xy}\\   J_{xy} & J_{yy}  \end{matrix}  \right],  \tag{10}</script><p>其中$S=S_{y^*,v^*}$为极线图。$S_x$以及$S_y$表示极线图在x以及y方向上的梯度，$G_{\sigma}$表示高斯平滑算子。最终极线图中局部斜线的斜率可以表示成如下形式：</p><script type="math/tex; mode=display"> J=\left[ \begin{matrix}   \Delta x  \\    \Delta v  \end{matrix}  \right]=  \left[ \begin{matrix}  \sin \varphi\\   \cos \varphi  \end{matrix}  \right],  \tag{11}</script><p>其中$\varphi = \frac{1}{2}\arctan\left(\frac{J_{yy}-J_{xx}}{2J_{xy}}\right)$。因此深度可以由公式（9）推出：</p><script type="math/tex; mode=display">Z=-f\frac{\Delta v}{\Delta x},  \tag{12}</script><p>通常情况下，可以用一种更加简单的形式，如视差对其进行表示：</p><script type="math/tex; mode=display">d_{y^*,v^*}=-f/Z=\frac{\Delta x}{\Delta v}=\tan \phi .  \tag{13}</script><p>至此，利用上述公式可以从EPI中估计出视差。</p><h2 id="散焦及融合的方法"><a href="#散焦及融合的方法" class="headerlink" title="散焦及融合的方法"></a>散焦及融合的方法</h2><p>光场相机一个很重要的卖点是先拍照后对焦，这其实是根据光场剪切原理<sup><a href="#fn_31" id="reffn_31">31</a></sup>得到的。通过衡量像素在不同焦栈处的“模糊度”可以得到其对应的深度。以下列举几种基于散焦的深度估计算法<sup><a href="#fn_7" id="reffn_7">7</a></sup> <sup><a href="#fn_16" id="reffn_16">16</a></sup> <sup><a href="#fn_17" id="reffn_17">17</a></sup> <sup><a href="#fn_22" id="reffn_22">22</a></sup> <sup><a href="#fn_23" id="reffn_23">23</a></sup>。</p><table>    <tr>        <td rowspan="5"> Defocus-based<br>        </td><td><b>Approach</b></td>        <td><b>Main Feature</b></td>    </tr>    <tr>        <td>Wang et al. <sup><a href="#fn_7" id="reffn_7">7</a></sup></td>        <td>Occlusion-aware</td>    </tr>    <tr>        <td>Tao et al. <sup><a href="#fn_16" id="reffn_16">16</a></sup></td>        <td>Defocus cues & Correspondence cues</td>    </tr>    <tr>        <td>Tao et al. <sup><a href="#fn_17" id="reffn_17">17</a></sup></td>        <td>Angular Coherence</td>    </tr>    <tr>        <td>Williem et al. <sup><a href="#fn_22" id="reffn_22">22</a></sup> <sup><a href="#fn_23" id="reffn_23">23</a></sup></td>        <td>Angular Entropy(AD, AE, CAD, CAE)</td>    </tr></table><p>这里介绍一个最具代表性的工作，由Tao等人<sup><a href="#fn_16" id="reffn_16">16</a></sup>在2013年提出，下图为其算法框架以及分部结果。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/tao2013.png" alt=""><br><img src="http://oofx6tpf6.bkt.clouddn.com/tao2013-results-step-by-step.png" alt=""></p><p>该工作其实就做了2件事情：1. 设计两种深度线索并估计原始深度；2. 置信度分析及MRF融合。以下对其进行具体介绍。</p><h3 id="双线索提取"><a href="#双线索提取" class="headerlink" title="双线索提取"></a>双线索提取</h3><p>首先对光场图像进行重聚焦，然后得到一系列具有不同深度的焦栈。然后对该焦栈分别提取2个线索：散焦量以及匹配量。其中散焦量被定义为：</p><script type="math/tex; mode=display">D_{\alpha}(x)=\frac{1}{|W_{D}|}{\sum _{x' \in W_D} {|\Delta _x{L}_{\alpha}(x')|}},\tag{14}</script><p>其中，$W_D$表示为当前像素领域窗口大小，$\Delta _x$表示水平方向拉式算子，$\overline{L}_{\alpha}(x)$为每个经过平均化后的重聚焦后光场图像，其表达式如下：</p><script type="math/tex; mode=display">\overline{L}_{\alpha}(x)=\frac{1}{N_{u}}\sum _{u'} {L}_{\alpha}(x,u'),\tag{15}</script><p>其中$N_{u}$表示每一个角度域内像素的数目。然后匹配量被定义成如下形式：</p><script type="math/tex; mode=display">{C}_{\alpha}(x)=\frac{1}{|W_{C}|}\sum _{x' \in W_C} {\sigma}_{\alpha}(x'),\tag{16}</script><p>其中，$W_C$表示为当前像素领域窗口大小，${\sigma}_{\alpha}(x)$表示每个宏像素强度的标准差，其表达式为：</p><script type="math/tex; mode=display">{\sigma}_{\alpha}(x)^2=\frac{1}{N_{u}}\sum _{u'} \left({L}_{\alpha}(x,u')-\overline{L}_{\alpha}(x)\right)^2.\tag{17}</script><p>经过以上两个线索可以通过赢者通吃（Winner Takes All，WTA）得到两张原始深度图。注意：对这两个线索使用WTA时略有不同，通过最大化空间对比度可以得到散焦线索对应的深度，最小化角度域方差能够获得匹配量对应的深度。因此二者深度可以分别表示为如下公式：</p><script type="math/tex; mode=display">\alpha ^{*}_D(x)=\mathop{\arg\max}_{\alpha} \ \ {D}_{\alpha}(x).\tag{18}</script><script type="math/tex; mode=display">\alpha ^{*}_C(x)=\mathop{\arg\min}_{\alpha} \ \ {C}_{\alpha}(x).\tag{19}</script><h3 id="置信度分析及深度融合"><a href="#置信度分析及深度融合" class="headerlink" title="置信度分析及深度融合"></a>置信度分析及深度融合</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/tao2013-confidence-analysis.png" alt=""></p><p>上图中显示了两个线索随着深度层次而变化的曲线。接下来的置信度分析用<strong>主次峰值比例</strong>（Peak Ratio）来定义每种线索的置信度，可表示为如下公式，其中的$\alpha ^{*2}_D(x)$以及$\alpha ^{*2}_C(x)$分别表示曲线的次峰值对应的深度层次。</p><script type="math/tex; mode=display">D_{conf}(x)=\frac{D_{\alpha ^{*}_D}(x)}{D_{\alpha ^{*2}_D}(x)}.\tag{20}</script><script type="math/tex; mode=display">C_{conf}(x)=\frac{C_{\alpha ^{*}_C}(x)}{C_{\alpha ^{*2}_C}(x)}.\tag{21}</script><p>接下来对原始深度进行MRF置信度融合：</p><script type="math/tex; mode=display">\mathop{minimize}_{Z} \ \ \sum_{source}\lambda _{source} \sum _i W_i|Z_i-Z_i^{source}|</script><script type="math/tex; mode=display">+\lambda _{flat} \sum _{(x,y)}\left( \left |\frac{\partial Z_i}{\partial x}\right|_{(x,y)}+\left|\frac{\partial Z_i}{\partial y}\right|_{(x,y)}\right)</script><script type="math/tex; mode=display"> + \lambda _{smooth} \sum _{(x,y)}|\Delta Z_i|_{(x,y)}.\tag{22}</script><p>其中，$source$控制着数据项，即优化后的深度要与原始深度尽量保持一致。第二项与第三项分别控制着平坦性（flatness）与平滑性（smoothness）。注意：<strong>平坦</strong>的意思是物体表面没有凹凸变化的沟壑，例如魔方任一侧面，无论是否拼好（忽略中间黑线）。而<strong>平滑</strong>则表示在平坦的基础上物体表面没有花纹，如拼好的魔方的一个侧面。另外的$W$是权重量，此处选用的是每个线索的置信度。</p><script type="math/tex; mode=display"> \{Z_1^{source},Z_2^{source}\}=\{\alpha_C^{*},\alpha_D^{*}\}.\tag{23}</script><script type="math/tex; mode=display"> \{W_1^{source},W_2^{source}\}=\{C_{conf},D_{conf}\}.\tag{24}</script><p>至此，该算法介绍完毕，其代码已经放在我的<a href="https://github.com/Vincentqyw/Depth-Estimation-Light-Field/tree/master/LF_DC" target="_blank" rel="external">Github</a>。</p><h2 id="学习的方法"><a href="#学习的方法" class="headerlink" title="学习的方法"></a>学习的方法</h2><p>目前而言，将深度学习应用于从双目或者单目中恢复深度已经不再新鲜，我在之前的<a href="https://www.vincentqin.tech/2017/12/06/depth-estimation-using-deeplearning-1/">博文1</a>&amp;<a href="https://www.vincentqin.tech/2017/12/10/depth-estimation-using-deeplearning-2/">博文2</a>中有过对这类算法的介绍。但是将其应用于光场领域进行深度估计的算法还真是寥寥无几。不过总有一些勇敢的践行者去探索如何将二者结合，以下列举几种基于学习的深度估计算法<sup><a href="#fn_18" id="reffn_18">18</a></sup> <sup><a href="#fn_19" id="reffn_19">19</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_28" id="reffn_28">28</a></sup> <sup><a href="#fn_29" id="reffn_29">29</a></sup> <sup><a href="#fn_30" id="reffn_30">30</a></sup>。</p><table>    <tr>        <td rowspan="5"> Learning-based<br>           </td><td><b>Approach</b></td>        <td><b>Main Feature</b></td>    </tr>    <tr>        <td>Johannsen et al. <sup><a href="#fn_18" id="reffn_18">18</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup></td>        <td>Sparse coding</td>    </tr>    <tr>        <td>Heber et al. <sup><a href="#fn_19" id="reffn_19">19</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_28" id="reffn_28">28</a></sup></td>        <td>CNN-based</td>    </tr>    <tr>        <td>Jeon et al. <sup><a href="#fn_29" id="reffn_29">29</a></sup></td>        <td>SAD, SGD, ZNCC, CT, Random Forests</td>    </tr>    <tr>        <td>Shin et al. <sup><a href="#fn_30" id="reffn_30">30</a></sup></td>        <td>4-Directions EPIs & CNN-based</td>    </tr></table><p>在此，我将对截止目前（2018年5月29日）而言，在HCI新数据集上表现最好的<a href="https://arxiv.org/abs/1804.02379" target="_blank" rel="external">EPINET: A Fully-Convolutional Neural Network Using Epipolar Geometry for Depth from Light Field Images</a><sup><a href="#fn_30" id="reffn_30">30</a></sup>算法进行介绍，下图为该算法在各个指标上的表现情况。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/epinet-architecture.png" alt=""></p><p>算法摘要：光场相机能够同时采集空间光线的空域以及角度域信息，因此可以根据这种特性恢复出空间场景的涉深度。在本文中，作者提出了一种基于CNN的快速准确的光场深度估计算法。作者在设计网络时将光场的几何结构加入考虑，同时提出了一种新的数据增强算法以克服训练数据不足的缺陷。作者提出的算法能够在HCI 4D-LFB上在多个指标上取得Top1的成绩。作者指出，光场相机存在优势的同时也有诸多缺点，例如：基线超级短且空间&amp;角度分辨率有一定的权衡关系。目前已有很多工作去克服这些问题，这样一来，深度图像的精度提升了，但是带来的后果就是计算量超级大，无法快速地估计出深度。因此作者为了解决精度以及速度之间权衡关系设计了该算法（感觉很有意义吧）。</p><p>上面表格中提到的诸如Johannsen<sup><a href="#fn_18" id="reffn_18">18</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup>以及Heber<sup><a href="#fn_19" id="reffn_19">19</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_28" id="reffn_28">28</a></sup>等人设计的算法仅仅考虑到了一个极线方向，从而容易导致低置信度的深度估计。为了解决他们算法中存在的问题，作者通过一种多流网络将不同的极线图像分别进行编码去预测深度。因为，每个极线图都有属于自己的集合特征，将这些极线图放入网络训练能够充分地利用其提供的信息。</p><h3 id="光场图像几何特征"><a href="#光场图像几何特征" class="headerlink" title="光场图像几何特征"></a>光场图像几何特征</h3><p>由于光场图像可以等效成多个视角图像的集合，这里的视角数目通常要比传统的立体匹配算法需要的视角数目多得多。所以，如果利用全部的视角做深度估计将会相当耗时，所以在实际情况下并不需要用到全部的视角。作者的思路就是想办法尽量减少实际要使用的视角数目，所以作者探究了不同角度域方向光场图像的特征。中心视角图像与其余视角的关系可以表示成如下公式：</p><script type="math/tex; mode=display">L(x,y,0,0)=L(x+d(x,y)*u,y+d(x,y)*v,u,v),\tag{25}</script><p>其中$d(x,y)$表示中心视角到其相应相邻视角之间的视差（disparity）。令角度方向为$\theta$（$\tan \theta=v/u$），我们可以将上式改写成如下公式：</p><script type="math/tex; mode=display">L(x,y,0,0)=L(x+d(x,y)*u,y+d(x,y)*u \tan \theta,u,u \tan \theta).\tag{26}</script><p>作者选择了四个方向$\theta$: 0<sup>o</sup>，45<sup>o</sup>，90<sup>o</sup>，135<sup>o</sup>，同时假设光场图像总视角数为$(2N+1)\times(2N+1)$。</p><h3 id="网络设计"><a href="#网络设计" class="headerlink" title="网络设计"></a>网络设计</h3><p>如本节开始的图所示的网络结构，该网络的开始为多路编码网络（类似于Flownet以及<a href="https://www.cs.toronto.edu/~urtasun/publications/luo_etal_cvpr16.pdf" target="_blank" rel="external">Efficient Deep Learning for Stereo Matching</a><sup><a href="#fn_32" id="reffn_32">32</a></sup>），其输入为4个不同方向视角图像集合，每个方向对应于一路网络，每一路都可以对其对应方向上图像进行编码提取特征。每一路网络都由3个全卷积模块组成，因为全卷积层对逐点稠密预测问题卓有成效，所以作者将每一个全卷积模块定义为这样的卷积层的集合：<strong>Conv-ReLU-Conv-BN-ReLU</strong>，这样的话就可以在局部块中预逐点预测视差。为了解决基线短的问题，作者设计了非常小的卷积核：$2\times 2$，同时stride = 1，这样的话就可以测量$\pm 4$的视差。为了验证这种多路网络的有效性，作者同单路的网络做了对比试验，其结果如下表所示，可见多路网络相对于单路网络有10%的误差降低。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/viewpoints-effect.png" width="60%"></p><p>在完成多路编码之后，网络将这些特征串联起来组成更维度更高的特征。后面的融合网络包含8个卷积块，其目的是寻找经多路编码之后特征之间的相关性。注意除了最后一个卷积块之外，其余的卷积块全部相同。为了推断得到亚像素精度的视差图，作者将最后一个卷积块设计为<strong>Conv-ReLU-Conv</strong>结构。</p><p>最后，图像增强方式包括视角偏移（从9*9视角中选7*7，可扩展3*3倍数据），图像旋转（90<sup>o</sup>，180<sup>o</sup>，270<sup>o</sup>），图像缩放（[0.25,1]），色彩值域变化（[0.5,2]），随机灰度变化，gamma变换（[0.8,1.2]）以及翻转，最终扩充了288倍。</p><p>以下为其各个指标上的性能表现：</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/epinet-evaluation.png" alt=""></p><p>以上介绍了目前已有的深度估计算法不同类别中具有代表性的算法，它们不一定是最优的，但绝对是最容易理解其精髓的。到目前为止，光场领域已经有一大波人做深度估计的工作，利用传统的方式其精度很难再往上提高。随着深度学习的大热，已经有一批先驱开始用深度学习做深度估计，虽然在仿真数据上可以表现得很好，但实际场景千变万化，即使是深度学习的策略也不敢保证对所有的场景都有效。路漫漫其修远兮，深度估计道路阻且长。我认为以后的趋势应该是从EPI图像下手，然后利用CNN提feature（或者响应）；此时可供选择的工具有<a href="http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo" target="_blank" rel="external">KITTI Stereo</a>/<a href="http://hci-lightfield.iwr.uni-heidelberg.de/" target="_blank" rel="external">HCI新数据集算法比较</a>/<a href="http://vision.middlebury.edu/stereo/" target="_blank" rel="external">Middlebury Stereo</a>中较好的几种算法，我们需要总结其算法优势并迁移到光场领域中来。GPU这个Powerful的计算工具一定要用到光场领域中来，发挥出多线程的优势。否则传统的CPU对于动辄上百兆的数据有心无力。这样一来，深度图像不仅仅可以从精度上得以提高，而且深度估计的速度也会更快。至此，本文介绍到此结束。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><blockquote id="fn_1"><sup>1</sup>. Gershun, A. “<a href="http://p9kx5cva1.bkt.clouddn.com/1.Gershun-1939-Journal_of_Mathematics_and_Physics.pdf" target="_blank" rel="external">The Light Field</a>.” Studies in Applied Mathematics 18.1-4(1939):51–151.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. Adelson, Edward H, and J. R. Bergen. “<a href="http://p9kx5cva1.bkt.clouddn.com/2.The%20plenoptic%20function%20and%20the%20elements%20of%20early%20vision.pdf" target="_blank" rel="external">The plenoptic function and the elements of early vision</a>. “ Computational Models of Visual Processing (1991):3-20.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. Levoy, Marc. “<a href="http://p9kx5cva1.bkt.clouddn.com/3.Light_Field_Rendering.pdf" target="_blank" rel="external">Light field rendering</a>.” Conference on Computer Graphics and Interactive Techniques ACM, 1996:31-42.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. Gortler, Steven J., et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/4.The%20lumigraph.pdf" target="_blank" rel="external">The Lumigraph</a>.” Proc Siggraph 96(1996):43-54.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. Wu, Gaochang, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/5.Light-Field-Image-Processing-An%20Overview.pdf" target="_blank" rel="external">Light Field Image Processing: An Overview</a>.” IEEE Journal of Selected Topics in Signal Processing PP.99(2017):1-1.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. Wanner, Sven, and B. Goldluecke. “<a href="http://p9kx5cva1.bkt.clouddn.com/6.Variational%20Light%20Field%20Analysis%20for%20Disparity%20Estimation%20and%20Super-Resolution.pdf" target="_blank" rel="external">Variational Light Field Analysis for Disparity Estimation and Super-Resolution</a>.” IEEE Transactions on Pattern Analysis &amp; Machine Intelligence 36.3(2013):1.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><blockquote id="fn_7"><sup>7</sup>. Wang, Ting Chun, A. A. Efros, and R. Ramamoorthi. “<a href="http://p9kx5cva1.bkt.clouddn.com/7.Occlusion-aware%20Depth%20Estimation%20Using%20Light-field%20Cameras.pdf" target="_blank" rel="external">Occlusion-Aware Depth Estimation Using Light-Field Cameras</a>.” IEEE International Conference on Computer Vision IEEE, 2016:3487-3495.<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></blockquote><blockquote id="fn_8"><sup>8</sup>. Jeon, Hae Gon, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/8.Accurate%20Depth%20Map%20Estimation%20from%20a%20Lenslet%20Light%20Field%20Camera.pdf" target="_blank" rel="external">Accurate depth map estimation from a lenslet light field camera</a>.” Computer Vision and Pattern Recognition IEEE, 2015:1547-1555.<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></blockquote><blockquote id="fn_9"><sup>9</sup>. Yu, Zhan, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/9.Line%20Assisted%20Light%20Field%20Triangulation%20and%20Stereo%20Matching.pdf" target="_blank" rel="external">Line Assisted Light Field Triangulation and Stereo Matching</a>.” IEEE International Conference on Computer Vision IEEE, 2014:2792-2799.<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></blockquote><blockquote id="fn_10"><sup>10</sup>. Heber, Stefan, and T. Pock. “<a href="http://p9kx5cva1.bkt.clouddn.com/10.Shape%20from%20Light%20Field%20meets%20Robust%20PCA.pdf" target="_blank" rel="external">Shape from Light Field Meets Robust PCA</a>.” Computer Vision – ECCV 2014. 2014:751-767.<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a></blockquote><blockquote id="fn_11"><sup>11</sup>. Kim, Changil, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/11.scene-reconstruction-from-high-spatio-angular-resolution-light-fields-siggraph-2013-compressed-kim-et-al.pdf" target="_blank" rel="external">Scene reconstruction from high spatio-angular resolution light fields</a>.” Acm Transactions on Graphics 32.4(2017):1-12.<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a></blockquote><blockquote id="fn_12"><sup>12</sup>. Li, J., M. Lu, and Z. N. Li. “<a href="http://p9kx5cva1.bkt.clouddn.com/12.Continuous%20Depth%20Map%20Reconstruction%20From%20Light%20Fields.pdf" target="_blank" rel="external">Continuous Depth Map Reconstruction From Light Fields</a>.” IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society 24.11(2015):3257.<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a></blockquote><blockquote id="fn_13"><sup>13</sup>. Krolla, Bernd, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/13.Spherical%20light%20field.pdf" target="_blank" rel="external">Spherical Light Fields</a>.” British Machine Vision Conference 2014.<a href="#reffn_13" title="Jump back to footnote [13] in the text."> &#8617;</a></blockquote><blockquote id="fn_14"><sup>14</sup>. Wanner, Sven, C. Straehle, and B. Goldluecke. “<a href="http://p9kx5cva1.bkt.clouddn.com/14.Globally%20consistent%20multi-label%20assignment%20on%20the%20ray%20space%20of%204D%20light%20field.pdf" target="_blank" rel="external">Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a>.” IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2013:1011-1018.<a href="#reffn_14" title="Jump back to footnote [14] in the text."> &#8617;</a></blockquote><blockquote id="fn_15"><sup>15</sup>. Diebold, Maximilian, B. Jahne, and A. Gatto. “<a href="http://p9kx5cva1.bkt.clouddn.com/15.Heterogeneous%20Light%20Fields.pdf" target="_blank" rel="external">Heterogeneous Light Fields</a>.” Computer Vision and Pattern Recognition IEEE, 2016:1745-1753.<a href="#reffn_15" title="Jump back to footnote [15] in the text."> &#8617;</a></blockquote><blockquote id="fn_16"><sup>16</sup>. Tao, M. W, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/16.Depth%20from%20Combining%20Defocus%20and%20Correspondence%20Using%20Light-Field%20Cameras.pdf" target="_blank" rel="external">Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a>.” IEEE International Conference on Computer Vision IEEE Computer Society, 2013:673-680.<a href="#reffn_16" title="Jump back to footnote [16] in the text."> &#8617;</a></blockquote><blockquote id="fn_17"><sup>17</sup>. Tao, Michael W., et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/17.Depth%20from%20Shading,%20Defocus,%20and%20Correspondence%20Using%20Light-Field%20Angular%20Coherence.pdf" target="_blank" rel="external">Depth from shading, defocus, and correspondence using light-field angular coherence</a>.” Computer Vision and Pattern Recognition IEEE, 2015:1940-1948.<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a></blockquote><blockquote id="fn_18"><sup>18</sup>. Johannsen, Ole, A. Sulc, and B. Goldluecke. “<a href="http://p9kx5cva1.bkt.clouddn.com/18.Variational%20separation%20of%20light%20field%20layers.pdf" target="_blank" rel="external">Variational Separation of Light Field Layers</a>.” (2015).<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a></blockquote><blockquote id="fn_19"><sup>19</sup>. Heber, Stefan, and T. Pock. “<a href="http://p9kx5cva1.bkt.clouddn.com/19.Heber_Convolutional_Networks_for_CVPR_2016_paper.pdf" target="_blank" rel="external">Convolutional Networks for Shape from Light Field</a>.” Computer Vision and Pattern Recognition IEEE, 2016:3746-3754.<a href="#reffn_19" title="Jump back to footnote [19] in the text."> &#8617;</a></blockquote><blockquote id="fn_20"><sup>20</sup>. Heber, Stefan, R. Ranftl, and T. Pock. “<a href="http://p9kx5cva1.bkt.clouddn.com/20.Variational%20Shape%20from%20Light%20Field.pdf" target="_blank" rel="external">Variational Shape from Light Field</a>.” Energy Minimization Methods in Computer Vision and Pattern Recognition. Springer Berlin Heidelberg, 2013:66-79.<a href="#reffn_20" title="Jump back to footnote [20] in the text."> &#8617;</a></blockquote><blockquote id="fn_21"><sup>21</sup>. Chen, Can, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/21.Light%20Field%20Stereo%20Matching%20Using%20Bilateral%20Statistics%20of%20Surface%20Cameras-Can_CVPR14_stereo.pdf" target="_blank" rel="external">Light Field Stereo Matching Using Bilateral Statistics of Surface Cameras</a>.” IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2014:1518-1525.<a href="#reffn_21" title="Jump back to footnote [21] in the text."> &#8617;</a></blockquote><blockquote id="fn_22"><sup>22</sup>. Williem W, Kyu P I. “<a href="http://p9kx5cva1.bkt.clouddn.com/22.Williem_Robust_Light_Field_CVPR_2016_paper.pdf" target="_blank" rel="external">Robust light field depth estimation for noisy scene with occlusion</a>.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016:4396-4404.<a href="#reffn_22" title="Jump back to footnote [22] in the text."> &#8617;</a></blockquote><blockquote id="fn_23"><sup>23</sup>. Williem W, Park I K, Lee K M. “<a href="http://p9kx5cva1.bkt.clouddn.com/23.TPAMI2017_Williem.pdf" target="_blank" rel="external">Robust light field depth estimation using occlusion-noise aware data costs</a>.” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017(99):1-1.<a href="#reffn_23" title="Jump back to footnote [23] in the text."> &#8617;</a></blockquote><blockquote id="fn_24"><sup>24</sup>. Zhang S, Sheng H, Li C, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/24.Robust%20Depth%20Estimation%20for%20Light%20Field%20via%20Spinning%20Parallelogram%20Operator.pdf" target="_blank" rel="external">Robust depth estimation for light field via spinning parallelogram operator</a>.” Computer Vision and Image Understanding, 2016, 145:148-159.<a href="#reffn_24" title="Jump back to footnote [24] in the text."> &#8617;</a></blockquote><blockquote id="fn_25"><sup>25</sup>. Johannsen O, Sulc A, Goldluecke B. “<a href="http://p9kx5cva1.bkt.clouddn.com/25.What%20Sparse%20Light%20Field%20Coding%20Reveals%20about%20Scene%20Structure.pdf" target="_blank" rel="external">What sparse light field coding reveals about scene structure</a>.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016(1/3/4):3262-3270.<a href="#reffn_25" title="Jump back to footnote [25] in the text."> &#8617;</a></blockquote><blockquote id="fn_26"><sup>26</sup>. Wanner S, Goldluecke B. “<a href="http://p9kx5cva1.bkt.clouddn.com/26.Reconstructing%20reflective%20and%20transparent%20surfaces%20from%20epipolar%20plane%20images.pdf" target="_blank" rel="external">Reconstructing reflective and transparent surfaces from epipolar plane images</a>.” In German Conference on Pattern Recognition (Proc. GCPR), 2013:1-10.<a href="#reffn_26" title="Jump back to footnote [26] in the text."> &#8617;</a></blockquote><blockquote id="fn_27"><sup>27</sup>. Heber S, Yu W, Pock T. “<a href="http://p9kx5cva1.bkt.clouddn.com/27.U-shaped%20Networks%20for%20Shape%20from%20Light%20Field.pdf" target="_blank" rel="external">U-shaped networks for shape from light field</a>.” British Machine Vision Conference, 2016, 37:1-12.<a href="#reffn_27" title="Jump back to footnote [27] in the text."> &#8617;</a></blockquote><blockquote id="fn_28"><sup>28</sup>. Heber S, Yu W, Pock T. “<a href="http://p9kx5cva1.bkt.clouddn.com/28.Neural%20EPI-volume%20Networks%20for%20Shape%20from%20Light%20Field.pdf" target="_blank" rel="external">Neural EPI-Volume networks for shape from light field</a>.” IEEE International Conference on Computer Vision (ICCV), IEEE Computer Society, 2017:2271-2279.<a href="#reffn_28" title="Jump back to footnote [28] in the text."> &#8617;</a></blockquote><blockquote id="fn_29"><sup>29</sup>. Jeon H G, Park J, Choe G, et.al. “<a href="http://p9kx5cva1.bkt.clouddn.com/29.Depth%20from%20a%20Light%20Field%20Image%20with%20Learning-based%20Matching%20Costs.pdf" target="_blank" rel="external">Depth from a Light Field Image with Learning-based Matching Costs</a>.” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018.<a href="#reffn_29" title="Jump back to footnote [29] in the text."> &#8617;</a></blockquote><blockquote id="fn_30"><sup>30</sup>. Shin C, Jeon H G, Yoon Y. “<a href="http://p9kx5cva1.bkt.clouddn.com/30.EPINET%20A%20fully-Convolutional%20Neural%20Network%20Using%20Epipolar%20Geometry%20for%20Depth%20from%20Light%20Field%20Images.pdf" target="_blank" rel="external">EPINET: A Fully-Convolutional Neural Network for Light Field Depth Estimation Using Epipolar Geometry</a>.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.<a href="#reffn_30" title="Jump back to footnote [30] in the text."> &#8617;</a></blockquote><blockquote id="fn_31"><sup>31</sup>. Ng, Ren. “<a href="http://p9kx5cva1.bkt.clouddn.com/31.Digital%20light%20field%20photography.pdf" target="_blank" rel="external">Digital light field photography</a>.” 2006, 115(3):38-39.<a href="#reffn_31" title="Jump back to footnote [31] in the text."> &#8617;</a></blockquote><blockquote id="fn_32"><sup>32</sup>. Luo, Wenjie, A. G. Schwing, and R. Urtasun. “<a href="http://p9kx5cva1.bkt.clouddn.com/32.Efficient%20deep%20learning%20for%20stereo%20matching.pdf" target="_blank" rel="external">Efficient Deep Learning for Stereo Matching</a>.” IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2016:5695-5703.<a href="#reffn_32" title="Jump back to footnote [32] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/street.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;note success&quot;&gt;&lt;p&gt;本文将介绍光场领域进行深度估计的相关研究。&lt;br&gt;In this post, I’ll introduce some depth estimation algorithms using Light field information. Here is some of the &lt;a href=&quot;https://github.com/Vincentqyw/Depth-Estimation-Light-Field&quot;&gt;code&lt;/a&gt;.&lt;br&gt;研究生阶段的研究方向是光场深度信息的恢复。再此做一些总结，以便于让大家了解光场数据处理的一般步骤以及深度估计的相关的知识，光场可视化部分代码见&lt;a href=&quot;https://github.com/Vincentqyw/light-field-Processing&quot;&gt;light-field-Processing&lt;/a&gt;。如有任何疑问或者建议，请大家在评论区提出。&lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="计算机视觉" scheme="https://www.vincentqin.tech/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="depth estimation" scheme="https://www.vincentqin.tech/tags/depth-estimation/"/>
    
      <category term="light field" scheme="https://www.vincentqin.tech/tags/light-field/"/>
    
  </entry>
  
  <entry>
    <title>CV Related References</title>
    <link href="https://www.vincentqin.tech/posts/cv-books/"/>
    <id>https://www.vincentqin.tech/posts/cv-books/</id>
    <published>2018-04-13T02:42:57.000Z</published>
    <updated>2018-06-03T08:23:26.061Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/moono.jpg" alt=""></p><a id="more"></a><h2 id="视觉"><a href="#视觉" class="headerlink" title="视觉"></a>视觉</h2><ul><li><a href="http://p73slabes.bkt.clouddn.com/.pdf" target="_blank" rel="external">Multiple View Geometry in Computer Vision：计算机视觉中的多视图几何</a>【69.52 MB】</li></ul><h2 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h2><ul><li><a href="http://p73slabes.bkt.clouddn.com/1.%20Selected%20Applications%20of%20Convex%20Optimization-Springer-Verlag%20Berlin%20Heidelberg%20%282015%29.pdf" target="_blank" rel="external">Selected Applications of Convex Optimization：凸优化应用讲义</a></li><li><a href="http://p73slabes.bkt.clouddn.com/CVX.pdf" target="_blank" rel="external">CVX:凸优化问题求解</a></li></ul><h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><ul><li><a href="http://p73slabes.bkt.clouddn.com/2.%20Iterative%20Methods%20for%20Sparse%20Linear%20Systems.pdf" target="_blank" rel="external">Iterative Methods for Sparse Linear Systems：稀疏线性系统求解</a></li><li><a href="http://p73slabes.bkt.clouddn.com/3.%20The%20Matrix%20Cookbook.pdf" target="_blank" rel="external">The Matrix Cookbook：矩阵分析</a></li><li><a href="http://p73slabes.bkt.clouddn.com/MRF.pdf" target="_blank" rel="external">Markov Random Field Image Modelling：马尔科夫随机场模型</a></li><li><a href="http://p73slabes.bkt.clouddn.com/numerical_recipes-3E.pdf" target="_blank" rel="external">Numerical Recipes：数值运算</a>【20.41 MB】</li></ul><h2 id="机器学习算法"><a href="#机器学习算法" class="headerlink" title="机器学习算法"></a>机器学习算法</h2><ul><li><a href="http://p73slabes.bkt.clouddn.com/machine-learning.pdf" target="_blank" rel="external">Machine Learning：机器学习中文版-周志华</a>【85.68 MB】</li><li><a href="http://p73slabes.bkt.clouddn.com/1-C4.5.pdf" target="_blank" rel="external">C4.5</a></li><li><a href="http://p73slabes.bkt.clouddn.com/1-C4.5.pdf" target="_blank" rel="external">K-means</a></li><li><a href="http://p73slabes.bkt.clouddn.com/3-SVM.pdf" target="_blank" rel="external">SVM</a></li><li><a href="http://p73slabes.bkt.clouddn.com/4-Apriori.pdf" target="_blank" rel="external">Apriori</a></li><li><a href="http://p73slabes.bkt.clouddn.com/5-EM.pdf" target="_blank" rel="external">EM</a></li><li><a href="http://p73slabes.bkt.clouddn.com/6-PageRank.pdf" target="_blank" rel="external">PageRank</a></li><li><a href="http://p73slabes.bkt.clouddn.com/8-kNN.pdf" target="_blank" rel="external">kNN</a></li><li><a href="http://p73slabes.bkt.clouddn.com/9-Naive-Bayes.pdf" target="_blank" rel="external">Naive-Bayes</a></li><li><a href="http://p73slabes.bkt.clouddn.com/10-CART.pdf" target="_blank" rel="external">CART</a></li><li><a href="http://p73slabes.bkt.clouddn.com/Deep%20Learning-Yoshua%20Bengio.pdf" target="_blank" rel="external">Deep Learning：深度学习中文版</a>【30.90 MB】</li><li><a href="http://p73slabes.bkt.clouddn.com/Building%20Machine%20Learning%20Projects%20with%20TensorFlow.pdf" target="_blank" rel="external">Building Machine Learning Projects with TensorFlow</a>【13.42 MB】</li><li><a href="http://p73slabes.bkt.clouddn.com/statistical-learning-method.pdf" target="_blank" rel="external">统计学习方法-李航</a>【17.56 MB】</li></ul><h2 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h2><ul><li><a href="http://p73slabes.bkt.clouddn.com/OpenCV-Guide-Primer.pdf" target="_blank" rel="external">OpenCV Guide：Opencv简明教程</a></li></ul><h2 id="会议"><a href="#会议" class="headerlink" title="会议"></a>会议</h2><ul><li><a href="http://www.cvpapers.com/" target="_blank" rel="external">CVPapers - Computer Vision Resource</a></li><li><a href="http://oofx6tpf6.bkt.clouddn.com/ccf_conferences.pdf" target="_blank" rel="external">中国计算机学会推荐国际会议及期刊目录</a></li></ul><p>To be continued, welcome to commit.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/moono.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="CV" scheme="https://www.vincentqin.tech/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>立体视觉综述：Stereo Vision Overview</title>
    <link href="https://www.vincentqin.tech/posts/stereo-vision-overview/"/>
    <id>https://www.vincentqin.tech/posts/stereo-vision-overview/</id>
    <published>2018-03-26T07:03:55.000Z</published>
    <updated>2018-06-14T08:44:56.132Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/depth-overview-cover.jpg" alt=""></p><p>本文主要翻译自<a href="www.vision.deis.unibo.it/smatt">Mattoccia</a>的双目视差估计综述，对于刚刚接触立体深度估计方向的小伙伴会有帮助；如果你是专家也可以做一下复习，如有错误请在评论中指出。文中主要介绍以下几个方面的内容：</p><ul><li>Introduction to stereo vision</li><li>Overview of a stereo vision system</li><li>Algorithms for visual correspondence</li><li>Computational optimizations</li><li>Hardware implementation</li><li>Applications</li></ul><a id="more"></a><h2 id="什么是立体视觉"><a href="#什么是立体视觉" class="headerlink" title="什么是立体视觉"></a>什么是立体视觉</h2><ul><li>是一个能够从双目或者多目相机中提取深度图像的技术</li><li>在计算机视觉领域很火爆的研究话题</li><li>这与以下几个方面的相关：双目立体视觉系统、稠密立体算法、立体视觉应用</li><li>偏好能够实时或者硬件实现</li></ul><h2 id="单目相机"><a href="#单目相机" class="headerlink" title="单目相机"></a>单目相机</h2><p><img src="http://p66ri5yke.bkt.clouddn.com/p6.png" width="1200px"></p><p>如图所示的是单目摄像机的拍摄原理，右侧实际场景可以抽象成左侧的模型。可以发现场景中的P点与Q点会同时汇聚在成像平面中的一点，同样遮挡问题出现在PQ连线的所有点。</p><h2 id="双目相机"><a href="#双目相机" class="headerlink" title="双目相机"></a>双目相机</h2><p>对于双目相机，$O_R$和$O_T$分别是左右相机的光学中心，对于在参考相机像平面上被汇聚的两点（p和q），在目标相机像平面上会被区分开来，那么我们可以找到双目或者多目相机中匹配的点利用三角相似原理来估计深度。那么我们怎么寻找相对应的点呢？一个直观的想法就是固定两幅图中的一幅，然后在另外一幅图中 进行2D范围的搜索匹配点。<br><img src="http://p66ri5yke.bkt.clouddn.com/p8.png" width="1200px"></p><p>但实际情况这样做的代价非常大。不过多亏了有<strong>极线约束</strong>，我们可以在图像的<strong>1D</strong>范围上进行搜索。以下将要对极线约束进行解释。</p><h3 id="极线约束-对极几何"><a href="#极线约束-对极几何" class="headerlink" title="极线约束(对极几何)"></a>极线约束(对极几何)</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p9.png" width="1200px"></p><ul><li>对于参考图像R而言，现实场景中的P与Q点在其像平面${\pi}_R$上被投影成为一个点p=q。</li><li>极线约束规定，属于（红色）视线的点对应位于目标图像T的图像平面${\pi}_T$上的绿线上。</li></ul><p>我们可以在维基百科上找到更为详细的<a href="https://en.wikipedia.org/wiki/Epipolar_geometry" target="_blank" rel="external">介绍</a>，具体描述可见下图。特别感谢<a href="https://blog.csdn.net/lin453701006/article/details/55096777" target="_blank" rel="external">@岳麓吹雪同学</a>的帮忙，以下是他已经整理好的译文。下图是针孔相机模型图。两个针孔相机看向空间点，实际相机的像面位于焦点中心后面，生成了一幅关于透镜的焦点中心对称的图像。<strong>这个问题可以简化为在焦点中心前方放置一个虚拟像面来生成正立图像，而不需要对称变换得到</strong>。$O_L$和$O_R$表示两个相机透镜中心，$X$表示两个相机共同的目标点，$X_L$和$X_R$是点$X$在两像面上的投影。<br><img src="http://p66ri5yke.bkt.clouddn.com/Epipolar_geometry.svg" width="900px"></p><ul><li><p><strong>epipolar points极点</strong><br>每一个相机的透镜中心是不同的，会投射到另一个相机像面的不同点上。这两个像点用$e_L$和$e_R$表示，被称为<strong>epipolar points极点</strong>。两个极点$e_L$、$e_R$分别与透镜中心$O_L$、$O_R$在空间中位于一条直线上。</p></li><li><p><strong>epipolar plane极面</strong><br>将$X$、$O_L$和$O_R$三点形成的面称为epipolar plane极面。</p></li><li><p><strong>epipolar line极线</strong><br>直线$O_LX$被左侧相机看做一个点，因为它和透镜中心位于一条线上。然而，从右相机看直线$O_LX$，则是像面上的一条线直线$e_RX_R$，被称为epipolar line极线。从另一个角度看，极面$XO_LO_R$与相机像面相交形成极线。极线是3D空间中点X的位置函数，随$X$变化，两幅图像会生成一组极线。直线$O_LX$通过透镜中心$O_L$，右像面中对应的极线必然通过极点$e_R$。一幅图像中的所有极线包含了该图像的所有极点。实际上，任意一条包含极点的线都是由空间中某一点$X$推导出的一条极线。</p></li></ul><p>如果两个相机位置已知，则：<br>1.如果投影点$X_L$已知，则极线$e_RX_R$已知，点X必定投影到右像面极线上的$X_R$处。这就意味着，在一个图像中观察到的每个点，在已知的极线上观察到该点的其他图像。这就是Epipolar constraint极线约束：<strong>$X$在右像面上的投影$X_R$必然被约束在$e_RX_R$极线上</strong>。对于$O_LX_L$上的$X$，$X_1$，$X_2$，$X_3$都受该约束。极线约束可以用于测试两点是否对应同一3D点。极线约束也可以用两相机间的基本矩阵来描述。<br>2.如果$X_L$和$X_R$已知，他们的投影线已知。如果两幅图像上的点对应同一点X，则投影线必然交于$X$。这就意味着$X$可以用两个像点的坐标计算得到。</p><p><img src="http://p66ri5yke.bkt.clouddn.com/p10.png" width="1200px"><br>由极线约束可知，我们可以将原来的匹配点搜索范围由2D转换成1D，这样做可以很大程度上减少计算量。我们将左右视图摆放成更容易理解的形式，可以发现对应点的匹配问题转换成了在同一条扫描线上（极线）的匹配问题。</p><p><img src="http://p66ri5yke.bkt.clouddn.com/p11.png" width="1200px"><br>可以发现相机的摆放姿势影响着扫描线的方向。在上图A中，相机与水平呈一定角度地摆放，其扫描线为右图所示，同样是与水平倾斜的扫描线。假如两个相机平行摆放的话，其拍出来匹配对是扫描线已经对齐了的。</p><h3 id="深度与视差"><a href="#深度与视差" class="headerlink" title="深度与视差"></a>深度与视差</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p12.png" width="1200px"><br>如上图所示为扫描线已经对齐了的匹配图像对（以下简称<strong>匹配对</strong>）。可以发现：$PO_RO_T$与$Ppp’$是相似三角形，由于相似三角形原理，我们可以很容易知道：</p><script type="math/tex; mode=display">\frac{b}{Z}=\frac{(b+x_T)-x_R}{Z-f}</script><p>其中，$x_R-x_T$就是视差，Z表示深度，B为基线，f是焦距。</p><p><img src="http://p66ri5yke.bkt.clouddn.com/p13.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p14-1.png" width="1200px"></p><p>所谓<strong>视差就是匹配对中对应点之间x方向上的差异</strong>，我们可以将这种差异转换成为灰度图（越近越白），如上最后一个图所示。<br><img src="http://p66ri5yke.bkt.clouddn.com/p14-2.png" width="1200px"></p><p>上图展示了物体距离相机越近的话，视差就越大。其实很容易理解，将人的双眼比作成双目相机，对比将手指放在双眼前方近处与远处晃动的区别，可以发现在近处的话人眼感知到手指的晃动是比远处晃动的“程度”明显的，那么这种程度就是视差在人脑中的反映。</p><h3 id="视界"><a href="#视界" class="headerlink" title="视界"></a>视界</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p15.png" width="1200px"></p><p>图为双摄装置，基线为b，焦距为f，那么双摄的视界被视差范围所限定{$d_{min},d_{max}$}，如图中绿色包裹的区域。</p><p><img src="http://p66ri5yke.bkt.clouddn.com/p16.png" width="1200px"></p><ul><li>深度是通过利用立体匹配系统将视差离散成一系列平行的平面来测量的；每一层平面对应着一个视差。</li><li>可以通过超像素的方法得到效果更好的深度图。</li></ul><p><img src="http://p66ri5yke.bkt.clouddn.com/p17.png" width="1200px"><br>图为5个视差{$d_{min},d_{min}+4$}组成的视场。</p><p><img src="http://p66ri5yke.bkt.clouddn.com/p18.png" width="1200px"></p><ul><li>图为5个视差{$\Delta+d_{min},\Delta+d_{min}+4$}组成的视场</li><li>$\Delta&gt;0$时，视场收缩并向相机靠近</li></ul><h2 id="深度估计"><a href="#深度估计" class="headerlink" title="深度估计"></a>深度估计</h2><p><img src="http://p66ri5yke.bkt.clouddn.com/p21.png" width="1200px"><br>图中为传统算法以及ICCV2011当时最好的结果。可以发现，能够达到较好的视差是具有挑战性的。下面将要展示视差估计的基本流程。</p><p><img src="http://p66ri5yke.bkt.clouddn.com/p22.png" width="1200px"><br>通过双摄设备采集图像，此时图像是存在镜头畸变的，在进行扫描线对齐之前要进行离线标定以消除镜头畸变。扫描线对齐的过程叫做<strong>镜头矫正</strong>（rectificaition），经过这步之后就可以进行1D的匹配点搜索（stereo correspondence）了。随后通过三角形相似原理得到相应的深度/视差图。</p><h3 id="离线标定"><a href="#离线标定" class="headerlink" title="离线标定"></a>离线标定</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p23.png" width="1200px"><br>标定的目标是寻找：</p><ul><li>相机内参：焦距、图像中心、镜头畸变参数</li><li>相机外参：排列相机使其对齐的参数</li></ul><p>注意的是，相机标定的话一般需要10对以上的图像（通常拍摄棋盘格图像，利用张氏标定法进行标定）。</p><ul><li>标定程序可以见Opencv<sup><a href="#fn_39" id="reffn_39">39</a></sup>和Matlab<sup><a href="#fn_40" id="reffn_40">40</a></sup>。</li><li>更为详细的介绍参见<sup><a href="#fn_20" id="reffn_20">20</a></sup> <sup><a href="#fn_21" id="reffn_21">21</a></sup> <sup><a href="#fn_22" id="reffn_22">22</a></sup>。</li></ul><p><img src="http://p66ri5yke.bkt.clouddn.com/p25.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p26.png" width="1200px"></p><h3 id="匹配矫正"><a href="#匹配矫正" class="headerlink" title="匹配矫正"></a>匹配矫正</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p27.png" width="1200px"><br>利用标定步骤得到的相机的内参对相机镜头畸变进行校正，同时对其扫描线。</p><h3 id="立体匹配"><a href="#立体匹配" class="headerlink" title="立体匹配"></a>立体匹配</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p28.png" width="1200px"><br>目标：从匹配对中寻找对应的点，反映在图像中就是视差图像。</p><h3 id="三角测量"><a href="#三角测量" class="headerlink" title="三角测量"></a>三角测量</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p29.png" width="1200px"><br>给定视差图像，基线长度以及焦距可以通过三角计算得到当前位置对应的3D位置。</p><h2 id="立体匹配的挑战性"><a href="#立体匹配的挑战性" class="headerlink" title="立体匹配的挑战性"></a>立体匹配的挑战性</h2><h3 id="光度失真以及噪声"><a href="#光度失真以及噪声" class="headerlink" title="光度失真以及噪声"></a>光度失真以及噪声</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p37-1.png" width="1200px"></p><h3 id="高光表面"><a href="#高光表面" class="headerlink" title="高光表面"></a>高光表面</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p37-2.png" width="1200px"></p><h3 id="透视收缩"><a href="#透视收缩" class="headerlink" title="透视收缩"></a>透视收缩</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p38.png" width="1200px"></p><h3 id="透视变形"><a href="#透视变形" class="headerlink" title="透视变形"></a>透视变形</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p39-1.png" width="1200px"></p><h3 id="无纹理区域"><a href="#无纹理区域" class="headerlink" title="无纹理区域"></a>无纹理区域</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p39-2.png" width="1200px"></p><h3 id="重复-混淆区域"><a href="#重复-混淆区域" class="headerlink" title="重复/混淆区域"></a>重复/混淆区域</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p40.png" width="1200px"></p><h3 id="透明物体"><a href="#透明物体" class="headerlink" title="透明物体"></a>透明物体</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p41-1.png" width="1200px"></p><h3 id="遮挡区以及不连续区域（1）"><a href="#遮挡区以及不连续区域（1）" class="headerlink" title="遮挡区以及不连续区域（1）"></a>遮挡区以及不连续区域（1）</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p41-2.png" width="1200px"></p><h3 id="遮挡区以及不连续区域（2）"><a href="#遮挡区以及不连续区域（2）" class="headerlink" title="遮挡区以及不连续区域（2）"></a>遮挡区以及不连续区域（2）</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p42.png" width="1200px"></p><h2 id="Middlebury数据集"><a href="#Middlebury数据集" class="headerlink" title="Middlebury数据集"></a>Middlebury数据集</h2><p><a href="http://vision.middlebury.edu/stereo/eval3/" target="_blank" rel="external">Middlebury数据集</a>提供了一套可供深度估计的数据集以及评价系统，深度估计算法可在该数据集上进行测试性能。2003年的数据集提供了Tsukuba, Venus, Teddy and Cones这几个场景的匹配对。</p><p><img src="http://p66ri5yke.bkt.clouddn.com/p44.png" width="1200px"></p><h2 id="匹配问题"><a href="#匹配问题" class="headerlink" title="匹配问题"></a>匹配问题</h2><p>立体匹配的算法可以分成以下几个步骤：</p><ol><li>匹配量/损失计算</li><li>损失聚合</li><li>视差计算/优化</li><li>视差精化</li></ol><ul><li>局部算法包括：<br>1-&gt;2-&gt;3（简单的WTA算法）</li><li>全局算法包括：<br>1（-&gt;2）-&gt;3（全局或者半全局算法）</li></ul><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>数据预处理是为了消除图像的光度失真。常见的操作有：</p><ul><li>LoG滤波器<sup><a href="#fn_41" id="reffn_41">41</a></sup></li><li>消减附近像素中计算的平均值<sup><a href="#fn_42" id="reffn_42">42</a></sup></li><li>双边滤波</li><li>统计变换</li></ul><p>最简单的立体匹配算法如下图所示，逐像素地计算SAD匹配损失；然后通过WTA得到初始视差，但是此时得到的视差质量是很差的。那么如何提高深度图像的质量呢？通常来说有两种不同类别的策略。<br><img src="http://p66ri5yke.bkt.clouddn.com/p47.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p48.png" width="1200px"></p><ul><li>局部算法。同样是利用到了简单的WTA提取到初始视差，但是通过计算窗口内的损失量提高了信噪比。有时需会加入平滑项。Steps 1+2 (+ WTA)<br><img src="http://p66ri5yke.bkt.clouddn.com/p50.png" width="1200px"></li><li>全局/半全局算法。寻找能够使得能量函数的最小值的视差以得到逐点视差。Steps 1+ Step3。（有时，损失函数需要聚合）</li></ul><p>两种算法都假设了匹配对是平滑的，但有时，该假设并不成立。这个假设在局部算法中隐晦地提及，却在全局算法中明确地建模，如下形式。</p><script type="math/tex; mode=display">E(d)=E_{data}(d)+E_{smooth}(d)</script><h2 id="损失量的计算"><a href="#损失量的计算" class="headerlink" title="损失量的计算"></a>损失量的计算</h2><h3 id="逐像素的匹配误差"><a href="#逐像素的匹配误差" class="headerlink" title="逐像素的匹配误差"></a>逐像素的匹配误差</h3><ul><li><p>绝对值误差</p><script type="math/tex; mode=display">e(x,y,d)=|I_R(x,y)-I_T(x+d,y)|</script></li><li><p>平方误差</p><script type="math/tex; mode=display">e(x,y,d)=(I_R(x,y)-I_T(x+d,y))^2</script></li><li><p>鲁棒匹配子（M-estimators）<br>如截断绝对误差（truncated absolute differences (TAD)）可以减少离群点的干扰：</p><script type="math/tex; mode=display">e(x,y,d)=min\{|I_R(x,y)-I_T(x+d,y),T\}</script></li><li><p>相异性测量对于图像噪声不敏感（Birchfield and Tomasi<sup><a href="#fn_27" id="reffn_27">27</a></sup>）</p></li></ul><p><img src="http://p66ri5yke.bkt.clouddn.com/p52.png" width="1200px"><br>视差空间图像（DSI）是一个如下图所示张量$W\times H\times(d_{max}-d_{min})$，其中的每一个元素$C(x,y,d)$表示$I_R(x_R,y)$与$I_T(x_R+d,y)$之间的匹配度。</p><p><img src="http://p66ri5yke.bkt.clouddn.com/p53.png" width="1200px"></p><h3 id="区域匹配损失"><a href="#区域匹配损失" class="headerlink" title="区域匹配损失"></a>区域匹配损失</h3><ul><li>绝对误差和（Sum of Absolute differences (SAD)）<script type="math/tex; mode=display">C(x,y,d)=\sum_{x\in S}|I_R(x,y)-I_T(x+d,y)|</script></li><li>绝对平方和（Sum of Squared differences (SSD)）<script type="math/tex; mode=display">C(x,y,d)=\sum_{x\in S}\left(I_R(x,y)-I_T(x+d,y)\right)^2</script></li><li>截断绝对误差和（Sum of truncated absolute differences (STAD)）<script type="math/tex; mode=display">C(x,y,d)=\sum_{x\in S}\{|I_R(x,y)-I_T(x+d,y),T\}</script></li><li>Normalized Cross Correlation <sup><a href="#fn_57" id="reffn_57">57</a></sup></li><li>Zero mean Normalized Cross Correlation <sup><a href="#fn_58" id="reffn_58">58</a></sup></li><li>Gradient based MF <sup><a href="#fn_59" id="reffn_59">59</a></sup></li><li>Non parametric <sup><a href="#fn_60" id="reffn_60">60</a></sup> <sup><a href="#fn_61" id="reffn_61">61</a></sup></li><li>Mutual Information <sup><a href="#fn_30" id="reffn_30">30</a></sup></li><li>Combination of matching costs</li></ul><h2 id="损失聚合"><a href="#损失聚合" class="headerlink" title="损失聚合"></a>损失聚合</h2><p>那么从最简单的固定窗口（FW）损失聚合开始，以下为利用FW聚合的TAD损失然后利用WTA得到的深度图。理想是完美的，但现实是骨感的，可以看到下图给出的结果并不佳，这是什么原因呢？<br><img src="http://p66ri5yke.bkt.clouddn.com/p57.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p58.png" width="1200px"><br>a. 隐含地假设前额表面处于同一视差<br>b. 忽略了深度的非连续性<br>c. 平坦区域的处理不佳<br>d. 重复的区域</p><p>对于a. 隐含地假设前额表面处于同一视差，很多即使是当前最好的损失聚合算法也会假设：在一个小的支持域里面的所有点所处的视差是相同的。但实际情况并非如此，可以观察以上两图，人体头像模型的面部是不规则的表面，展现出来的是视差的不断变化；下面的图是桌子平面，它表面是倾斜的，同样表现出来的是视差的变化。</p><p><img src="http://p66ri5yke.bkt.clouddn.com/p59.png" width="1200px"></p><p>对于b. 忽略了深度的非连续性，原本假设真实场景中的正面平行表面在支持域内深度不会变化，但是这个假设在深度不连续处的附近被打破。可以看到下图中在台灯灯罩的边界处出现了深度的间断，这样经过损失聚合之后得到的深度就会出现边界误匹配的现象，表现在图中为边界没有很好的对齐。不过利用TAD可以在一定程度上减少这种现象。</p><p><img src="http://p66ri5yke.bkt.clouddn.com/p60-1.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p60-2.png" width="1200px"></p><p>目前最好的损失聚合算法都在想方设法地改变支持域的形状以适应在仅在相同的已知视差上做匹配。对于FW而言，就是减小其窗口大小，来减少边界定位问题。但是与此同时，这个改变使得匹配问题变得含糊不清，特别是对于有重复区域以及平滑区域的情形。<br><img src="http://p66ri5yke.bkt.clouddn.com/p61.png" width="1200px"></p><p>对于c与d，FW并不能很好地处理。在这两种情况下，损失聚合算法应该不断地加大支持域的尺寸以获得更多的相同深度上的点。<br><img src="http://p66ri5yke.bkt.clouddn.com/p62.png" width="1200px"></p><p>以上为FW所面对的诸多问题。令人吃惊的是，虽然FW看起来如此不堪一击，但是其应用却是如此广泛。原因可能有以下几点：</p><ol><li>容易实现；</li><li>快！(特别感谢增量计算框架)；</li><li>可以在传统的处理器上实时完成计算；</li><li>仅需要很小的内存；</li><li>可硬件（FPGA）实时实现，且功率小（&lt;1W）</li></ol><p>在介绍更加复杂的算法之前，我们首先介绍积分图像（Integral Images (II)）以及箱滤波（Box-Filtering (BF)）。</p><h3 id="积分图像"><a href="#积分图像" class="headerlink" title="积分图像"></a>积分图像</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p64.png" width="1200px"></p><h3 id="箱滤波器"><a href="#箱滤波器" class="headerlink" title="箱滤波器"></a>箱滤波器</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p65.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p66.png" width="1200px"></p><p>可以总结出积分图与箱滤波器的关系：</p><ol><li>每个点需要4个运算</li><li>积分图可以支持不同的支持域尺寸</li><li>积分图有溢出风险</li><li>积分图对内存消耗较大</li></ol><p>在实际应用当中，积分图对于可变支持域的情况会有帮助。</p><h2 id="立体匹配中损失聚合策略的分类及评估"><a href="#立体匹配中损失聚合策略的分类及评估" class="headerlink" title="立体匹配中损失聚合策略的分类及评估"></a>立体匹配中损失聚合策略的分类及评估</h2><p>在文献<sup><a href="#fn_1" id="reffn_1">1</a></sup>中，作者实现、分类以及评估了超过10种损失聚合算法。这些损失聚合的策略包含几种方式：</p><ul><li>位置</li><li>方向</li><li>位置与方向</li><li>权重</li></ul><p>接下来就对文中但不限于文中提到的诸多算法进行介绍 (i.e. Fast Aggregation <sup><a href="#fn_64" id="reffn_64">64</a></sup>, Fast Bilateral Stereo (FBS) <sup><a href="#fn_65" id="reffn_65">65</a></sup> and the Locally Consistent (LC) methodology <sup><a href="#fn_66" id="reffn_66">66</a></sup>)。</p><h3 id="固定窗口"><a href="#固定窗口" class="headerlink" title="固定窗口"></a>固定窗口</h3><p><img src="http://p66ri5yke.bkt.clouddn.com/p87.png" width="1200px"></p><h3 id="可移动窗口11"><a href="#可移动窗口11" class="headerlink" title="可移动窗口11"></a>可移动窗口<sup><a href="#fn_11" id="reffn_11">11</a></sup></h3><p>这种方法是为了应对场景边界定位问题，这种算法不限制当前位置位于支持域中心。<br><img src="http://p66ri5yke.bkt.clouddn.com/p88.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p89.png" width="1200px"></p><h3 id="多窗口7"><a href="#多窗口7" class="headerlink" title="多窗口7"></a>多窗口<sup><a href="#fn_7" id="reffn_7">7</a></sup></h3><p>支持域内元素个数为常数；支持域的形状不限于为矩形；支持域大小可为5、9、25（5W,9W,25W）。下图所示的为9W：<br><img src="http://p66ri5yke.bkt.clouddn.com/p90.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p91.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p92.png" width="1200px"></p><h3 id="可变窗口12"><a href="#可变窗口12" class="headerlink" title="可变窗口12"></a>可变窗口<sup><a href="#fn_12" id="reffn_12">12</a></sup></h3><p>这种方式，支持域的形状是固定的但是尺寸是变化的。支持域的位置是可变的。<br><img src="http://p66ri5yke.bkt.clouddn.com/p94.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p95.png" width="1200px"></p><h3 id="基于分割的窗口5"><a href="#基于分割的窗口5" class="headerlink" title="基于分割的窗口5"></a>基于分割的窗口<sup><a href="#fn_5" id="reffn_5">5</a></sup></h3><p>这种方式根据图像的颜色相似性将其分割成一系列图像块，这对于损失聚合、深度图像优化以及离群点检测都有帮助。这种算法假设：每个分割块内深度平滑变化。由于涉及到图像分割此时要求分割的精度很高，并且分割后的每个支持域的形状也是不规则的。如下图所示，对于一个可允许的最大支持域范围内，包含支持域中心点所在的分割所覆盖支持域权重赋值为1，支持域的其余部分赋值为$\lambda$，其中$\lambda&lt;<1$。 <img="" src="http://p66ri5yke.bkt.clouddn.com/p98-1.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p98-2.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p99.png" width="1200px"></1$。></p><h3 id="自适应加权14-51"><a href="#自适应加权14-51" class="headerlink" title="自适应加权14 51"></a>自适应加权<sup><a href="#fn_14" id="reffn_14">14</a></sup> <sup><a href="#fn_51" id="reffn_51">51</a></sup></h3><p>首先介绍双边滤波，双边滤波是一种边缘保持滤波器，它是根据图像的邻域的颜色以及空间相关性对每个中心点进行加权。类似于双边滤波，自适应加权对其进行了简化，只考虑颜色的相关性。每一个损失都被乘以了一个这样的权重可以得到$C(p_c,q_c)$。<br><img src="http://p66ri5yke.bkt.clouddn.com/p101.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p102.png" width="1200px"><br>以下是自适应加权的结果：<br><img src="http://p66ri5yke.bkt.clouddn.com/p103.png" width="1200px"></p><h3 id="可分支持域10"><a href="#可分支持域10" class="headerlink" title="可分支持域10"></a>可分支持域<sup><a href="#fn_10" id="reffn_10">10</a></sup></h3><h3 id="快速聚合64"><a href="#快速聚合64" class="headerlink" title="快速聚合64"></a>快速聚合<sup><a href="#fn_64" id="reffn_64">64</a></sup></h3><ul><li>假设：在每个分割块内的深度变化平缓；</li><li>损失量：TAD；</li><li>只对参考图像进行聚合；</li><li>对称的支持域</li><li>支持域覆盖整个分割块<br><img src="http://p66ri5yke.bkt.clouddn.com/p109.png" width="1200px"><script type="math/tex; mode=display">C_{agg}(p,q,d)=\frac{C_S(p,q,d)}{|S_p|}+\frac{C_W(p,q,d)}{|r^2|}</script><script type="math/tex; mode=display">C_S(p,q,d)=\sum_{p_i \in S_p}{TAD(p_i,q_{i+d})}</script><script type="math/tex; mode=display">C_W(p,q,d)=\sum_{p_i \in W_p}{TAD(p_i,q_{i+d})}</script>其中$C_W$为了消除“分割锁定”，这一项可能在纹理稠密的区域很有帮助，但是这一项有可能带来深度的不连续性。<br><img src="http://p66ri5yke.bkt.clouddn.com/p111.png" width="1200px"></li></ul><h3 id="快速双边滤波65"><a href="#快速双边滤波65" class="headerlink" title="快速双边滤波65"></a><a href="http://vision.deis.unibo.it/~smatt/fast_bilateral_stereo.htm" target="_blank" rel="external">快速双边滤波</a><sup><a href="#fn_65" id="reffn_65">65</a></sup></h3><p>该方法兼顾了自适应加权的精度以及传统方法的效率。通过逐块的利用双边滤波对损失进行规范化，通过这种方式可以增加对噪声的鲁棒性。可以利用前面提及的积分图或者箱滤波的方式快速计算。由于双边滤波的局部计算特性，可以利用GPU进行加速，<a href="http://vision.deis.unibo.it/~smatt/FBS_GPU.html" target="_blank" rel="external">GPU加速版本</a>。结果如下所示：<br><img src="http://p66ri5yke.bkt.clouddn.com/p115.png" width="1200px"></p><h3 id="局部一致性"><a href="#局部一致性" class="headerlink" title="局部一致性"></a><a href="http://vision.deis.unibo.it/~smatt/lc_stereo.htm" target="_blank" rel="external">局部一致性</a></h3><p>通过对像素之间的一致性约束进行建模寻找像素之间的相关关系，这种方法对于目前最好的方法具有显著的效果提升。<br><img src="http://p66ri5yke.bkt.clouddn.com/p122.png" width="1200px"></p><h3 id="O-1-adaptive-cost-aggregation75"><a href="#O-1-adaptive-cost-aggregation75" class="headerlink" title="O(1) adaptive cost aggregation75"></a>O(1) adaptive cost aggregation<sup><a href="#fn_75" id="reffn_75">75</a></sup></h3><p>该方法受引导滤波的启发，效果还不错，可与最佳效果相媲美。<br><img src="http://p66ri5yke.bkt.clouddn.com/p127.png" width="1200px"></p><h2 id="视差-深度计算以及优化"><a href="#视差-深度计算以及优化" class="headerlink" title="视差/深度计算以及优化"></a>视差/深度计算以及优化</h2><p>该步骤是为了寻找可最小化损失函数的视差（或者得到DSI图像的最佳路径以最小化能量函数）。通常情况下，能量函数可以表示为以下形式</p><script type="math/tex; mode=display">E(d)=E_{data}(d)+E_{smooth}(d)</script><p>其中的数据项$E_{data}(d)$为了衡量目前假定的视差能够以何等程度接近真实视差。目前已经有不少逐像素的损失构造策略，但是目前也涌现了许多基于支持域的数据项。<br>另外一项是平滑项或者叫做正则项$E_{smooth}(d)$，它可以对像素之间的连续性或者相似性进行约束：这一项对大的视差给予大的惩罚，同时对于边界处的大视差变化以及小的惩罚。也就是说，视差的变化在边界处是被允许的。</p><p>以上模型的求解是个NP-hard问题，在这里可以借助几种常用的策略对其进行求解。</p><ul><li>Graph Cuts <sup><a href="#fn_52" id="reffn_52">52</a></sup></li><li>Belief Propagation <sup><a href="#fn_53" id="reffn_53">53</a></sup></li><li>Cooperative optimization <sup><a href="#fn_54" id="reffn_54">54</a></sup></li></ul><p>这些方法的比较在[63]中进行了详述。有意思的是，上述问题的解决可以由动态规划以及扫描线优化来解决。</p><h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><ul><li>高效 (polynomial time) ≈ 1 sec</li><li>边界以及纹理稀疏区域有所帮助</li><li>条带现象<br><img src="http://p66ri5yke.bkt.clouddn.com/p135.png" width="1200px"></li></ul><h3 id="扫描线优化（Scanline-Optimization，SO30）"><a href="#扫描线优化（Scanline-Optimization，SO30）" class="headerlink" title="扫描线优化（Scanline Optimization，SO30）"></a>扫描线优化（Scanline Optimization，SO<sup><a href="#fn_30" id="reffn_30">30</a></sup>）</h3><ul><li>高效 (polynomial time) ≈ few seconds</li><li>边界以及纹理稀疏区域有所帮助</li><li>条带现象</li><li>高内存消耗<br><img src="http://p66ri5yke.bkt.clouddn.com/p139.png" width="1200px"></li></ul><h2 id="视差精化"><a href="#视差精化" class="headerlink" title="视差精化"></a>视差精化</h2><p>原始视差中包含的离群点需要被检测以及被移除；同时，视差是离散的数据点，有时需要更高的精度；以下将会介绍亚像素插值、图像滤波、双向验证。</p><p><img src="http://p66ri5yke.bkt.clouddn.com/p166.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p167.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p168.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p169.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p170.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p181.png" width="1200px"><br><img src="http://p66ri5yke.bkt.clouddn.com/p183.png" width="1200px"></p><font color="F08080" size="5">未完待续...</font><p><strong>注：</strong>特此感谢Stefano Mattoccia给出如此良心的立体视觉综述，<a href="http://www.vision.deis.unibo.it/smatt/Seminars/StereoVision.pdf" target="_blank" rel="external">本文最新版本在此</a>(速度较慢)或者<a href="http://oofx6tpf6.bkt.clouddn.com/StereoVision.pdf" target="_blank" rel="external">这里</a>(较快，大小51.61M)。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><blockquote id="fn_1"><sup>1</sup>. F. Tombari, S. Mattoccia, L. Di Stefano, E. Addimanda, Classification and evaluation of cost aggregation methods for stereo correspondence, IEEE International Conference on Computer Vision and Pattern Recognition (CVPR 2008)<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. Y. Boykov, O. Veksler, and R. Zabih, A variable window approach to early vision IEEE Trans. PAMI, 20(12):1283–1294, 1998<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. S. Chan, Y. Wong, and J. Daniel, Dense stereo correspondence based on recursive adaptive size multi-windowing In Proc. Image and Vision Computing New Zealand (IVCNZ’03), volume 1, pages 256–260, 2003<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><blockquote id="fn_4"><sup>4</sup>. C. Demoulin and M. Van Droogenbroeck, A method based on multiple adaptive windows to improve the determination of disparity maps. In Proc. IEEE Workshop on Circuit, Systems and Signal Processing, pages 615–618, 2005<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. M. Gerrits and P. Bekaert. Local Stereo Matching with Segmentation-based Outlier Rejection In Proc. Canadian Conf. on Computer and Robot Vision (CRV 2006), pages 66-66, 2006.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. M. Gong and R. Yang. Image-gradient-guided real-time stereo on graphics hardware In Proc. Int. Conf. 3D Digital Imaging and Modeling (3DIM), pages 548–555, 2005<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><blockquote id="fn_7"><sup>7</sup>. H. Hirschmuller, P. Innocent, and J. Garibaldi, Real-time correlation-based stereo vision with reduced border errors Int. Journ. of Computer Vision, 47:1–3, 2002<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a></blockquote><blockquote id="fn_8"><sup>8</sup>. S. Kang, R. Szeliski, and J. Chai, Handling occlusions in dense multi-view stereo In Proc. Conf. on Computer Vision and Pattern Recognition (CVPR 2001), pages 103–110, 2001<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a></blockquote><blockquote id="fn_9"><sup>9</sup>. J. Kim, K. Lee, B. Choi, and S. Lee. A dense stereo matching using two-pass dynamic programming with generalized ground control points, In Proc. Conf. on Computer Vision and Pattern Recognition (CVPR 2005), pages 1075–1082, 2005<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a></blockquote><blockquote id="fn_10"><sup>10</sup>. F. Tombari, S. Mattoccia, and L. Di Stefano, Segmentation-based adaptive support for accurate stereo correspondence PSIVT 2007<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a></blockquote><blockquote id="fn_11"><sup>11</sup>. D. Scharstein and R. Szeliski, A taxonomy and evaluation of dense two-frame stereo correspondence algorithms Int. Jour. Computer Vision, 47(1/2/3):7–42, 2002.<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a></blockquote><blockquote id="fn_12"><sup>12</sup>. O. Veksler. Fast variable window for stereo correspondence using integral images, In Proc. Conf. on Computer Vision and Pattern Recognition (CVPR 2003), pages 556–561, 2003<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a></blockquote><blockquote id="fn_13"><sup>13</sup>. Y. Xu, D. Wang, T. Feng, and H. Shum, Stereo computation using radial adaptive windows, In Proc. Int. Conf. on Pattern Recognition (ICPR 2002), volume 3, pages 595–598, 2002<a href="#reffn_13" title="Jump back to footnote [13] in the text."> &#8617;</a></blockquote><blockquote id="fn_14"><sup>14</sup>. K. Yoon and I. Kweon, Adaptive support-weight approach for correspondence search, IEEE Trans. PAMI, 28(4):650–656,2006<a href="#reffn_14" title="Jump back to footnote [14] in the text."> &#8617;</a></blockquote><blockquote id="fn_15"><sup>15</sup>. D. Scharstein and R. Szeliski, <a href="http://vision.middlebury.edu/stereo/eval/" target="_blank" rel="external">http://vision.middlebury.edu/stereo/eval/</a><a href="#reffn_15" title="Jump back to footnote [15] in the text."> &#8617;</a></blockquote><blockquote id="fn_16"><sup>16</sup>. A. Ansar, A. Castano, L. Matthies, Enhanced real-time stereo using bilateral filtering IEEE Conference on Computer Vision and Pattern Recognition 2004<a href="#reffn_16" title="Jump back to footnote [16] in the text."> &#8617;</a></blockquote><blockquote id="fn_17"><sup>17</sup>. D. Scharstein and R. Szeliski, 􀀃High-accuracy stereo depth maps using structured light. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2003), volume 1, pages 195-202<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a></blockquote><blockquote id="fn_18"><sup>18</sup>. D. Scharstein and C. Pal. Learning conditional random fields for stereo.In IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2007)<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a></blockquote><blockquote id="fn_19"><sup>19</sup>. H. Hirschmüller and D. Scharstein. Evaluation of cost functions for stereo matching.In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2007)<a href="#reffn_19" title="Jump back to footnote [19] in the text."> &#8617;</a></blockquote><blockquote id="fn_20"><sup>20</sup>. E. Trucco, A. Verri, Introductory Techniques for 3-D Computer Vision, Prentice Hall, 1998<a href="#reffn_20" title="Jump back to footnote [20] in the text."> &#8617;</a></blockquote><blockquote id="fn_21"><sup>21</sup>. R.I.Hartley, A. Zisserman, Multiple View Geometry in Computer Vision, Cambridge University Press, 2000<a href="#reffn_21" title="Jump back to footnote [21] in the text."> &#8617;</a></blockquote><blockquote id="fn_22"><sup>22</sup>. G. Bradsky, A. Kaehler, Learning Opencv, O’Reilly, 2008<a href="#reffn_22" title="Jump back to footnote [22] in the text."> &#8617;</a></blockquote><blockquote id="fn_23"><sup>23</sup>. OpenCV Computer Vision Library, <a href="http://sourceforge.net/projects/opencvlibrary/" target="_blank" rel="external">http://sourceforge.net/projects/opencvlibrary/</a><a href="#reffn_23" title="Jump back to footnote [23] in the text."> &#8617;</a></blockquote><blockquote id="fn_24"><sup>24</sup>. Jean-Yves Bouguet , Camera Calibration Toolbox for Matlab, <a href="http://www.vision.caltech.edu/bouguetj/calib_doc/" target="_blank" rel="external">http://www.vision.caltech.edu/bouguetj/calib_doc/</a><a href="#reffn_24" title="Jump back to footnote [24] in the text."> &#8617;</a></blockquote><blockquote id="fn_25"><sup>25</sup>. M. A. Fischler and R. C. Bolles, Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography, Comm. of the ACM 24: 381–395, June 1981<a href="#reffn_25" title="Jump back to footnote [25] in the text."> &#8617;</a></blockquote><blockquote id="fn_26"><sup>26</sup>. Z. Wang and Z. Zheng, A region based stereo matching algorithm using cooperative optimization IEEE CVPR 2008<a href="#reffn_26" title="Jump back to footnote [26] in the text."> &#8617;</a></blockquote><blockquote id="fn_27"><sup>27</sup>. S. Birchfield and C. Tomasi. A pixel dissimilarity measure that is insensitive to image sampling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(4):401-406, April 1998<a href="#reffn_27" title="Jump back to footnote [27] in the text."> &#8617;</a></blockquote><blockquote id="fn_28"><sup>28</sup>. J. Zabih, J. Woodfill, Non-parametric local transforms for computing visual correspondence. European Conf. on Computer Vision, Stockholm, Sweden, 151–158<a href="#reffn_28" title="Jump back to footnote [28] in the text."> &#8617;</a></blockquote><blockquote id="fn_29"><sup>29</sup>. S. Mattoccia, F. Tombari, and L. Di Stefano, Stereo vision enabling precise border localization within a scanline optimization framework, ACCV 2007<a href="#reffn_29" title="Jump back to footnote [29] in the text."> &#8617;</a></blockquote><blockquote id="fn_30"><sup>30</sup>. H. Hirschmüller. Stereo vision in structured environments by consistent semi-global matching. CVPR 2006, PAMI 30(2):328-341, 2008<a href="#reffn_30" title="Jump back to footnote [30] in the text."> &#8617;</a></blockquote><blockquote id="fn_31"><sup>31</sup>. F. Tombari, S. Mattoccia, L. Di Stefano, F. Tonelli, Detecting motion by means of 2D and 3D information ACCV’07 Workshop on Multi-dimensional and Multi-view Image Processing (ACCV 2007 WS)<a href="#reffn_31" title="Jump back to footnote [31] in the text."> &#8617;</a></blockquote><blockquote id="fn_32"><sup>32</sup>. P. Azzari, L. Di Stefano, F. Tombari, S. Mattoccia, Markerless augmented reality using image mosaics International Conference on Image and Signal Processing (ICISP 2008)<a href="#reffn_32" title="Jump back to footnote [32] in the text."> &#8617;</a></blockquote><blockquote id="fn_33"><sup>33</sup>. Li Zhang, Brian Curless, and Steven M. Seitz Spacetime Stereo: Shape Recovery for Dynamic Scenes IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2003), pp. 367-374<a href="#reffn_33" title="Jump back to footnote [33] in the text."> &#8617;</a></blockquote><blockquote id="fn_34"><sup>34</sup>. J. Davis, D. Nehab, R. Ramamoothi, S. Rusinkiewicz. Spacetime Stereo : A Unifying Framework for Depth from Triangulation, IEEE Trans. On Pattern Analysis and Machine Intelligence (PAMI), vol. 27, no. 2, Feb 2005<a href="#reffn_34" title="Jump back to footnote [34] in the text."> &#8617;</a></blockquote><blockquote id="fn_35"><sup>35</sup>. F. Tombari, L. Di Stefano, S. Mattoccia, A. Zanetti, Graffiti detection using a Time-Of-Flight camera Advanced Concepts for Intelligent Vision Systems (ACIVS 2008)<a href="#reffn_35" title="Jump back to footnote [35] in the text."> &#8617;</a></blockquote><blockquote id="fn_36"><sup>36</sup>. L. Di Stefano, F. Tombari, A. Lanza, S. Mattoccia, S. Monti, Graffiti detection using two views ECCV 2008 - 8th International Workshop on Visual Surveillance (VS 2008)<a href="#reffn_36" title="Jump back to footnote [36] in the text."> &#8617;</a></blockquote><blockquote id="fn_37"><sup>37</sup>. T. Darrell, D. Demirdijan, N. Checka, P. Felzenszwalb, Plan-view trajectory estimation with dense stereo background models, International Conference on Computer Vision (ICCV 2001), 2001<a href="#reffn_37" title="Jump back to footnote [37] in the text."> &#8617;</a></blockquote><blockquote id="fn_38"><sup>38</sup>. M. Harville, Stereo person tracking with adaptive plan-view templates of height and occupancy statistics Image and Vision Computing 22(2) pp 127-142, February 2004<a href="#reffn_38" title="Jump back to footnote [38] in the text."> &#8617;</a></blockquote><blockquote id="fn_39"><sup>39</sup>. OpenCV Computer Vision Library, <a href="http://sourceforge.net/projects/opencvlibrary/" target="_blank" rel="external">http://sourceforge.net/projects/opencvlibrary/</a><a href="#reffn_39" title="Jump back to footnote [39] in the text."> &#8617;</a></blockquote><blockquote id="fn_40"><sup>40</sup>. Jean-Yves Bouguet , Camera Calibration Toolbox for Matlab, <a href="http://www.vision.caltech.edu/bouguetj/calib_doc/" target="_blank" rel="external">http://www.vision.caltech.edu/bouguetj/calib_doc/</a><a href="#reffn_40" title="Jump back to footnote [40] in the text."> &#8617;</a></blockquote><blockquote id="fn_41"><sup>41</sup>. T. Kanade, H. Kato, S. Kimura, A. Yoshida, and K. Oda, Development of a Video-Rate Stereo Machine International Robotics and Systems Conference (IROS ‘95), Human Robot Interaction and Cooperative Robots, 1995<a href="#reffn_41" title="Jump back to footnote [41] in the text."> &#8617;</a></blockquote><blockquote id="fn_42"><sup>42</sup>. O. Faugeras, B. Hotz, H. Mathieu, T. Viville, Z. Zhang, P. Fua, E. Thron, L. Moll, G. Berry, Real-time correlation-based stereo: Algorithm. Implementation and Applications, INRIA TR n. 2013, 1993<a href="#reffn_42" title="Jump back to footnote [42] in the text."> &#8617;</a></blockquote><blockquote id="fn_43"><sup>43</sup>. F. Crow, Summed-area tables for texture mapping, Computer Graphics, 18(3):207–212, 1984<a href="#reffn_43" title="Jump back to footnote [43] in the text."> &#8617;</a></blockquote><blockquote id="fn_44"><sup>44</sup>. M. Mc Donnel. Box-filtering techniques, Computer Graphics and Image Processing, 17:65–70, 1981<a href="#reffn_44" title="Jump back to footnote [44] in the text."> &#8617;</a></blockquote><blockquote id="fn_45"><sup>45</sup>. A. Goshtasby, 2-D and 3-D Image Registration for Medical, Remote Sensing and Industrial Applications New York: Wiley, 2005<a href="#reffn_45" title="Jump back to footnote [45] in the text."> &#8617;</a></blockquote><blockquote id="fn_46"><sup>46</sup>. B. Zitova and J. Flusser, Image registration methods:A survey, Image Vision Computing, vol. 21, no. 11, pp. 977–1000, 2003<a href="#reffn_46" title="Jump back to footnote [46] in the text."> &#8617;</a></blockquote><blockquote id="fn_47"><sup>47</sup>. Changming Sun, Recursive Algorithms for Diamond, Hexagon and General Polygonal Shaped Window Operations Pattern Recognition Letters, 27(6):556-566, April 2006<a href="#reffn_47" title="Jump back to footnote [47] in the text."> &#8617;</a></blockquote><blockquote id="fn_48"><sup>48</sup>. L. Di Stefano, M. Marchionni, S. Mattoccia, A fast area-based stereo matching algorithm, Image and Vision Computing, 22(12), pp 983-1005, October 2004<a href="#reffn_48" title="Jump back to footnote [48] in the text."> &#8617;</a></blockquote><blockquote id="fn_49"><sup>49</sup>. L. Di Stefano, M. Marchionni, S. Mattoccia, A PC-based real-time stereo vision system, Machine Graphics &amp; Vision, 13(3), pp. 197-220, January 2004<a href="#reffn_49" title="Jump back to footnote [49] in the text."> &#8617;</a></blockquote><blockquote id="fn_50"><sup>50</sup>. D. Comaniciu and P. Meer, Mean shift: A robust approach toward feature space analysis, IEEE Transactions on Pattern Analysis and Machine Intelligence, 24:603–619, 2002<a href="#reffn_50" title="Jump back to footnote [50] in the text."> &#8617;</a></blockquote><blockquote id="fn_51"><sup>51</sup>. C. Tomasi and R. Manduchi. Bilateral filtering for gray and color images. In ICCV98, pages 839–846, 1998<a href="#reffn_51" title="Jump back to footnote [51] in the text."> &#8617;</a></blockquote><blockquote id="fn_52"><sup>52</sup>. V. Kolmogorov and R. Zabih, Computing visual correspondence with occlusions using graph cuts, ICCV 2001<a href="#reffn_52" title="Jump back to footnote [52] in the text."> &#8617;</a></blockquote><blockquote id="fn_53"><sup>53</sup>. A. Klaus, M. Sormann and K. Karner, Segment-based stereo matching using belief propagation and a self-adapting dissimilarity measure, ICPR 2006<a href="#reffn_53" title="Jump back to footnote [53] in the text."> &#8617;</a></blockquote><blockquote id="fn_54"><sup>54</sup>. Z. Wang and Z. Zheng, A region based stereo matching algorithm using cooperative optimization, CVPR 2008<a href="#reffn_54" title="Jump back to footnote [54] in the text."> &#8617;</a></blockquote><blockquote id="fn_55"><sup>55</sup>. L. Di Stefano, S. Mattoccia, Real-time stereo within the VIDET project Real-Time Imaging, 8(5), pp. 439-453, Oct. 2002<a href="#reffn_55" title="Jump back to footnote [55] in the text."> &#8617;</a></blockquote><blockquote id="fn_56"><sup>56</sup>. F. Tombari, S. Mattoccia, L. Di Stefano, Full search-equivalent pattern matching with Incremental Dissimilarity Approximations, IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(1), pp 129-141, January 2009<a href="#reffn_56" title="Jump back to footnote [56] in the text."> &#8617;</a></blockquote><blockquote id="fn_57"><sup>57</sup>. S. Mattoccia, F. Tombari, L. Di Stefano, Fast full-search equivalent template matching by Enhanced Bounded Correlation, IEEE Transactions on Image Processing, 17(4), pp 528-538, April 2008<a href="#reffn_57" title="Jump back to footnote [57] in the text."> &#8617;</a></blockquote><blockquote id="fn_58"><sup>58</sup>. L. Di Stefano, S. Mattoccia, F. Tombari, ZNCC-based template matching using Bounded Partial Correlation Pattern Recognition Letters, 16(14), pp 2129-2134, October 2005<a href="#reffn_58" title="Jump back to footnote [58] in the text."> &#8617;</a></blockquote><blockquote id="fn_59"><sup>59</sup>. F. Tombari, L. Di Stefano, S. Mattoccia, A. Galanti, Performance evaluation of robust matching measures 3rd International Conference on Computer Vision Theory and Applications (VISAPP 2008)<a href="#reffn_59" title="Jump back to footnote [59] in the text."> &#8617;</a></blockquote><blockquote id="fn_60"><sup>60</sup>. R. Zabih, J John Woodll Non-parametric Local Transforms for Computing Visual Correspondence, ECCV 1994<a href="#reffn_60" title="Jump back to footnote [60] in the text."> &#8617;</a></blockquote><blockquote id="fn_61"><sup>61</sup>. D. N. Bhat, S. K. Nayar, Ordinal measures for visual correspondence, CVPR 1996<a href="#reffn_61" title="Jump back to footnote [61] in the text."> &#8617;</a></blockquote><blockquote id="fn_62"><sup>62</sup>. D. G. Lowe, Distinctive image features from scale-invariant keypoints, International Journal of Computer Vision, 60, 2 (2004), pp. 91-110<a href="#reffn_62" title="Jump back to footnote [62] in the text."> &#8617;</a></blockquote><blockquote id="fn_63"><sup>63</sup>. R.Szeliski, R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala, M. Tappen, C. Rother, A Comparative Study of Energy Minimization Methods for Markov Random Fields with Smoothness-Based Priors, IEEE Transactions on Pattern Analysis and Machine Intelligence, 30, 6, June 2008, pp 1068-1080<a href="#reffn_63" title="Jump back to footnote [63] in the text."> &#8617;</a></blockquote><blockquote id="fn_64"><sup>64</sup>. F. Tombari, S. Mattoccia, L. Di Stefano, E. Addimanda, Near real-time stereo based on effective cost aggregation International Conference on Pattern Recognition (ICPR 2008)<a href="#reffn_64" title="Jump back to footnote [64] in the text."> &#8617;</a></blockquote><blockquote id="fn_65"><sup>65</sup>. S. Mattoccia, S. Giardino,A. Gambini, Accurate and efficient cost aggregation strategy for stereo correspondence based on approximated joint bilateral filtering, Asian Conference on Computer Vision (ACCV 2009), September 23-27 2009, Xiang, China<a href="#reffn_65" title="Jump back to footnote [65] in the text."> &#8617;</a></blockquote><blockquote id="fn_66"><sup>66</sup>. S. Mattoccia, A locally global approach to stereo correspondence, 3D Digital Imaging and Modeling (3DIM 2009), pp 1763-1770, October 3-4, 2009, Kyoto, Japan<a href="#reffn_66" title="Jump back to footnote [66] in the text."> &#8617;</a></blockquote><blockquote id="fn_67"><sup>67</sup>. S. Mattoccia, Improving the accuracy of fast dense stereo correspondence algorithms by enforcing local consistency of disparity fields, 3D Data Processing, Visualization, and Transmission (3DPVT 2010), 17-20 May 2010, Paris, France<a href="#reffn_67" title="Jump back to footnote [67] in the text."> &#8617;</a></blockquote><blockquote id="fn_68"><sup>68</sup>. S. Mattoccia, Fast locally consistent dense stereo on multicore, Sixth IEEE Embedded Computer Vision Workshop (ECVW2010), CVPR workshop, June 13, 2010, San Francisco, USA<a href="#reffn_68" title="Jump back to footnote [68] in the text."> &#8617;</a></blockquote><blockquote id="fn_69"><sup>69</sup>. S. Mattoccia, Accurate dense stereo by constraining local consistency on superpixels, 20th International Conference on Pattern Recognition (ICPR2010), August 23-26, 2010, Istanbul, Turkey<a href="#reffn_69" title="Jump back to footnote [69] in the text."> &#8617;</a></blockquote><blockquote id="fn_70"><sup>70</sup>. L. Wang, M. Liao, M. Gong, R. Yang, and D. Nistér. High-quality real-time stereo using adaptive cost aggregation and dynamic programming. 3DPVT 2006<a href="#reffn_70" title="Jump back to footnote [70] in the text."> &#8617;</a></blockquote><blockquote id="fn_71"><sup>71</sup>. S. Mattoccia, M. Viti, F. Ries,. Near real-time Fast Bilateral Stereo on the GPU, 7th IEEE Workshop on Embedded Computer Vision (ECVW20011), CVPR Workshop, June 20, 2011, Colorado Springs (CO), USA<a href="#reffn_71" title="Jump back to footnote [71] in the text."> &#8617;</a></blockquote><blockquote id="fn_72"><sup>72</sup>. S. Mattoccia, L. De-Maeztu, “A fast segmentation-driven algorithm for stereo correspondence”, International Conference on 3D (IC3D 2011), December 7-8, 2011, Liege, Belgium<a href="#reffn_72" title="Jump back to footnote [72] in the text."> &#8617;</a></blockquote><blockquote id="fn_73"><sup>73</sup>. L. De-Maeztu, S. Mattoccia, A. Villanueva, R. Cabeza, “Efficient aggregation via iterative block-based adapting support weight”,International Conference on 3D (IC3D 2011), December 7-8, 2011, Liege, Belgium<a href="#reffn_73" title="Jump back to footnote [73] in the text."> &#8617;</a></blockquote><blockquote id="fn_74"><sup>74</sup>. D. Min, J. Lu, and M. Do, A revisit to cost aggregation in stereo matching: how far can we reduce its computational redundancy?, ICCV 2011<a href="#reffn_74" title="Jump back to footnote [74] in the text."> &#8617;</a></blockquote><blockquote id="fn_75"><sup>75</sup>. L. De-Maeztu, S. Mattoccia, A. Villanueva, R. Cabeza, “Linear stereo matching”, International Conference on Computer Vision (ICCV 2011), November 6-13, 2011, Barcelona, Spain<a href="#reffn_75" title="Jump back to footnote [75] in the text."> &#8617;</a></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/depth-overview-cover.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文主要翻译自&lt;a href=&quot;www.vision.deis.unibo.it/smatt&quot;&gt;Mattoccia&lt;/a&gt;的双目视差估计综述，对于刚刚接触立体深度估计方向的小伙伴会有帮助；如果你是专家也可以做一下复习，如有错误请在评论中指出。文中主要介绍以下几个方面的内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduction to stereo vision&lt;/li&gt;
&lt;li&gt;Overview of a stereo vision system&lt;/li&gt;
&lt;li&gt;Algorithms for visual correspondence&lt;/li&gt;
&lt;li&gt;Computational optimizations&lt;/li&gt;
&lt;li&gt;Hardware implementation&lt;/li&gt;
&lt;li&gt;Applications&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="stereo matching" scheme="https://www.vincentqin.tech/tags/stereo-matching/"/>
    
      <category term="computer vision" scheme="https://www.vincentqin.tech/tags/computer-vision/"/>
    
  </entry>
  
  <entry>
    <title>Lytro的光场AR之路：从巅峰到死亡</title>
    <link href="https://www.vincentqin.tech/posts/lytro-light-field/"/>
    <id>https://www.vincentqin.tech/posts/lytro-light-field/</id>
    <published>2018-03-23T01:57:02.000Z</published>
    <updated>2018-04-14T12:14:20.676Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/lytro_cover.jpg" width="1000px"></p><div class="note success"><p>这篇文章将会介绍Lytro公司在光场以及VR领域的进展。我们知道NG博士在06年创办了这家公司，曾经被誉为硅谷最优潜力的公司之一。它曾经推出了世界上第一款商用的手持式光场相机，进而推出了第二代产品。但是低调的Lytro已经许久不出现大众的视野中，难道是盛名之下其实难副？<br><span id="inline-red">声明</span>：<u><strong>一切理解都是本人观点，如有疑问，还望在</strong>评论<strong>中留言。如需转载请与本人联系，谢谢合作</strong></u>! 邮箱：<a href="/about">点我</a></p></div><a id="more"></a><p>其实不然，原来人家在搞大事情，Lytro公司在用光场技术搞AR！</p><h2 id="6自由度"><a href="#6自由度" class="headerlink" title="6自由度"></a>6自由度</h2><p>言归正传，提到AR首先讲下自由度的概念。人类在地球上的运动可以由6自由度来描述。6自由度描述了一个人在空间中的自然运动，6自由度会比3自由度提供两倍的自由度感受空间！具体而言，前三个自由度是大多数VR都支持的围着轴的旋转运动，后三个是沿着轴向的平移运动，这需要配套的位置跟踪设备的支持，如Oculus Rift, HTC Vive 或者Playstation VR。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/6DOF_Lytro_Color_Version-930x1024.jpg" alt=""></p><p>接下来，详细介绍一下这六个神奇的自由度到底包括哪些。你想或者不想，我们每天都在6个自由度（6DoF）里运动着。</p><h3 id="Yaw（平摇）"><a href="#Yaw（平摇）" class="headerlink" title="Yaw（平摇）"></a>Yaw（平摇）</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/Yaw.gif" width="1000px"></p><h3 id="Pitch（俯仰）"><a href="#Pitch（俯仰）" class="headerlink" title="Pitch（俯仰）"></a>Pitch（俯仰）</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/Pitch.gif" width="1000px"></p><h3 id="Roll（翻滚）"><a href="#Roll（翻滚）" class="headerlink" title="Roll（翻滚）"></a>Roll（翻滚）</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/roll.gif" width="1000px"></p><h3 id="Left-Right（左右）"><a href="#Left-Right（左右）" class="headerlink" title="Left/Right（左右）"></a>Left/Right（左右）</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/left-right.gif" width="1000px"></p><h3 id="Up-Down（上下）"><a href="#Up-Down（上下）" class="headerlink" title="Up/Down（上下）"></a>Up/Down（上下）</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/updown.gif" width="1000px"></p><h3 id="Forward-Backward（前后）"><a href="#Forward-Backward（前后）" class="headerlink" title="Forward/Backward（前后）"></a>Forward/Backward（前后）</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/Forward-Backward.gif" width="1000px"></p><h2 id="光场与AR"><a href="#光场与AR" class="headerlink" title="光场与AR"></a>光场与AR</h2><p>我们生活在一个充满光明的世界，无论是自然光还是人造光，光无时无刻不照普照大地。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/LF_Volume_Video_ScreenGrab_900W-1-678x381.png" width="1000"></p><p>光的定向传播会照射在我们周围的物体表面，从而物体才能够显示出其纹理特征。即使是在虚拟的环境中，光场也可以通过计算机图形学(CG)的模拟光线通过在虚拟场景中反射3D对象来进行创建。在光场的所有无限光线中，我们只能看到那些照在我们眼睛上光线，这些光线穿过瞳孔打在视网膜上从而让我们感知光线的存在。对于这些光线，我们的眼睛会根据光的刺激产生某种神经信号，这些信号中包括光线的颜色，强度以及方向等信息。我们的大脑正是通过这些信号来感知大千世界。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/Screen-Shot-2016-09-28-at-11.17.02-AM.png" width="1000px"></p><p>当我们在光场中移动时，不同的光线会通过我们的瞳孔，给我们的大脑提供额外的信息，使我们能够理解物体在空间中的位置，同时也会了解物体的其他信息例如：材质、反射、折射等等。</p><h3 id="记录光线角度-方向"><a href="#记录光线角度-方向" class="headerlink" title="记录光线角度/方向"></a>记录光线角度/方向</h3><p>为了捕获和再现光场，需要记录<strong>光线的颜色及其路径（方向/角度）</strong>。确定光线的颜色和亮度很简单，那么问题在于如何记录光线的方向/角度。在一个光场内捕获的2D图像中的任何像素（实际动作或被渲染），可以提供与该像素所有相交光线的颜色和亮度信息。这通常被称为“光束”（所有的光线被一个像素捕获），但为了简单起见，我们将使用术语“光线”来描述由光线捕获的光线单个像素。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/Light_Field_two_planes_FINAL-1024x802.png" width="1000px"></p><p>那么如何记录光线的角度和方向呢？目前已经有很多种记录光线的角度和方向路径的技术，但这些技术都需要至少两个点来确定实际的方向/角度。一种常见的方法是使用<strong>双平面法</strong>（2PP）来计算光线路径。光线穿过两个平面并相交于两点，利用这两个交点，可以确定射线的角度和方向。</p><h3 id="视差"><a href="#视差" class="headerlink" title="视差"></a>视差</h3><p><strong>视差也是记录光场的比较常用的技术</strong>。通过计算两个或者多个相邻相机之间拍摄的2D图像的差异可以得到视差信息。这些2D图像是由场景中物体光的颜色亮度组成的彩色像素组成。通过对一系列图像的显著特征进行三角测量，同时比较图像之间像素间的视差，可以计算出各个物体在空间中的位置和距相机的距离（深度）。这种方式可以从2D数据恢复光场。对3D场景的计算机图形渲染，通常会提供对象与摄像机的距离（深度信息），并作为渲染过程的一部分。在以下场景中，我们使用三个相邻的相机对苹果和橘子进行记录。每个单独的相机从不同的位置记录该场景，这会产生图像之间的差异（视差）。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/Disparity_Light_Field_A_FINAL-1024x896.png" width="1000px"></p><p>每个2D图像是一组彩色像素的集合，仅仅表示场景中苹果和橙色表面的颜色和亮度。视差必须通过分析这些2D图像之间的差异得到。<br><img src="http://oofx6tpf6.bkt.clouddn.com/Disparity_Light_Field_B_FINAL-631x1024.png" width="1000px"></p><p>利用三个2D图像之间的视差，可以确定苹果和橘子在空间中的位置以及与三个视点之间的距离。随后经过处理的光线角度和颜色信息会记录在光场体（Light Field Volume）中。在VR中，光场体能够为我们提供沉浸式的高质量视觉体验。为了能够达到这个水平，光场体验需要囊括多种视觉效果。例如每个方向上的完美立体感，光场体的全视差和六个自由度，以及正确的场景流，以实现视觉相关效果（镜面反射和折射）。光场无论在实景动作还是计算机渲染，都可以产生最为优秀的VR电影体验。</p><h2 id="光场VR设备"><a href="#光场VR设备" class="headerlink" title="光场VR设备"></a>光场VR设备</h2><p><img src="http://oofx6tpf6.bkt.clouddn.com/Tracing-Explanation-Hero-678x381.png" width="1000px"></p><h3 id="Lytro-VT"><a href="#Lytro-VT" class="headerlink" title="Lytro VT"></a>Lytro VT</h3><p>不安分的Lytro最近发布了名为“Lytro Volume Tracer”(Lytro VT)的产品，它作为一套强大的工具可以用于CG 3D场景的光场体的创建，同时能够为用户提供视觉高质量以及完全沉浸式的VR体验。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/vr1.jpg" width="1000px"></p><p>Lytro VT可以使用任何DCC和渲染引擎（例如Maya和VRay）来生成一组3D场景的2D采样。首先，Lytro VT将虚拟相机放置于CG场景中，虚拟相机包含场景中任何可能的视角，需要注意的是这些场景已经包含在定义好的光场体中，并且虚拟相机可以根据需要调整以最大限度地提高显示质量和性能。渲染引擎用于追踪场景中的虚拟光线，并从设备中每个摄像头捕获一定数量的2D图像样本。Lytro VT通过追踪从每个被渲染的像素到其相机的原点的光线(光积跟踪)来创建<strong>视觉体</strong>，通过以上神操作就可以感受到沉浸式的光场VR体验。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/10x10x10_Sequence_700px_copyright_2018.gif" alt=""></p><p>以上是由<strong>1000个视点组成的视觉体</strong>（图片加载慢，24.74M）。在该视觉体中，VR HMD中的观看者可以体验具有最高级别的光线追踪光学效果，每个方向上完美的视差以及六个自由度（6DOF）的重建虚拟场景。</p><p>光线跟踪的样本包括对<strong>颜色和深度信息（RGBZ等数据）的跟踪</strong>。摄像机的数量及其配置取决于场景的视觉复杂程度以及播放过程中所需视图的预定大小。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/devices.jpg" width="1000px"></p><p>Lytro VT处理来自于该2D样本的颜色以及深度信息，并通过Lytro Player创建用于在VR中展示的光场体。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/Tracing-Explanation-1-copyright-2018-1024x697.png" width="1000px"></p><p>该3D场景中的视图体由白色立方体表示。单个相机由绿色球体表示，它具有自己单独的视点。虚拟的Lytro VT摄像机包含有成百上千个独立的摄像机。2D场景样本渲染使用虚拟装备中每个独立像机进行光线追踪。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/Tracing-Explanation-Close-Up-2-copyright-2018-1024x697.png" width="1000px"></p><p>以上是一个相机跟踪的来自于场景中5个不同位置的光线的局部放大图，通过对每个独立相机进行光线跟踪就可以重建光场。</p><h3 id="光线追迹"><a href="#光线追迹" class="headerlink" title="光线追迹"></a>光线追迹</h3><p>在将来，Lytro VT与渲染可以和并为一个无缝过程，允许光场直接进行光线跟踪，而不需要2D图像样本的中间步骤。然而这是需要代价的，这一过程需要很强的渲染器集成，并且要放弃这个如今如此灵活的Lytro VT。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/lytro_vt.jpg" width="1000px"></p><p>作为从虚拟3D场景创建真实2D图像的渲染技术，光线追踪能够产生极高质量的图像。用最简单的术语来说，基于模拟光线与3D场景中的物体表面的相互作用，反映在2D图像平面就是被渲染的彩色像素。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/vr_experience.jpg" width="1000px"></p><p>光线追踪适用于精确渲染某些光学效果，例如如反射，折射和散射（光度），但这些需要大量的计算时间。具有全光学效果的光线追踪对于实时帧率而言简直太慢。但是不得不说，光线追踪非常适合需要最高级别图像质量并可以脱机的应用，如电影视觉效果。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/Raytracing-copyright-2018-1024x729.png" width="1000px"></p><p>上图为光线跟踪的过程：<strong>通过虚拟相机的视角可以看到，虚拟相机跟踪到了物体与物体之间的光线反复反射，并最终到达光源的位置</strong>。如果有些物体遮挡了光线，那么就会产生被遮挡的光线。这种技术的计算效率很高，因为它只需追踪相机通过虚拟镜头看到的光线路径。Lytro VT和光线追踪是相辅相成的，然而在光线追踪的概念方向上形成对比。如上所示，光线跟踪通过跟踪从固定摄像机向外看光线的路径，从而呈现图像中的彩色像素。<strong>相反，Lytro VT通过从一个视觉体内的每个视点向内朝着观察者，去追踪来自每个渲染像素的光线来重建光场体</strong>（这句话翻译的不佳，原因是我没太理解VT与光线跟踪的区别…有大神能够理解的话，请在评论区给出）。于是在Lytro Player中，观众在这些密集的光线的移动，沉浸在具有最高级视觉质量的重建CG场景中，并且在每个方向都具有完美的视差和六个自由度。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/cinema.jpg" width="1000px"></p><p>在这种体验中，光线不是实时呈现，而是从大量预先渲染的光线中实时获取，为视图体积内每个位置的每只眼睛组成一张图像。</p><p>PS: Lytro公司在2017年11月30号之后停止了对lytro live photo的线上支持，其相机业务至此告一段落。通过光场VR转型，不知Lytro能否再次创造辉煌？这里留下一个疑问，等待时间的检验吧！</p><h2 id="从巅峰到倒闭"><a href="#从巅峰到倒闭" class="headerlink" title="从巅峰到倒闭"></a>从巅峰到倒闭</h2><p>3月29日更新。</p><p>世界上第一个光场技术初创公司Lytro昨日发表声明，<strong>正式宣布倒闭</strong>！</p><p>看来时间并不允许Lytro继续存活，光场进阶之路就此截止了吗？早些时间就有传闻称Lytro即将倒闭，Google或将接盘。起因是Google早前公布了一款能够显示沉浸式VR场景的App，这种VR场景据说是由多摄像机采集得到，貌似用到了第三方公司的技术。有人猜测这个第三方公司就是Lytro，这是一家以光场技术著称的公司，它利用其光场采集设备获取场景深度，并将其利用到了VR技术之中。</p><p>但这只是猜测，并没有得到印证。有来源显示Lytro早前进行的属于“<strong>资产抛售</strong>”，抛售额度不超过<strong>4000万美元</strong>。也有人说，这个额度更低，不超过2500万美元。</p><p><center><img src="http://oofx6tpf6.bkt.clouddn.com/omd.gif" width="600px"></center><br>有可能接盘的公司包括Google，Facebook，Apple等。有知情人透露，Lytro内部员工已经陆续离职，与此抛售的还有Lytro公司的59项光场专利。这个抛售金额对于Lytro而言简直是低价销售，因为在其成立之初融资金额已经达到了2亿美元，并且到其2017年最后一次融资时已经达到了3.6亿美元！</p><p>投资者多是科技投资巨头，例如Andreessen Horowitz，富士康，GSV，Greylock，NEA，Qualcomm Ventures等。创业艰辛，Lytro同样面对。从2006年成立之初，Lytro就面临着创业圈共同面对的问题。<strong>光场技术的硬件实现异常艰难，同时VR技术的发展并没有想像中那么快。同时，大型平台逐渐成为具有说服力的整合商，这是其发展的一大阻力</strong>。</p><p>与此同时，Lytro的推出的光场相机迷之昂贵，这是VR技术的重要技术支点，同时也成为了其发展中最大的短板。如今看来，Lytro完全有足够的时间和资金提供一个面向大众更具有说服力的报价，以等待在正确的市场条件下推出正确的产品。同时，Lytro公司应该考虑把光场应用到更加广阔的领域，例如无人汽车、智能导航、地图以及游戏等。</p><p>一年前Lytro CEO Jason Rosenthal 在其官方博客中写道：“我认为我们有能力重新定义下一代Lytro的产品线、技术以及品控”。仅仅时隔一年，此时或许该轮到Google来完成Lytro的雄心壮志了。虽然Google对Lytro具体有何企图不得而知，但是我们可以确定的是，借助Google这个世界上最大的移动手机系统提供商，如果光场技术能够成功整合，这将是科技界的一大奇迹！</p><p>以下是Lytro官方通告原文（大意是：我虽已死，光场犹存）：<br><div class="note "><p>At Lytro, we believe that Light Field will continue to shape the course of Virtual and Augmented Reality, and we’re incredibly proud of the role we’ve been able to play in pushing the boundaries of what’s possible. We’ve uncovered challenges we never dreamed of and made breakthroughs at a seemingly impossible pace. We’ve had some spectacular successes, and built entire systems that no one thought possible. More importantly, we built a team that was singularly unified in its focus and unrivaled in its dedication. It has been an honor and a pleasure to contribute to the cinema and Virtual Reality communities, but starting today we will not be taking on new productions or providing professional services as we prepare to wind down the company. We’re excited to see what new opportunities the future brings for the Lytro team as we go our separate ways. We would like to thank the various communities that have supported us and hope that our paths will cross in the future.</p><p>Lytro was founded in 2006 by Executive Chairman Ren Ng, whose Ph.D. research on Light Field imaging won Stanford University’s prize for best thesis in computer science. In late 2015, Lytro announced the world’s first Light Field solution for Virtual Reality (VR), Lytro Immerge, that was quickly followed by the 2016 launch of Lytro Cinema, the world’s first Light Field capture system for cinematic content. With these products, Lytro pioneered the generational shift from legacy 2D imaging to 3D volumetric video.</p></div></p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>光场民用领域的践行者离我们而去，不知光场的未来将何去何从？敢问Raytrix和Magic Leap你们可好？</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://blog.lytro.com/glossary/6dof/" target="_blank" rel="external">6DoF</a></li><li><a href="http://blog.lytro.com/holiday-edition-what-are-the-six-degrees-of-freedom/" target="_blank" rel="external">Holiday Edition: What Are the Six Degrees of Freedom?</a></li><li><a href="http://blog.lytro.com/what-is-a-light-field/" target="_blank" rel="external">What is a Light Field?</a></li><li><a href="http://blog.lytro.com/ray-tracing-lytro-volume-tracing-and-cg-generated-light-fields-in-vr/" target="_blank" rel="external">Ray tracing, Lytro Volume Tracing and CG generated Light Fields in VR</a></li><li><a href="http://blog.lytro.com/primer-on-360-video-for-vr/" target="_blank" rel="external">Primer on Types of 360° Video for VR</a></li><li><a href="https://techcrunch.com/" target="_blank" rel="external">Homepage: Techcrunch</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/lytro_cover.jpg&quot; width =1000px&gt;&lt;/p&gt;
&lt;div class=&quot;note success&quot;&gt;&lt;p&gt;这篇文章将会介绍Lytro公司在光场以及VR领域的进展。我们知道NG博士在06年创办了这家公司，曾经被誉为硅谷最优潜力的公司之一。它曾经推出了世界上第一款商用的手持式光场相机，进而推出了第二代产品。但是低调的Lytro已经许久不出现大众的视野中，难道是盛名之下其实难副？&lt;br&gt;&lt;span id=&quot;inline-red&quot;&gt;声明&lt;/span&gt;：&lt;u&gt;&lt;strong&gt;一切理解都是本人观点，如有疑问，还望在&lt;/strong&gt;评论&lt;strong&gt;中留言。如需转载请与本人联系，谢谢合作&lt;/strong&gt;&lt;/u&gt;! 邮箱：&lt;a href=&quot;/about&quot;&gt;点我&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="计算机视觉" scheme="https://www.vincentqin.tech/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="Light Field" scheme="https://www.vincentqin.tech/tags/Light-Field/"/>
    
      <category term="光场" scheme="https://www.vincentqin.tech/tags/%E5%85%89%E5%9C%BA/"/>
    
      <category term="计算成像" scheme="https://www.vincentqin.tech/tags/%E8%AE%A1%E7%AE%97%E6%88%90%E5%83%8F/"/>
    
  </entry>
  
  <entry>
    <title>Stephen Hawking</title>
    <link href="https://www.vincentqin.tech/posts/stephen-hawking/"/>
    <id>https://www.vincentqin.tech/posts/stephen-hawking/</id>
    <published>2018-03-14T15:43:53.000Z</published>
    <updated>2018-05-15T05:38:27.429Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/Hawkingsig.svg" width="1600px"></p><a id="more"></a><center><p>昨天晚上，拜访许久没去的羽毛球馆</p><p>在去之前有种不好的预感</p><p>结果真出事了</p><p>脚踝意外扭伤</p><p>今天上午去医院拍片</p><p>人很多，需要排队</p><p>谁也没料到</p><p>就在这等待的过程中</p><p>霍金去世了<img src="http://oofx6tpf6.bkt.clouddn.com/stephen-hawking-child.jpg" width="1600px"></p><p>这条消息惊得我一身冷汗</p><p>整个人半天没有反应过来</p><p>我确认了几遍</p><p>无法挽回，这就是事实</p><p>维基百科霍金主页瞬间把</p><p>1942年1月8日－至今</p><p>改成了</p><p>2018年3月14日<img src="http://oofx6tpf6.bkt.clouddn.com/stephen-hawking.jpg" width="1600px"></p><p>巧合的是</p><p>他的生日恰好是伽利略·伽利莱的忌日</p><p>而忌日亦恰好是阿尔伯特·爱因斯坦的生日</p><p>世界圆周率日</p><p>或许在世界的某个角落</p><p>一个婴儿降临在地球</p><p>若干年后</p><p>响彻寰宇</p><p>桌上还有一本他写的</p><p>《A Brief History of Time》</p><p>至今没有读完...<img src="http://oofx6tpf6.bkt.clouddn.com/stephen-hawking-a-brief-history-of-time.jpg" width="1600px"></p><p>高中起迷恋你的宇宙大爆炸和虫洞理论</p><p>晚自习第三节课</p><p>经常会拿出日记本</p><p>书写着由你构建的虫洞世界</p><p>大学看了</p><p>《万物理论》</p><p>你作为一个立体的人</p><p>重新定义了我对你的认知<img src="http://oofx6tpf6.bkt.clouddn.com/stephen-hawking-the-theory-of-everything.jpg" width="1600px"></p><p>考研时英语作文里写到了你</p><p>一个永垂不朽的凡人<img src="http://oofx6tpf6.bkt.clouddn.com/stephen-hawking-wedding.jpeg" width="1600px"></p><p>代表了人类好奇心的极值</p><p>你是最接近于外星人的人类</p><p>我的偶像</p><p>霍金老爷子，一路走好</p><p>时间简史，霍耀苍穹<img src="http://oofx6tpf6.bkt.clouddn.com/stephen-hawking-student.jpg" width="1600px"></p><p>冥想</p><p>也许在如此浩瀚的宇宙中</p><p>我们人类</p><p>仅仅作为宇宙大爆炸数百亿年后</p><p>出现的粒子随机组合<img src="http://oofx6tpf6.bkt.clouddn.com/stephen-hawking-black-hole-2.jpg" width="1600px"></p><p>相比于宇宙的无穷</p><p>人类的寿命抑或人类的历史</p><p>只是这其中的</p><p>一朵不经意的涟漪<img src="http://oofx6tpf6.bkt.clouddn.com/stephen-hawking-smile.jpg" width="1600px"></p><p>但</p><p>这何尝不是一种幸运呢</p><p>对吧？</p><p>霍老爷子<img src="http://oofx6tpf6.bkt.clouddn.com/stephen-hawking-life.jpeg" width="1600px"></p></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/Hawkingsig.svg&quot; width=1600px&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Stephen Hawking" scheme="https://www.vincentqin.tech/tags/Stephen-Hawking/"/>
    
  </entry>
  
  <entry>
    <title>常用的生产力工具</title>
    <link href="https://www.vincentqin.tech/posts/some-useful-tools/"/>
    <id>https://www.vincentqin.tech/posts/some-useful-tools/</id>
    <published>2018-01-06T15:27:38.000Z</published>
    <updated>2018-04-21T08:34:33.776Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/18-1-6/42665014.jpg" alt=""></p><div class="note success"><p>强大的工具能够极大地提高生产力，而我们的要求似乎会更多一些：我们总是在寻找功能与美观兼得的工具。我是一个喜欢折腾的人，正好我有几个不错的工具推荐给大家。需要注意的是，这些软件是为提高生产力服务的，一味地追求炫酷而不求效率可谓顾此失彼，总之，不可亵玩。</p></div><a id="more"></a><h2 id="Writing"><a href="#Writing" class="headerlink" title="Writing"></a>Writing</h2><ul><li>目前写作使用的是<strong>Markdown</strong>文本标记语言，比较好的编辑软件有<strong><a href="https://www.zybuluo.com" target="_blank" rel="external">作业部落</a></strong>和<strong><a href="https://note.youdao.com" target="_blank" rel="external">有道云笔记</a></strong>。除此之外，印象笔记也不错。这些工具都能够实现不同客户端之间的同步，将使我们不会受制于空间与时间。</li><li>桌面端Markdown编辑器使用<a href="http://pad.haroopress.com/" target="_blank" rel="external">Haroopad</a>，但有个缺点就是速度比较慢。</li></ul><p><img src="http://oofx6tpf6.bkt.clouddn.com/18-1-7/46685512.jpg" width="800px"></p><ul><li><a href="http://p7itjlzj6.bkt.clouddn.com/MathType6.9.rar" target="_blank" rel="external">Mathtype破解版</a></li><li><a href="http://p7itjlzj6.bkt.clouddn.com/Microsoft-Toolkit-2.6-beta-5.exe" target="_blank" rel="external">Windows/Office大礼包破解工具</a></li><li><h2 id="Programing"><a href="#Programing" class="headerlink" title="Programing"></a>Programing</h2></li><li><p>推荐一款跨平台的终端<a href="http://www.termius.com/" target="_blank" rel="external">Termius</a>。它同时支持Andriod、IOS以及PC端（Ubuntu &amp; Windows），可以说很好用了。</p></li></ul><p><img src="http://oofx6tpf6.bkt.clouddn.com/18-1-5/26764288.jpg" width="800px"></p><ul><li><a href="http://cmder.net/" target="_blank" rel="external">Cmder</a>完全可以取代Windows下的<code>cmd</code>，同时它还集成了<code>git shell</code>和<code>power shell</code>，可以说很好用很强大了。</li></ul><p><img src="http://oofx6tpf6.bkt.clouddn.com/18-1-6/69629280.jpg" width="800px"></p><ul><li>另外推荐一个可以在windows系统操纵Linux的神器——<a href="https://mobaxterm.mobatek.net/#" target="_blank" rel="external">MobaXterm</a>，支持直接拖动粘贴复制。这里是<a href="http://p7itjlzj6.bkt.clouddn.com/MobaXterm_v8.2.rar" target="_blank" rel="external">v8.2版本</a>。</li></ul><p><img src="http://p7itjlzj6.bkt.clouddn.com/mobaxterm.png" width="800px"></p><ul><li><a href="https://www.jetbrains.com/clion/" target="_blank" rel="external">Clion大礼包</a>，C/C++多平台支持集成开发环境，学生可以申请免费使用。</li></ul><p><img src="http://oofx6tpf6.bkt.clouddn.com/18-1-6/8065251.jpg" width="800px"></p><ul><li><a href="https://code.visualstudio.com/" target="_blank" rel="external">Visual Studio Code</a>，一个强大的轻量级文本编辑器，支持多插件下载。它将开发者的重心放在了coding上，并没有涉及编译。类似的还有<a href="https://www.sublimetext.com/" target="_blank" rel="external">Sublime</a>，<a href="https://notepad-plus-plus.org/" target="_blank" rel="external">Notepad++</a>和<a href="https://atom.io/" target="_blank" rel="external">Atom</a>。</li></ul><p><img src="http://oofx6tpf6.bkt.clouddn.com/18-1-6/60783128.jpg" width="800px"></p><h2 id="Screenshots"><a href="#Screenshots" class="headerlink" title="Screenshots"></a>Screenshots</h2><ul><li><a href="https://www.snipaste.com/index.html" target="_blank" rel="external">Snipaste</a>，一款支持多种自定义的Windows截图工具，颜值与高效并存。</li><li>轻量级录屏软件<a href="http://p7itjlzj6.bkt.clouddn.com/GifCam.exe" target="_blank" rel="external">GifCam</a></li></ul><p><img src="http://oofx6tpf6.bkt.clouddn.com/18-1-6/79224982.jpg" width="800px"></p><h2 id="Search"><a href="#Search" class="headerlink" title="Search"></a>Search</h2><ul><li><a href="https://www.voidtools.com/" target="_blank" rel="external">Everything</a>，在NTFS卷上快速地根据名称查找文件和目录，全盘秒搜！类似的还有<a href="http://www.listary.com/" target="_blank" rel="external">Listary</a>等。</li></ul><h2 id="APPs-on-Phone"><a href="#APPs-on-Phone" class="headerlink" title="APPs on Phone"></a>APPs on Phone</h2><ul><li>我觉得GitHub能设计个手机客户端就好了，目前App store里有几个非官方版本的GitHub客户端，但是用户参差不齐。我目前发现的比较好用的有<strong>GitBucket</strong> (free)，<strong>PPHub for Github</strong> (￥12)。<br><img src="http://oofx6tpf6.bkt.clouddn.com/18-1-7/96412734.jpg" width="50%"></li><li><strong>Coding</strong>手机客户端，能够很方便地查看自己以及他人的coding仓库。界面小清新，还加入了社交功能，但是貌似不太活跃。<br><img src="http://oofx6tpf6.bkt.clouddn.com/18-1-7/74869287.jpg" width="50%"></li><li><p><strong><a href="https://gitter.im/" target="_blank" rel="external">Gitter</a></strong>一款聊天社交平台，我的网站（右下角）就用了Gitter提供的API。<br><img src="http://oofx6tpf6.bkt.clouddn.com/18-1-7/12300517.jpg" width="50%"></p></li><li><p><strong>MeassureKit</strong>，能够利用手机摄像头测距，绘制运动轨迹等非常炫酷的功能。这是一款在内侧阶段一直在玩的AR应用，应该是最早的一批使用Apple提供的ARKIT的APP了吧。<br><img src="http://oofx6tpf6.bkt.clouddn.com/18-1-7/58186493.jpg" width="50%"><br>世上神器千万，如有推荐，欢迎在评论区中出现。</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/18-1-6/42665014.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;note success&quot;&gt;&lt;p&gt;强大的工具能够极大地提高生产力，而我们的要求似乎会更多一些：我们总是在寻找功能与美观兼得的工具。我是一个喜欢折腾的人，正好我有几个不错的工具推荐给大家。需要注意的是，这些软件是为提高生产力服务的，一味地追求炫酷而不求效率可谓顾此失彼，总之，不可亵玩。&lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Tools" scheme="https://www.vincentqin.tech/tags/Tools/"/>
    
  </entry>
  
  <entry>
    <title>深度学习在深度(视差)估计中的应用(2)</title>
    <link href="https://www.vincentqin.tech/posts/depth-estimation-using-deeplearning-2/"/>
    <id>https://www.vincentqin.tech/posts/depth-estimation-using-deeplearning-2/</id>
    <published>2017-12-10T09:07:19.000Z</published>
    <updated>2018-02-09T13:45:13.897Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/wonderwomanv3.jpg" alt=""></p><div class="note success"><p>最近一段时间一直痴迷于如何将深度学习用于深度估计，看了不少关于该方面的介绍，再次做一个简单的总结。虽说<code>深度学习</code>和<code>深度估计</code>都有<code>深度</code>二字，但是其意义确是完全不一样。一个是<code>deep</code>一个是<code>depth</code>，前者表示网络层纵向的延伸度，后者表示三维场景中物体距离摄像头的距离。这两个差异如此之大的名词是如何结合在一起的呢？且听我慢慢解释。</p></div><a id="more"></a><p>深度学习的历史在此不做介绍，我们只关心深度学习在深度估计的方面的成果。在开始之前要到一个著名的网络<strong><a href="https://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15/" target="_blank" rel="external">FlowNet</a></strong>，这是<em>Dosovitskiy</em>等人发表在<strong>ICCV2015</strong>上的作品。这篇文章其实就是做了两件事情：1. 建立了两种结构的FlowNet；2. 建立了一个虚拟场景训练集（Flying Chairs）。最后的测试效果还不错，虽说仅仅在这个数据集上进行了训练，但是泛化能力能够达到业界水平。</p><h2 id="FlowNet"><a href="#FlowNet" class="headerlink" title="FlowNet"></a>FlowNet</h2><p>首先大体看一下这个可以end-to-end训练的网络长得如何，如下图所示：对于输入的图像对，依次经过一个收缩（contractive）网络以及放大（expanding）网络，最后输出得到对应的光流。很难想象CNN可以用来做classification的同时，也可以做到寻找图像之间的相关信息。作者这么做的目的就是为了验证CNN这种强大的特性。原话如是说“<em>The idea is to exploit the ability of convolutional networks to learn strong features at multiple levels of scale and abstraction and to help it with finding the actual correspondences based on these features</em>”。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/flow-net-structure.png" width="60%" alt="A Encoder-Decoder Network for Disparity Estimation"></p><p>接下来就是网络的设计环节，首先作者回顾了之前的网络设计策略。一种是最简单的“sliding window”方式，但这种方式的缺点在于计算量很大，它使用了各种优化包括重用网络的临时输出；另外一种对各个层的临时输出做上采样到全分辨率，然后将这些图叠起来，这行对于每一点而言，都能够得到相应的多级特征向量，这个向量可用来学习想要的信息。</p><h3 id="Contrasting-Part"><a href="#Contrasting-Part" class="headerlink" title="Contrasting Part"></a>Contrasting Part</h3><p>作者受到“per-pixel prediction tasks”的相关工作的启发，设计了两种光流网络框架。一种是相对简单的实现：首先将输入的图像对叠加起来作为输入，然后输入一个网络，让网络自己学，最后提取运动信息。<br><img src="http://oofx6tpf6.bkt.clouddn.com/flownet-simple.png" alt=""></p><p>另外一种方式就是将输入的图像pair（左图&amp;右图）分开训练，提取出高维丰富的信息之后再做相关性连接，即增加了<code>correlation layer</code>。<br><img src="http://oofx6tpf6.bkt.clouddn.com/flownet-corr.png" alt=""><br>这个correlation layer是为了衡量左右图相应位置的相似度而设置的。一个的很直观地理解就是，在左图选取一个patch，同时在右图中的可能的位置选择同样大小的patch进行匹配运算(点积运算或者说是卷积运算)。<br>具体而言，分别在左右<code>feature map</code>（$f_1$和$f_2$）以$x_1$和$x_2$为中心的块之间进行卷积运算。<code>correlation</code>的定义如下：</p><script type="math/tex; mode=display">c(x_1,x_2)=\sum_{o \in [-k,k] \times [-k,k]}{<f_1(x_1+o),f_2(x_2+o)>}</script><p>其中<script type="math/tex">f_1</script>和<script type="math/tex">f_2</script>的维度为<script type="math/tex">w \times h \times c</script>。可以看到计算一次$c(x_1,x_2)$需要$c \times K^2$次乘法，$K:=2k+1$。而对于所有的位置则需要$w^2 \times h^2 \times c \times K^2 $次乘法，可想而知，这个计算量是巨大的。于是作者为了减少运算量，对搜素窗口进行了限制，设置了最大的搜索半径为$d$，则$x_2$就能在窗口大小是$D=2d+1$里计算<code>correlation</code>了。另外值得一提的是，我们以上的过程是在计算光流信息，所以应该在某个<strong>某个窗口</strong>内进行匹配，而不是在某个方向，而后续即将提到的<code>DispNet</code>的 <code>correlation</code> 是在某一个方向上进行搜索。那么最后得到的<code>correlation</code>的维度是$w \times h \times D^2$。</p><!--![](http://oofx6tpf6.bkt.clouddn.com/flow-net-simple-corr.png)--><h3 id="Expanding-Part"><a href="#Expanding-Part" class="headerlink" title="Expanding Part"></a>Expanding Part</h3><p>如下是优化网络的结构，大部分都是结合缩放阶段信息的<strong>反卷积</strong>操作。这里不再赘述，最后得到的结果图像大小是输入图像的$1/4$。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/refinement.png" alt="refinement"></p><p><del>最后作者利用<code>variational approach</code>对低分辨率的输出做了20次迭代以得到高分辨率的光流图，之后又对全分辨率的光流图做了进一步优化。</del></p><p>很多深度估计的工作受到这一篇论文启发，特别是<code>correlation layer</code>实现了寻找图像对之间的相关性这一点，对于后续<code>DispNet</code>的诞生起到重要作用。其实<code>DispNet</code>就是在<code>FlowNet</code>的基础上进行的改进，接下来就会详细的介绍<code>DispNet</code>。</p><h2 id="DispNet"><a href="#DispNet" class="headerlink" title="DispNet"></a>DispNet</h2><p>受到<strong>FlowNet</strong>的启发，另外一篇论文将光流估计拓广到了视差以及场景流的估计。2015年12月放在arxiv上的大作<em><a href="https://arxiv.org/abs/1512.02134" target="_blank" rel="external">A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation</a></em>便是具体代表性的作品。这篇文章有两个主要贡献：</p><ol><li>建立了三个仿真视差数据集（3 stereo video datasets），这是当时第一个超大规模的用于视差以及光流场景流训练以及评估的数据集；</li><li>设计了<strong>DispNet</strong>，<strong>SceneFlownet</strong>。</li></ol><p><img src="http://oofx6tpf6.bkt.clouddn.com/disp-flow.png" width="50%"></p><h3 id="开篇"><a href="#开篇" class="headerlink" title="开篇"></a>开篇</h3><p>文章一开始就说估计场景流这个问题，堪称“皇家赛事”级别的任务（场景流是估计空间三维物体的运动场，相比于光流多了一个深度信息）。然后又提到没有好的数据集无法做到完美的训练，那么这么好的数据集又不是天上掉下来的，那只能自食其力，自力更生，自己造吧，于是他们的超大规模（<strong>35000</strong> stereo frames）的数据集便成功诞生！</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>参考了前方开创性的<strong>FlowNet</strong>工作之后，他们就把<strong>FlowNet</strong>改造成<strong>DispNet</strong>，真是华丽丽的变身。然后对于场景流模型的构建，作者提到：虽然在以往的成千上万的论文中均有涉及到光流的估计，但是仅仅有极少数的工作敢去尝试估计场景流。然后作者就是厉害，他们足够相信CNN具有强大的学习与抽象能力，能够通过某种方式的组合使场景流的估计问题转化成为学习问题。文中提到“相机无关的场景流的学习可以转化成视差，光流以及光流变化学习问题”，于是在实际上文中提到的<strong>SceneFlownet</strong>就是<strong>FlowNet</strong>与<strong>DispNet</strong>的组合。以下重点介绍<strong>DispNet</strong>的实现。</p><h3 id="DispNet-amp-DispNetC"><a href="#DispNet-amp-DispNetC" class="headerlink" title="DispNet &amp; DispNetC"></a>DispNet &amp; DispNetC</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/dispnet.png" alt="dispnet"><br>收缩部分从conv1到conv6b；在放大部分，upconvolutions (upconvN), convolutions (iconvN, prN)和loss layers是交替出现的。从低层提取的特征与高层的特征进行串联，增加特征的多样性。最后的网络输出是pr1。</p><p>二者与<em>Dosovitskiy</em>提出的<strong>FlowNet</strong>两种结构差不多，总结起来共有3个变化：</p><ol><li>对原来的<strong>FLowNet</strong>进行了改造，在上卷积层之间增加了卷积层，这样可以使得最终的深度图像更加的平滑；</li><li>将原来的<code>2D correlation</code> 改造成了<code>1D correlation</code>；并且发现加入<code>correlation</code> 层之后会有普遍的效果提升；原因在于左右图均进行了<code>rectify</code>，基于极线约束，我们就可以在一个方向进行搜索。所以类似于<code>FlowNet</code>，我们得到的<code>correlation</code> map的大小是$D \times w  \times h$。</li><li>放大部分比Flownet多做了一次deconv，使输出为原来的$1/2$。<br>注意：DispNet对应于FlowNet的第一种实现，DispNetC对应于FlowNet的第二种实现；</li></ol><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>虽然文中提出了一个超大的数据集，但是仍然需要一定的数据增强以获得更加多样的训练数据。具体方法：空间变化（旋转，变形，裁剪，缩放），色度变换（颜色，对比度，明暗）。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/dispnet-res.png" alt=""><br>文中比较了Zbontar&amp;LeCun的<a href="https://github.com/jzbontar/mc-cnn" target="_blank" rel="external">MC-CNN</a>以及opencv中<strong>SGBM</strong>方法。<strong>DispNet</strong>是在<code>FlyingThings3D</code>数据集上做得训练，然后在<code>KITTI 2015</code>数据集上做了优化，注意“K”表示优化之后的网络。</p><h2 id="其他网络"><a href="#其他网络" class="headerlink" title="其他网络"></a>其他网络</h2><p>To be continued, more depth net pls refer to this <a href="https://sites.google.com/site/yorkyuhuang/home/research/machine-learning-information-retrieval/disparityestimationbydeeplearning" target="_blank" rel="external">link</a>.</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul><li>此处对<code>correlation</code>的理解还不透彻;</li><li>待补充其他类型深度/视差估计网络；</li></ul><h2 id="Change-Log"><a href="#Change-Log" class="headerlink" title="Change Log"></a>Change Log</h2><ul><li>添加了对<code>correlation</code>的解释。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://lmb.informatik.uni-freiburg.de/resources/binaries/" target="_blank" rel="external">Homepage: Freiburg: Pattern Recognition and Image Processing</a></li><li><a href="https://lmb.informatik.uni-freiburg.de/Publications/2015/DFIB15/" target="_blank" rel="external">Homepage: FlowNet: Learning Optical Flow with Convolutional Networks</a></li><li><a href="http://oofx6tpf6.bkt.clouddn.com/FlowNet-Learning-Optical-Flow-with-Convolutional-Networks.pdf" target="_blank" rel="external">Paper: FlowNet</a></li><li><a href="http://oofx6tpf6.bkt.clouddn.com/A%20Large%20Dataset%20to%20Train%20Convolutional%20Networks%20for%20Disparity,%20Optical%20Flow,%20and%20Scene%20Flow%20Estimation.pdf" target="_blank" rel="external">Paper: A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation</a></li><li><a href="https://github.com/jzbontar/mc-cnn" target="_blank" rel="external">Github: Stereo matching by training a convolutional neural network to compare image patches</a></li><li><a href="https://sites.google.com/site/yorkyuhuang/home/research/machine-learning-information-retrieval/disparityestimationbydeeplearning" target="_blank" rel="external">Blog: Disparity Estimation by Deep Learning</a></li><li><a href="http://oofx6tpf6.bkt.clouddn.com/Unsupervised%20Adaptation%20for%20Deep%20Stereo.pdf" target="_blank" rel="external">Paper: Unsupervised Adaptation for Deep Stereo</a></li><li><a href="https://github.com/CVLAB-Unibo/Unsupervised-Adaptation-for-Deep-Stereo" target="_blank" rel="external">Code: Unsupervised Adaptation for Deep Stereo</a></li><li><a href="http://blog.csdn.net/hysteric314/article/details/50529804" target="_blank" rel="external">Blog: 【论文学习】神经光流网络——用卷积网络实现光流预测（FlowNet: Learning Optical Flow with Convolutional Networks）</a></li><li><a href="http://blog.csdn.net/kongfy4307/article/details/75212800" target="_blank" rel="external">Blog: 论文阅读笔记之Dispnet</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/wonderwomanv3.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;note success&quot;&gt;&lt;p&gt;最近一段时间一直痴迷于如何将深度学习用于深度估计，看了不少关于该方面的介绍，再次做一个简单的总结。虽说&lt;code&gt;深度学习&lt;/code&gt;和&lt;code&gt;深度估计&lt;/code&gt;都有&lt;code&gt;深度&lt;/code&gt;二字，但是其意义确是完全不一样。一个是&lt;code&gt;deep&lt;/code&gt;一个是&lt;code&gt;depth&lt;/code&gt;，前者表示网络层纵向的延伸度，后者表示三维场景中物体距离摄像头的距离。这两个差异如此之大的名词是如何结合在一起的呢？且听我慢慢解释。&lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://www.vincentqin.tech/tags/Deep-Learning/"/>
    
      <category term="depth estimation" scheme="https://www.vincentqin.tech/tags/depth-estimation/"/>
    
      <category term="disparity" scheme="https://www.vincentqin.tech/tags/disparity/"/>
    
      <category term="DispNet" scheme="https://www.vincentqin.tech/tags/DispNet/"/>
    
  </entry>
  
  <entry>
    <title>深度学习在深度(视差)估计中的应用(1)</title>
    <link href="https://www.vincentqin.tech/posts/depth-estimation-using-deeplearning-1/"/>
    <id>https://www.vincentqin.tech/posts/depth-estimation-using-deeplearning-1/</id>
    <published>2017-12-06T15:56:45.000Z</published>
    <updated>2018-02-09T13:53:14.624Z</updated>
    
    <content type="html"><![CDATA[<!--![Kitti](http://oofx6tpf6.bkt.clouddn.com/17-12-7/11107720.jpg)--><p><img src="http://oofx6tpf6.bkt.clouddn.com/game-jaime.jpg" alt=""></p><div class="note "><p>本文对KITTI stereo 2015 datasets 冠军之作<a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w17/Pang_Cascade_Residual_Learning_ICCV_2017_paper.pdf" target="_blank" rel="external">Cascade Residual Learning: A Two-stage Convolutional Neural Network for Stereo Matching</a>进行简要解读。</p></div><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>目前深度学习发展的如火如荼，利用CNN可以将图像对的<strong>匹配问题</strong>看成一个学习问题。但是如何能够得到高质量的深度图像仍然是一个普世问题。本文作者提出了一种新型的层叠式（cascade）CNN结构（CRL:Cascade Residual Learning）去估计深度信息。深度估计的过程大致可以分成两个步骤：</p><ol><li>在现有的DispNet的基础上添加几个反卷积模块，目的是为了得到full resolution的初始的深度信息，同时能够学习到更多的细节信息；</li><li>第二步是对第一步中学习到的深度信息进行校准（rectify）；这一步利用到了第一步得到的多尺度的深度信息，然后并非是直接学习到优化后的深度信息，而是学习了每个尺度下的深度残差，然后结合第一步中得到的多尺度深度信息合成最终的深度图（这里有点类似于何凯明的residual的思想： It is easier to learn the residual than to learn the disparity directly）。</li></ol><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>下面详细的介绍下这个网络的结构:<br><img src=" http://oofx6tpf6.bkt.clouddn.com/17-12-6/38104630.jpg" )="" width="120%/"></p><p>可以很清楚地在上图中看到这两个不同的阶段。对于第一个阶段，类似于文献[1]中提到的<strong>DispNetC</strong>结构（C是correlation层的意思），本文作者同样采取了沙漏形的网络结构。但是<strong>DispNetC</strong>网络的输出图像的分辨率只有原始尺寸的一半！CRL中的<strong>DispFulNet</strong>在<strong>DispNetC</strong>的基础上，在最后的两个卷积层增加了添加了反卷积模块，然后再串联左图；通过再次添加一个额外的卷积层，可以使得网络输出为全分辨率（和左右图大小一致）。注意：每个尺度（共6个尺度）上的临时输出与其对应的ground truth之间计算$l_1$损失。<br>总结一下就是，这个<strong>DispFulNet</strong>学习了这样一个网络：通过输入一对图片$I_L$和$I_R$，学习到了视差$d_1$，使得：</p><script type="math/tex; mode=display">\tilde{I}_L(x,y)=I_R(x+d_1(x,y),y)</script><p>上式中的$\tilde{I}_L$就是把右图根据视差移动后的结果，我们的目标就是$\tilde{I}_L$越来越接近$I_L$。</p><p>接下来就是第二阶段，将$I_L$,$I_R$,$\tilde{I}_L$,$d_1$以及$e_L=|I_L-\tilde{I}_L|$串联起来[2]作为<strong>dispResNet</strong>的输入。此优化网络最后学到的是多尺度的残差$ \{r_2^{(s)} \} _s^S$，其中s=0时表示全尺度残差。最后与<strong>DispFulNet</strong>输出的多尺度深度图$\{d_1^{(s)}\}_s^S$做和运算得到最后优化后的深度$\{d_2^{(s)}\}_s^S$：</p><script type="math/tex; mode=display">d_2^{(s)}=d_1^{(s)}+r_2^{(s)},0 \leq s \leq S</script><p>于是$d_2^{(0)}$就是最后的全尺度输出。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>以下是对其结果展示：<br><img src="http://oofx6tpf6.bkt.clouddn.com/17-12-6/68873144.jpg" alt=""></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]. N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. <a href="https://arxiv.org/abs/1512.02134" target="_blank" rel="external"><strong>A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</strong></a>. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4040–4048, 2016.<br>[2]. E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. <a href="https://arxiv.org/abs/1612.01925" target="_blank" rel="external"><strong>Flownet 2.0: Evolution of optical flow estimation with deep networks</strong></a>. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2462–2470, 2017.<br>[3]. <a href="http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo" target="_blank" rel="external">KITTI: Stereo Evaluation 2015</a><br>[4]. <a href="https://github.com/Artifineuro/crl" target="_blank" rel="external">code: Cascade Residual Learning (CRL)</a></p>]]></content>
    
    <summary type="html">
    
      &lt;!--![Kitti](http://oofx6tpf6.bkt.clouddn.com/17-12-7/11107720.jpg)--&gt;
&lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/game-jaime.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;note &quot;&gt;&lt;p&gt;本文对KITTI stereo 2015 datasets 冠军之作&lt;a href=&quot;http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w17/Pang_Cascade_Residual_Learning_ICCV_2017_paper.pdf&quot;&gt;Cascade Residual Learning: A Two-stage Convolutional Neural Network for Stereo Matching&lt;/a&gt;进行简要解读。&lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="https://www.vincentqin.tech/tags/Deep-Learning/"/>
    
      <category term="depth estimation" scheme="https://www.vincentqin.tech/tags/depth-estimation/"/>
    
      <category term="disparity" scheme="https://www.vincentqin.tech/tags/disparity/"/>
    
      <category term="DispNet" scheme="https://www.vincentqin.tech/tags/DispNet/"/>
    
  </entry>
  
  <entry>
    <title>Matlab Deep Learning学习笔记</title>
    <link href="https://www.vincentqin.tech/posts/Matlab-Deep-Learning/"/>
    <id>https://www.vincentqin.tech/posts/Matlab-Deep-Learning/</id>
    <published>2017-11-18T17:22:40.000Z</published>
    <updated>2018-06-03T10:06:17.986Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/cifar10-fig.jpg" alt=""></p><div class="note "><p>最近对深度学习尤其着迷，是时候用万能的Matlab去践行我的DL学习之路了。之所以用Matlab，是因为Matlab真的太强大了！自从大学开始我就一直用这个神奇的软件，算是最熟悉的编程工具。加上最近mathworks公司一大波大佬的不懈努力，在今年下半年发行的R2017b版本中又加入了诸多新颖的<a href="https://cn.mathworks.com/products/new_products/latest_features.html?s_tid=hp_release_2017b&amp;from=timeline&amp;isappinstalled=0" target="_blank" rel="external">特性</a>，尤其在<a href="https://cn.mathworks.com/solutions/deep-learning.html" target="_blank" rel="external">DL</a>方面，可以发现：仅仅几条简单的代码，就能够实现复杂的功能。基于以上，我在本文列举了几个在Matlab上学习Deep Learning的例子：1. <a href="#example1">手写字符识别</a>；2. <a href="#example2">搭建网络对CIFAR10分类</a>；3.<a href="#example3">搭建一个Resnet</a>。务必保证主机已经安装Matlab 2017a及以上。</p></div><a id="more"></a><h2 id="手写字符识别"><a href="#手写字符识别" class="headerlink" title="手写字符识别"></a><span id="example1">手写字符识别</span></h2><p>利用CNN做数字分类实验。</p><p>接下来的实验会阐明如何进行：</p><ul><li>加载图像数据</li><li>设计网络结构</li><li>设置网络训练参数</li><li>训练网络</li><li>预测新数据的类别</li></ul><h3 id="加载图像数据"><a href="#加载图像数据" class="headerlink" title="加载图像数据"></a>加载图像数据</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">digitDatasetPath = fullfile(matlabroot,<span class="string">'toolbox'</span>,<span class="string">'nnet'</span>,<span class="string">'nndemos'</span>,...</div><div class="line">    <span class="string">'nndatasets'</span>,<span class="string">'DigitDataset'</span>);</div><div class="line"><span class="comment">% imageDatastore函数 能够通过文件夹名自动地把数据存储成ImageDatastore 对象</span></div><div class="line">digitData = imageDatastore(digitDatasetPath,...</div><div class="line">    <span class="string">'IncludeSubfolders'</span>,true,<span class="string">'LabelSource'</span>,<span class="string">'foldernames'</span>);</div><div class="line"></div><div class="line"><span class="comment">% Display some of the images in the datastore.</span></div><div class="line">figure;</div><div class="line">perm = randperm(<span class="number">10000</span>,<span class="number">25</span>);</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="number">25</span></div><div class="line">    subplot(<span class="number">5</span>,<span class="number">5</span>,<span class="built_in">i</span>);</div><div class="line">    imshow(digitData.Files&#123;perm(i)&#125;);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure><p>以下是手写字符的部分数据：</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/17-11-18/63361958.jpg" alt=""></p><h3 id="创建训练集与验证集"><a href="#创建训练集与验证集" class="headerlink" title="创建训练集与验证集"></a>创建训练集与验证集</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">trainNumFiles = <span class="number">750</span>;</div><div class="line">[trainDigitData,valDigitData] = splitEachLabel(digitData,<span class="number">750</span>,<span class="string">'randomize'</span>); </div><div class="line"><span class="comment">% 每类有1000个，选择其中的750类作为训练集，剩下的作为验证集；此处750可以换成一个比例：75%</span></div></pre></td></tr></table></figure><p>注意Matlab里面支持的<strong>层</strong>的类型，包括：<a href="https://cn.mathworks.com/help/nnet/ref/nnet.cnn.layer.layer.html?searchHighlight=softmaxlayer&amp;s_tid=doc_srchtitle" target="_blank" rel="external">CLICK THIS LINK</a>。如下所示：</p><div class="table-container"><table><thead><tr><th>Epoch</th><th>Iteration</th></tr></thead><tbody><tr><td>Layer Type</td><td>Function</td></tr><tr><td>Image input layer</td><td>imageInputLayer</td></tr><tr><td>Sequence input layer</td><td>sequenceInputLayer</td></tr><tr><td>2-D convolutional layer</td><td>convolution2dLayer</td></tr><tr><td>2-D transposed convolutional layer</td><td>transposedConv2dLayer</td></tr><tr><td>Fully connected layer</td><td>fullyConnectedLayer</td></tr><tr><td>Long short-term memory (LSTM) layer</td><td>LSTMLayer</td></tr><tr><td>Rectified linear unit (ReLU) layer</td><td>reluLayer</td></tr><tr><td>Leaky rectified linear unit (ReLU) layer</td><td>leakyReluLayer</td></tr><tr><td>Clipped rectified linear unit (ReLU) layer</td><td>clippedReluLayer</td></tr><tr><td>Batch normalization layer</td><td>batchNormalizationLayer</td></tr><tr><td>Channel-wise local response normalization (LRN) layer</td><td>crossChannelNormalizationLayer</td></tr><tr><td>Dropout layer</td><td>dropoutLayer</td></tr><tr><td>Addition layer</td><td>additionLayer</td></tr><tr><td>Depth concatenation layer</td><td>depthConcatenationLayer</td></tr><tr><td>Average pooling layer</td><td>averagePooling2dLayer</td></tr><tr><td>Max pooling layer</td><td>maxPooling2dLayer</td></tr><tr><td>Max unpooling layer</td><td>maxUnpooling2dLayer</td></tr><tr><td>Softmax layer</td><td>softmaxLayer</td></tr><tr><td>Classification layer</td><td>classificationLayer</td></tr><tr><td>Regression layer</td><td>regressionLayer</td></tr></tbody></table></div><h3 id="创建自己的网络结构"><a href="#创建自己的网络结构" class="headerlink" title="创建自己的网络结构"></a>创建自己的网络结构</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="comment">%% Define Network Architecture</span></div><div class="line"><span class="comment">% Define the convolutional neural network architecture.</span></div><div class="line">layers = </div><div class="line">    [</div><div class="line">    imageInputLayer([<span class="number">28</span> <span class="number">28</span> <span class="number">1</span>])</div><div class="line">    convolution2dLayer(<span class="number">3</span>,<span class="number">16</span>,<span class="string">'Padding'</span>,<span class="number">1</span>)</div><div class="line">    batchNormalizationLayer()</div><div class="line">    reluLayer()</div><div class="line">    maxPooling2dLayer(<span class="number">2</span>,<span class="string">'Stride'</span>,<span class="number">2</span>)</div><div class="line">    </div><div class="line">    convolution2dLayer(<span class="number">3</span>,<span class="number">32</span>,<span class="string">'Padding'</span>,<span class="number">1</span>)</div><div class="line">    batchNormalizationLayer()</div><div class="line">    reluLayer()</div><div class="line">    </div><div class="line">    maxPooling2dLayer(<span class="number">2</span>,<span class="string">'Stride'</span>,<span class="number">2</span>)</div><div class="line">    </div><div class="line">    convolution2dLayer(<span class="number">3</span>,<span class="number">64</span>,<span class="string">'Padding'</span>,<span class="number">1</span>)</div><div class="line">    batchNormalizationLayer()</div><div class="line">    reluLayer()</div><div class="line">    </div><div class="line">    fullyConnectedLayer(<span class="number">10</span>)</div><div class="line">    softmaxLayer()</div><div class="line">    classificationLayer()</div><div class="line">    ];</div></pre></td></tr></table></figure><p>以下就是该网络结构及参数设置：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"> <span class="number">1</span>   <span class="string">''</span>   Image Input             <span class="number">28</span>x28x1 images <span class="keyword">with</span> <span class="string">'zerocenter'</span> normalization</div><div class="line"> <span class="number">2</span>   <span class="string">''</span>   Convolution             <span class="number">16</span> <span class="number">3</span>x3 convolutions <span class="keyword">with</span> stride [<span class="number">1</span>  <span class="number">1</span>] and padding [<span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>]</div><div class="line"> <span class="number">3</span>   <span class="string">''</span>   Batch Normalization     Batch normalization</div><div class="line"> <span class="number">4</span>   <span class="string">''</span>   ReLU                    ReLU</div><div class="line"> <span class="number">5</span>   <span class="string">''</span>   Max Pooling             <span class="number">2</span>x2 max pooling <span class="keyword">with</span> stride [<span class="number">2</span>  <span class="number">2</span>] and padding [<span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>]</div><div class="line"> <span class="number">6</span>   <span class="string">''</span>   Convolution             <span class="number">32</span> <span class="number">3</span>x3 convolutions <span class="keyword">with</span> stride [<span class="number">1</span>  <span class="number">1</span>] and padding [<span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>]</div><div class="line"> <span class="number">7</span>   <span class="string">''</span>   Batch Normalization     Batch normalization</div><div class="line"> <span class="number">8</span>   <span class="string">''</span>   ReLU                    ReLU</div><div class="line"> <span class="number">9</span>   <span class="string">''</span>   Max Pooling             <span class="number">2</span>x2 max pooling <span class="keyword">with</span> stride [<span class="number">2</span>  <span class="number">2</span>] and padding [<span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>]</div><div class="line"><span class="number">10</span>   <span class="string">''</span>   Convolution             <span class="number">64</span> <span class="number">3</span>x3 convolutions <span class="keyword">with</span> stride [<span class="number">1</span>  <span class="number">1</span>] and padding [<span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>]</div><div class="line"><span class="number">11</span>   <span class="string">''</span>   Batch Normalization     Batch normalization</div><div class="line"><span class="number">12</span>   <span class="string">''</span>   ReLU                    ReLU</div><div class="line"><span class="number">13</span>   <span class="string">''</span>   Fully Connected         <span class="number">10</span> fully connected layer</div><div class="line"><span class="number">14</span>   <span class="string">''</span>   Softmax                 softmax</div><div class="line"><span class="number">15</span>   <span class="string">''</span>   Classification Output   crossentropyex</div></pre></td></tr></table></figure><h3 id="网络训练参数设计"><a href="#网络训练参数设计" class="headerlink" title="网络训练参数设计"></a>网络训练参数设计</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"> options = trainingOptions(<span class="string">'sgdm'</span>,...</div><div class="line"><span class="string">'MaxEpochs'</span>,<span class="number">3</span>, ...                 <span class="comment">% 训练最大轮回</span></div><div class="line"><span class="string">'ValidationData'</span>,valDigitData,...  <span class="comment">% 验证集</span></div><div class="line"><span class="string">'ValidationFrequency'</span>,<span class="number">30</span>,...</div><div class="line"><span class="string">'Verbose'</span>,false,...</div><div class="line"><span class="string">'Plots'</span>,<span class="string">'training-progress'</span>);</div></pre></td></tr></table></figure><h3 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">net = trainNetwork(trainDigitData,layers,options);</div></pre></td></tr></table></figure><h3 id="测试新的数据"><a href="#测试新的数据" class="headerlink" title="测试新的数据"></a>测试新的数据</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">predictedLabels = classify(net,valDigitData);</div><div class="line">valLabels = valDigitData.Labels;</div><div class="line">accuracy = sum(predictedLabels == valLabels)/<span class="built_in">numel</span>(valLabels)</div></pre></td></tr></table></figure><h3 id="查看某层参数"><a href="#查看某层参数" class="headerlink" title="查看某层参数"></a>查看某层参数</h3><p>例如查看第2层的weight参数，输入以下命令：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">montage(imresize(mat2gray(net.Layers(<span class="number">2</span>).Weights),[<span class="number">128</span> <span class="number">128</span>]));</div><div class="line">set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]); </div><div class="line">frame=getframe(gcf); <span class="comment">% get the frame</span></div><div class="line">image=frame.cdata;</div><div class="line">[image,map]     =  rgb2ind(image,<span class="number">256</span>);  </div><div class="line">imwrite(image,map,<span class="string">'weight-layer2.png'</span>);</div></pre></td></tr></table></figure></p><p>图像如下所示：<br><img src="http://oofx6tpf6.bkt.clouddn.com/weight-layer2.png" alt=""></p><p>再看一下第10层的参数：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">[~,~,iter,~]=<span class="built_in">size</span>(net.Layers(<span class="number">10</span>).Weights);</div><div class="line">name=<span class="string">'weight.gif'</span>;</div><div class="line">dt=<span class="number">0.4</span>;</div><div class="line"><span class="keyword">for</span> <span class="built_in">i</span>=<span class="number">1</span>:iter</div><div class="line">montage(imresize(mat2gray(net.Layers(<span class="number">10</span>).Weights(:,:,<span class="built_in">i</span>,:)),[<span class="number">128</span> <span class="number">128</span>]));</div><div class="line">    set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]); <span class="comment">%变白</span></div><div class="line">title([<span class="string">'Layer(10), Channel: '</span>,num2str(i)]);</div><div class="line">axis normal</div><div class="line">truesize</div><div class="line"><span class="comment">%Creat GIF</span></div><div class="line">frame(<span class="built_in">i</span>)=getframe(gcf); <span class="comment">% get the frame</span></div><div class="line">image=frame(<span class="built_in">i</span>).cdata;</div><div class="line">[image,map]     =  rgb2ind(image,<span class="number">256</span>);  </div><div class="line"><span class="keyword">if</span> <span class="built_in">i</span>==<span class="number">1</span></div><div class="line"> imwrite(image,map,name,<span class="string">'gif'</span>);</div><div class="line"><span class="keyword">else</span></div><div class="line"> imwrite(image,map,name,<span class="string">'gif'</span>,<span class="string">'WriteMode'</span>,<span class="string">'append'</span>,<span class="string">'DelayTime'</span>,dt);</div><div class="line">    <span class="keyword">end</span></div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure></p><p><img src="http://oofx6tpf6.bkt.clouddn.com/weight-inf.gif" alt=""></p><h2 id="搭建网络对CIFAR10分类"><a href="#搭建网络对CIFAR10分类" class="headerlink" title="搭建网络对CIFAR10分类"></a><span id="example2">搭建网络对CIFAR10分类</span></h2><p>CIFAR10和CIFAR100是<a href="http://groups.csail.mit.edu/vision/TinyImages/" target="_blank" rel="external">80 million tiny images</a>的子集，是由Geoffrey Hinton的弟子们Alex Krizhevsky和Vinod Nair共同采集。</p><h3 id="CIFAR10"><a href="#CIFAR10" class="headerlink" title="CIFAR10"></a><a href="http://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="external">CIFAR10</a></h3><p>CIFAR10由60000张32*32的彩色图像组成，一种分成10类，平均每类图像6000张。共有50000张训练图像，10000张测试图像。这个数据集被分成了5个分支，其中每个分支10000张。测试集包含每类中随机选择的1000张图像。训练集就是剩下的那些图像。<br>对于每个分支的数据的大小是：10000*3072；其中3072=32*32*3。数据以行优先的顺序存储，所以前1024个数据是r通道的数据，接下来的1024个数据是g通道的数据，最后1024个数据是b通道的。<br>假如原始的数据是data，我们想要将其重新排列成我们需要的数据。首先对其进行转置，然后再用reshape函数对图像重组（可选：最后将图像前两维互换（转置），之所以这么做，可以更好的可视化）。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">XBatch = data';</div><div class="line">XBatch = <span class="built_in">reshape</span>(XBatch, <span class="number">32</span>,<span class="number">32</span>,<span class="number">3</span>,[]);</div><div class="line">XBatch = <span class="built_in">permute</span>(XBatch, [<span class="number">2</span> <span class="number">1</span> <span class="number">3</span> <span class="number">4</span>]);</div></pre></td></tr></table></figure><p>以下是cifar10的部分数据。<br><img src="http://oofx6tpf6.bkt.clouddn.com/cifar10-images.png" alt=""><br>共有10类，包括：airplane，automobile，bird，cat，deer，dog，frog，horse，ship，truck。</p><h3 id="Just-run-it"><a href="#Just-run-it" class="headerlink" title="Just run it"></a>Just run it</h3><p>接下来我们就开始运行以下代码，来训练我们的网络。闲话少说，我把代码放在了<a href="https://github.com/Vincentqyw/DeepLearning/blob/master/demo_cifar10.m" target="_blank" rel="external">Github</a>，欢迎$star$。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="number">1</span>   <span class="string">'imageinput'</span>    Image Input         <span class="number">28</span>x28x1 images <span class="keyword">with</span> <span class="string">'zerocenter'</span> normalization</div><div class="line"><span class="number">2</span>   <span class="string">'conv_1'</span>        Convolution         <span class="number">16</span> <span class="number">3</span>x3x1 convolutions <span class="keyword">with</span> stride [<span class="number">1</span>  <span class="number">1</span>] and padding [<span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>]</div><div class="line"><span class="number">3</span>   <span class="string">'batchnorm_1'</span>   Batch Normalization Batch normalization <span class="keyword">with</span> <span class="number">16</span> channels</div><div class="line"><span class="number">4</span>   <span class="string">'relu_1'</span>        ReLU               ReLU</div><div class="line"><span class="number">5</span>   <span class="string">'maxpool_1'</span>     Max Pooling         <span class="number">2</span>x2 max pooling <span class="keyword">with</span> stride [<span class="number">2</span>  <span class="number">2</span>] and padding [<span class="number">0</span> <span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span>]</div><div class="line"><span class="number">6</span>   <span class="string">'conv_2'</span>        Convolution         <span class="number">32</span> <span class="number">3</span>x3x16 convolutions <span class="keyword">with</span> stride [<span class="number">1</span>  <span class="number">1</span>] and padding [<span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>]</div><div class="line"><span class="number">7</span>   <span class="string">'batchnorm_2'</span>   Batch Normalization Batch normalization <span class="keyword">with</span> <span class="number">32</span> channels</div><div class="line"><span class="number">8</span>   <span class="string">'relu_2'</span>        ReLU                ReLU</div><div class="line"><span class="number">9</span>   <span class="string">'maxpool_2'</span>     Max Pooling         <span class="number">2</span>x2 max pooling <span class="keyword">with</span> stride [<span class="number">2</span>  <span class="number">2</span>] and padding [<span class="number">0</span>  <span class="number">0</span>  <span class="number">0</span> <span class="number">0</span>]</div><div class="line"><span class="number">10</span>   <span class="string">'conv_3'</span>       Convolution         <span class="number">64</span> <span class="number">3</span>x3x32 convolutions <span class="keyword">with</span> stride [<span class="number">1</span>  <span class="number">1</span>] and padding [<span class="number">1</span> <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span>]</div><div class="line"><span class="number">11</span>   <span class="string">'batchnorm_3'</span>  Batch Normalization Batch normalization <span class="keyword">with</span> <span class="number">64</span> channels</div><div class="line"><span class="number">12</span>   <span class="string">'relu_3'</span>       ReLUReLU</div><div class="line"><span class="number">13</span>   <span class="string">'fc'</span>           Fully Connected<span class="number">10</span> fully connected layer</div><div class="line"><span class="number">14</span>   <span class="string">'softmax'</span>      Softmaxsoftmax</div><div class="line"><span class="number">15</span>   <span class="string">'classoutput'</span>  Classification Outputcrossentropyex <span class="keyword">with</span> <span class="string">'0'</span>, <span class="string">'1'</span>, and <span class="number">8</span> other classes</div></pre></td></tr></table></figure><p>以下是训练过程输出：</p><div class="table-container"><table><thead><tr><th>Epoch</th><th>Iteration</th><th style="text-align:center">Time Elapsed (seconds)</th><th style="text-align:center">Mini-batch Loss</th><th style="text-align:center">Mini-batch Accuracy</th><th style="text-align:center">Base Learning Rate</th></tr></thead><tbody><tr><td>1</td><td>1</td><td style="text-align:center">0.06</td><td style="text-align:center">2.3026</td><td style="text-align:center">8.59%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>50</td><td style="text-align:center">1.27</td><td style="text-align:center">2.3026</td><td style="text-align:center">14.06%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>100</td><td style="text-align:center">2.52</td><td style="text-align:center">2.3024</td><td style="text-align:center">7.81%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>150</td><td style="text-align:center">3.73</td><td style="text-align:center">2.2999</td><td style="text-align:center">20.31%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>200</td><td style="text-align:center">5.01</td><td style="text-align:center">2.2740</td><td style="text-align:center">15.63%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>250</td><td style="text-align:center">6.28</td><td style="text-align:center">2.1194</td><td style="text-align:center">21.09%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>300</td><td style="text-align:center">7.58</td><td style="text-align:center">1.9100</td><td style="text-align:center">23.44%</td><td style="text-align:center">0.0020</td></tr><tr><td>1</td><td>350</td><td style="text-align:center">8.86</td><td style="text-align:center">1.8892</td><td style="text-align:center">28.13%</td><td style="text-align:center">0.0020</td></tr><tr><td>2</td><td>400</td><td style="text-align:center">10.08</td><td style="text-align:center">1.7490</td><td style="text-align:center">29.69%</td><td style="text-align:center">0.0020</td></tr><tr><td>2</td><td>450</td><td style="text-align:center">11.32</td><td style="text-align:center">1.8377</td><td style="text-align:center">31.25%</td><td style="text-align:center">0.0020</td></tr><tr><td>2</td><td>500</td><td style="text-align:center">12.57</td><td style="text-align:center">1.6073</td><td style="text-align:center">39.84%</td><td style="text-align:center">0.0020</td></tr><tr><td>…</td><td>…</td><td style="text-align:center">…</td><td style="text-align:center">…</td><td style="text-align:center">…</td><td style="text-align:center">…</td></tr><tr><td>20</td><td>7650</td><td style="text-align:center">407.74</td><td style="text-align:center">0.2858</td><td style="text-align:center">93.75%</td><td style="text-align:center">2.00e-05</td></tr><tr><td>20</td><td>7700</td><td style="text-align:center">409.06</td><td style="text-align:center">0.3127</td><td style="text-align:center">89.84%</td><td style="text-align:center">2.00e-05</td></tr><tr><td>20</td><td>7750</td><td style="text-align:center">410.38</td><td style="text-align:center">0.3254</td><td style="text-align:center">87.50%</td><td style="text-align:center">2.00e-05</td></tr><tr><td>20</td><td>7800</td><td style="text-align:center">411.64</td><td style="text-align:center">0.2456</td><td style="text-align:center">92.19%</td><td style="text-align:center">2.00e-05</td></tr></tbody></table></div><p>最后测试我们的模型的性能，accuracy=76%左右。但是训练时，我们的batch-accuracy已经达到了90%以上，说明我们的<strong>模型过拟合</strong>了。显然这不是我们想要的结果，进一步的调参将会在此补充。</p><h3 id="可视化某层的参数"><a href="#可视化某层的参数" class="headerlink" title="可视化某层的参数"></a>可视化某层的参数</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">% Extract the first convolutional layer weights</span></div><div class="line">w = cifar10Net.Layers(<span class="number">2</span>).Weights;</div><div class="line"></div><div class="line"><span class="comment">% rescale and resize the weights for better visualization</span></div><div class="line">w = mat2gray(w);</div><div class="line">w = imresize(w, [<span class="number">100</span> <span class="number">100</span>]);</div><div class="line"></div><div class="line">figure</div><div class="line">montage(w)</div><div class="line">name=<span class="string">'cifar10-weight-layer2'</span>;</div><div class="line">set(gcf,<span class="string">'color'</span>,[<span class="number">1</span> <span class="number">1</span> <span class="number">1</span>]);</div><div class="line">frame=getframe(gcf); <span class="comment">% get the frame</span></div><div class="line">image=frame.cdata;</div><div class="line">[image,map]     =  rgb2ind(image,<span class="number">256</span>);  </div><div class="line">imwrite(image,map,[name,<span class="string">'.png'</span>]);</div></pre></td></tr></table></figure><p><img src="http://oofx6tpf6.bkt.clouddn.com/cifar10-weight-layer2.png" alt=""></p><h2 id="搭建一个Resnet"><a href="#搭建一个Resnet" class="headerlink" title="搭建一个Resnet"></a><span id="example3">搭建一个Resnet</span></h2><p>接下来，为了验证下这个DL工具包的强大之处，我打算纯手工建一个Resnet。为方便起见，我搭了一个Resnet34（更深的网络敬请期待吧）。这里是它的<a href="./resnet34.prototxt">prototxt</a>，我们可以用<a href="http://ethereon.github.io/netscope/#/editor" target="_blank" rel="external">网络可视化工具</a>进行查看resnet34的结构。以下是Resnet34的一部分（太长了没有截下全部视图）。<br><img src="http://oofx6tpf6.bkt.clouddn.com/resnet34.png" alt=""></p><h3 id="定义每一层与连接层"><a href="#定义每一层与连接层" class="headerlink" title="定义每一层与连接层"></a>定义每一层与连接层</h3><p>以从pool1到res2a为例子建立网络。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">layers_example=[</div><div class="line">    % pool1 - res2a</div><div class="line">    maxPooling2dLayer(<span class="number">3</span>, <span class="string">'Stride'</span>, <span class="number">2</span>,<span class="string">'Name'</span>,<span class="string">'pool1'</span>);</div><div class="line">    %  branch2a </div><div class="line">    convolution2dLayer(<span class="number">3</span>,<span class="number">64</span>,<span class="string">'Stride'</span>, <span class="number">1</span>,<span class="string">'Padding'</span>, <span class="number">1</span>,<span class="string">'Name'</span>,<span class="string">'res2a_branch2a'</span>)</div><div class="line">    batchNormalizationLayer(<span class="string">'Name'</span>,<span class="string">'bn2a_branch2a'</span>)</div><div class="line">    reluLayer(<span class="string">'Name'</span>,<span class="string">'res2a_branch2a_relu'</span>)</div><div class="line"></div><div class="line">    %  branch2b</div><div class="line">    convolution2dLayer(<span class="number">3</span>,<span class="number">64</span>,<span class="string">'Stride'</span>, <span class="number">1</span>,<span class="string">'Padding'</span>, <span class="number">1</span>,<span class="string">'Name'</span>,<span class="string">'res2a_branch2b'</span>)</div><div class="line">    batchNormalizationLayer(<span class="string">'Name'</span>,<span class="string">'bn2a_branch2b'</span>)</div><div class="line"></div><div class="line">    % add together</div><div class="line">    additionLayer(<span class="number">2</span>,<span class="string">'Name'</span>,<span class="string">'res2a'</span>)</div><div class="line">    reluLayer(<span class="string">'Name'</span>,<span class="string">'res2a_relu'</span>)</div><div class="line">];</div></pre></td></tr></table></figure><p>上述过程仅仅完成了网络的一个小分支，记下来要完成<code>res2a_branch1</code>这部分的连接。这时候要用到<strong>DAG</strong>的<a href="https://cn.mathworks.com/help/nnet/ref/dagnetwork.html" target="_blank" rel="external">一些方法</a>。通过添加新层同时建立新的连接即可，方式如下。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">lgraph = layerGraph(layers_example);</div><div class="line">figure</div><div class="line">plot(lgraph)</div><div class="line"></div><div class="line"><span class="comment">%% add some connections (shortcut)</span></div><div class="line">layers_2a=[</div><div class="line">    convolution2dLayer(<span class="number">1</span>,<span class="number">64</span>,<span class="string">'Stride'</span>, <span class="number">1</span>,<span class="string">'Padding'</span>, <span class="number">0</span>,<span class="string">'Name'</span>,<span class="string">'res2a_branch1'</span>)</div><div class="line">    batchNormalizationLayer(<span class="string">'Name'</span>,<span class="string">'bn2a_branch1'</span>)</div><div class="line">];</div><div class="line">lgraph = addLayers(lgraph,layers_2a);</div><div class="line">lgraph = connectLayers(lgraph,<span class="string">'pool1'</span>,<span class="string">'res2a_branch1'</span>);</div><div class="line">lgraph = connectLayers(lgraph,<span class="string">'bn2a_branch1'</span>,<span class="string">'res2a/in2'</span>);</div><div class="line"><span class="comment">% show net</span></div><div class="line">plot(lgraph)</div></pre></td></tr></table></figure><p>其他部分的构建同上，经过一系列重复的工作，我们可以构建出这个不太深的Resnet34，全部代码见我的<a href="https://github.com/Vincentqyw/DeepLearning" target="_blank" rel="external">Github</a>。</p><h2 id="一些基本问题"><a href="#一些基本问题" class="headerlink" title="一些基本问题"></a>一些基本问题</h2><ul><li>参数的基本格式</li></ul><script type="math/tex; mode=display">Height \times Width \times (\#Channels) \times (\#Filters)</script><ul><li>SGD是什么？<br>可以参见好友写的一篇<a href="https://sttomato.github.io/2017/08/13/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/#随机梯度下降" target="_blank" rel="external">博文</a>。</li><li>什么是epoch？<br>模型训练的时候一般采用stochastic gradient descent（SGD），一次迭代选取一个batch进行update。一个epoch的意思就是迭代次数*batch的数目 和训练数据的个数一样，就是一个epoch。</li><li>为什么要是用BN？<br>Batch normalization layers normalize the activations and gradients propagating through a network, making network training an easier optimization problem. Use batch normalization layers between convolutional layers and nonlinearities, such as ReLU layers, to speed up network training and reduce the sensitivity to network initialization.</li><li>RELU的作用？<br><em>Max-Pooling Layer</em> Convolutional layers (with activation functions) are sometimes followed by a down-sampling operation that reduces the spatial size of the feature map and removes redundant spatial information. Down-sampling makes it possible to increase the number of filters in deeper convolutional layers without increasing the required amount of computation per layer. One way of down-sampling is using a max pooling. The max pooling layer returns the maximum values of rectangular regions of inputs.</li><li><p><em>add more</em></p></li><li><p>Resnet中scale层是如何定义的？有什么用途？ </p></li><li><p>Resnet中为何残差$F(x)$比$H(x)$好学？ </p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/cifar10-fig.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;note &quot;&gt;&lt;p&gt;最近对深度学习尤其着迷，是时候用万能的Matlab去践行我的DL学习之路了。之所以用Matlab，是因为Matlab真的太强大了！自从大学开始我就一直用这个神奇的软件，算是最熟悉的编程工具。加上最近mathworks公司一大波大佬的不懈努力，在今年下半年发行的R2017b版本中又加入了诸多新颖的&lt;a href=&quot;https://cn.mathworks.com/products/new_products/latest_features.html?s_tid=hp_release_2017b&amp;amp;from=timeline&amp;amp;isappinstalled=0&quot;&gt;特性&lt;/a&gt;，尤其在&lt;a href=&quot;https://cn.mathworks.com/solutions/deep-learning.html&quot;&gt;DL&lt;/a&gt;方面，可以发现：仅仅几条简单的代码，就能够实现复杂的功能。基于以上，我在本文列举了几个在Matlab上学习Deep Learning的例子：1. &lt;a href=&quot;#example1&quot;&gt;手写字符识别&lt;/a&gt;；2. &lt;a href=&quot;#example2&quot;&gt;搭建网络对CIFAR10分类&lt;/a&gt;；3.&lt;a href=&quot;#example3&quot;&gt;搭建一个Resnet&lt;/a&gt;。务必保证主机已经安装Matlab 2017a及以上。&lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="CNN" scheme="https://www.vincentqin.tech/tags/CNN/"/>
    
      <category term="CIFAR10" scheme="https://www.vincentqin.tech/tags/CIFAR10/"/>
    
      <category term="Deep Learning" scheme="https://www.vincentqin.tech/tags/Deep-Learning/"/>
    
      <category term="Matlab" scheme="https://www.vincentqin.tech/tags/Matlab/"/>
    
  </entry>
  
  <entry>
    <title>CNN框架(CNN Architectures)</title>
    <link href="https://www.vincentqin.tech/posts/CNN-Architectures/"/>
    <id>https://www.vincentqin.tech/posts/CNN-Architectures/</id>
    <published>2017-11-06T16:24:45.000Z</published>
    <updated>2017-11-15T08:06:53.245Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://oofx6tpf6.bkt.clouddn.com/17-11-7/9721733.jpg" width="75%" dense-net=""></center><div class="note "><p>本文来自于CS231N（2017 Spring），将介绍几种较为常见的CNN结构。以下网络均是ImageNet比赛的冠军之作，我们将从网络结构，参数规模，运算量等来描述各个网络的特点。</p></div><a id="more"></a><ul><li>AlexNet</li><li>VGG</li><li>GoogLeNet</li><li>ResNet</li></ul><p>后续将补充以下几种网络：</p><ul><li>NiN(Network in Network)</li><li>wide ResNet</li><li>ResNeXT</li><li>stochastic Depth</li><li><a href="https://github.com/liuzhuang13/DenseNet" target="_blank" rel="external">DenseNet</a></li><li>FractalNet</li><li>SqueezeNet</li></ul><p>以下是正文。</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/17-10-24/99013631.jpg" alt="AlexNet"><br>网络的输入大小为：227*227*3，每一层的结构以及参数设置如下：</p><div class="table-container"><table><thead><tr><th style="text-align:left">Layer Type</th><th style="text-align:left">#Filters</th><th style="text-align:left">Stride</th><th style="text-align:left">Pading</th><th style="text-align:left">OUTPUT SIZE</th><th style="text-align:left">Parameters</th></tr></thead><tbody><tr><td style="text-align:left">CONV1</td><td style="text-align:left">#96 @11*11</td><td style="text-align:left">4</td><td style="text-align:left">0</td><td style="text-align:left">55*55*96</td><td style="text-align:left">11*11*3*96</td></tr><tr><td style="text-align:left">MAXPOOL1</td><td style="text-align:left">3*3</td><td style="text-align:left">2</td><td style="text-align:left">0</td><td style="text-align:left">27*27*96</td><td style="text-align:left">0</td></tr><tr><td style="text-align:left">NORM1</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left">27*27*96</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">CONV1</td><td style="text-align:left">#256 @5*5</td><td style="text-align:left">1</td><td style="text-align:left">2</td><td style="text-align:left">27*27*256</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">MAXPOOL2</td><td style="text-align:left">3*3</td><td style="text-align:left">2</td><td style="text-align:left">0</td><td style="text-align:left">13*13*256</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">NORM2</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left">13*13*256</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">CONV3</td><td style="text-align:left">#384 @3*3</td><td style="text-align:left">1</td><td style="text-align:left">1</td><td style="text-align:left">13*13*384</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">CONV4</td><td style="text-align:left">#384 @3*3</td><td style="text-align:left">1</td><td style="text-align:left">1</td><td style="text-align:left">13*13*384</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">CONV5</td><td style="text-align:left">#256 @3*3</td><td style="text-align:left">1</td><td style="text-align:left">1</td><td style="text-align:left">13*13*256</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">MAXPOOL3</td><td style="text-align:left">3*3</td><td style="text-align:left">2</td><td style="text-align:left">0</td><td style="text-align:left">6*6*256</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">FC6</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left">4096</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">FC7</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left">4096</td><td style="text-align:left">55*55*96</td></tr><tr><td style="text-align:left">FC8</td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left">1000</td><td style="text-align:left">55*55*96</td></tr></tbody></table></div><p>The size of output image is $\frac{N-Conv+2\times Pading}{stride}+1$</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/17-10-24/33818113.jpg" alt="AlexNet-details"></p><p><em>后续将使用<a href="https://cn.mathworks.com/help/nnet/examples.html?s_cid=doc_flyout#bvljehw" target="_blank" rel="external">Matlab DL 工具包</a>补充Alexnet实验…</em></p><h2 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h2><p>The winner of ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014.</p><h3 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h3><p><strong>small filters, deeper networks</strong>。<br>将原来8层的AlexNet扩展到了16&amp;19层。卷积层的大小仅仅有3*3，stride=1，pad=1；池化层仅仅有stride=2的2*2的MAXPOOL。以下是其与AlexNet的结构对比图。<br><img src="http://oofx6tpf6.bkt.clouddn.com/17-11-6/89475867.jpg" alt="VGG"></p><p>更加具体的，VGG16的网络的参数个数以及内存消耗如下：<br><img src="http://oofx6tpf6.bkt.clouddn.com/17-11-6/31257061.jpg" alt="VGG-details"></p><p>Q：为何采用更小的CONV？<br>A：几个3*3的CONV叠加后的接受域和一个7*7大小的CONV的接受域一致，但是与此同时，<strong>网络层数变深，引入了更多的非线性，参数数量更少</strong>。（Stack of three 3x3 conv (stride 1) layers has same effective receptive field as one 7x7 conv layer，But deeper, more non-linearities. And fewer parameters: $3\times3^2C^2$ vs. $7^2C^2$ for C channels per layer）</p><h3 id="更多细节"><a href="#更多细节" class="headerlink" title="更多细节"></a>更多细节</h3><ul><li>ILSVRC’14 2nd in classification, 1st in localization Similar training</li><li>procedure as Krizhevsky 2012 No Local Response Normalisation (LRN)</li><li>Use VGG16 or VGG19 (VGG19 only slightly better, more memory) </li><li>Use ensembles for best results </li><li>FC7 features generalize well to other tasks</li></ul><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p>论文地址：<a href="https://arxiv.org/pdf/1409.4842.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1409.4842.pdf</a><br>代码地址：NULL<br><strong>Deeper networks, with computational efficiency</strong>。GoogLeNet是ILSVRC’14的图像分类冠军网络，它加入了<strong>Inception</strong>模块，并且去除了全连接层，大大减少了参数的个数。</p><ul><li>22 layers (with weights)</li><li>Efficient “Inception” module</li><li>No FC layers</li><li>Only 5 million parameters! 12x less than AlexNet</li><li>ILSVRC’14 classification winner (6.7% top 5 error)</li></ul><h3 id="“Inception-module”"><a href="#“Inception-module”" class="headerlink" title="“Inception module”"></a>“Inception module”</h3><p>精心设计了一个局部网络模块，并且将这些模块叠加构成GoolgeNet。这种经过精心设计的模块就是Inception。（design a good local network topology (network within a network) and then stack these modules on top of each other）。<br>Inception包含几个接受域不同的CONV核（1*1，3*3，5*5）以及池化操作（3*3）；最终将这些操作后的输出在depth方向串联。以下是两种两种不同的实现方式，左图时原始的inception模块，右图是改进版的inception模块。<br><img src="http://oofx6tpf6.bkt.clouddn.com/17-11-6/33298717.jpg" alt="inception"><br>对于naive inception而言，它面临这运算量巨大的问题。由于池化层的输出会保留原始输入的depth，所以经过CONV&amp;MAXPOOL过后的输出的feature map势必比原始输入的depth更深。<br><img src="http://oofx6tpf6.bkt.clouddn.com/17-11-6/53626485.jpg" alt="inception-naive"><br>那么如何去解决以上问题呢，一个通常的方式就是降维。我们在每个CONV前加上1*1的CONV（“bottleneck” layers）来减少feature map的维度。所谓的1*1CONV就是在保持输入的空间分辨率不变的情况下来减小depth维度，即通过将不同depth上的feature map进行组合，从而将输入的feature map映射到更低的depth维度上。经过以上操作就可以将运算的操作次数大大降低。<br><img src="http://oofx6tpf6.bkt.clouddn.com/17-11-6/95592762.jpg" alt="inception-improve"></p><p>于是GoogLeNet的全貌如下：<br><img src="http://oofx6tpf6.bkt.clouddn.com/17-11-6/75689322.jpg" alt="googlenet"></p><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><p>利用<strong>残差</strong>连接成的超级深网络。<br>这里有一个何凯明在ICML2016的Tutorial，内容比较详细。<a href="http://kaiminghe.com/icml16tutorial/index.html" target="_blank" rel="external">ICML 2016 Tutorial on Deep Residual Networks</a><br>代码在这里<a href="https://github.com/KaimingHe/deep-residual-networks" target="_blank" rel="external">Code: deep-residual-networks</a></p><h3 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h3><ul><li>152-layer model for ImageNet</li><li>ILSVRC’ 15 classification winner (3.57% top 5 error)</li><li>Swept all classification and detection competitions in ILSVRC’ 15 and COCO’ 15!</li></ul><h3 id="深度增加带来的问题"><a href="#深度增加带来的问题" class="headerlink" title="深度增加带来的问题"></a>深度增加带来的问题</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/17-11-6/22279699.jpg" alt="deeper-nets-problems"><br>从上图可以发现，当网络层数增加时，训练误差和测试误差都有所下降。这并不符合以往的经验，我们会想，既然网络层数增加了，那么模型参数势必增多，此时会造成过拟合。然而过拟合的表现是：训练误差减小，测试误差增大。但是事实和分析并不吻合。<br>何凯明认为：<strong>The problem is an optimization problem, deeper models are harder to optimize</strong>。这是一个优化问题，更深的网络更难优化。并且，更深的网络应该至少比浅层网络不差，这是因为我们可以通过拷贝浅层网络+identity mapping（恒等映射）来构造一个更深的网络，这个结构化的方案表明深层网络可以达到和浅层网络一致的性能。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/17-11-6/63404628.jpg" alt="resnet-layer"><br>Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping.<br>作者假设：<strong>相较于最优化最初的无参照映射（残差函数以输入x作为参照），最优化残差映射是更容易的</strong>。利用网络去拟合残差$F(x)$，并非直接拟合$H(x)$。</p><h3 id="整个ResNet框架"><a href="#整个ResNet框架" class="headerlink" title="整个ResNet框架"></a>整个ResNet框架</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/17-11-6/82101608.jpg" alt="resnet-structure"></p><ul><li>Stack residual blocks</li><li>Every residual block has two 3x3 conv layers</li><li>Periodically, double # of filters and downsample spatially using stride 2 (/2 in each dimension)</li><li>Additional conv layer at the beginning</li><li>No FC layers at the end (only FC 1000 to output classes)</li></ul><p>对于ImageNet比赛而言，ResNet设置的网络深度有34、50、101以及152层。对于层数较多的网络，利用“bottleneck”（类似于GoogLeNet的1*1卷积操作）来提高效率。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>论文<a href="https://arxiv.org/pdf/1605.07678.pdf" target="_blank" rel="external">An Analysis of Deep Neural Network Models for Practical Applications</a> 比较了2016年以来的一些神经网络的规模、运算量、能耗以及精度等项目。<br><img src="http://oofx6tpf6.bkt.clouddn.com/17-11-6/40479799.jpg" alt="complexity-compare"><br>可以从上图总结出以下几点：</p><ul><li>GoogLeNet: most efficient</li><li>VGG: Highest memory, most operations</li><li>AlexNet: Smaller compute, still memory heavy, lower accuracy</li><li>ResNet: Moderate efficiency depending on model, highest accuracy</li></ul><h2 id="其他网络变体"><a href="#其他网络变体" class="headerlink" title="其他网络变体"></a>其他网络变体</h2><p>后续补充。</p><h2 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h2><ul><li><strong><font color="red">ResNet为何能够使网络层数更深，应如何正确理解残差网络？He是受何启发从而发明了这种结构？</font></strong></li><li>more questions will be added…</li></ul><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="http://deeplearning.net/" target="_blank" rel="external">DeepLearning.net</a></li><li><a href="http://deeplearning.net/reading-list/" target="_blank" rel="external">Reading List</a></li><li><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="external">ImageNet Classification with Deep Convolutional Neural Networks</a></li><li><a href="https://zhuanlan.zhihu.com/p/28124810" target="_blank" rel="external">为什么ResNet和DenseNet可以这么深？一文详解残差块为何有助于解决梯度弥散问题</a></li><li><a href="https://arxiv.org/pdf/1605.07678.pdf" target="_blank" rel="external">An Analysis of Deep Neural Network Models for Practical Applications</a></li><li><a href="http://cs231n.stanford.edu/" target="_blank" rel="external">CS231n: Convolutional Neural Networks for Visual Recognition</a></li><li><a href="https://arxiv.org/pdf/1608.06993.pdf" target="_blank" rel="external">Densely Connected Convolutional Networks</a></li><li><a href="http://kaiminghe.com/icml16tutorial/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf" target="_blank" rel="external">Deep Residual Networks (Deep Learning Gets Way Deeper)</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/17-11-7/9721733.jpg&quot; width=&quot;75%&quot; Dense-Net&gt;&lt;/center&gt;

&lt;div class=&quot;note &quot;&gt;&lt;p&gt;本文来自于CS231N（2017 Spring），将介绍几种较为常见的CNN结构。以下网络均是ImageNet比赛的冠军之作，我们将从网络结构，参数规模，运算量等来描述各个网络的特点。&lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="AlexNet" scheme="https://www.vincentqin.tech/tags/AlexNet/"/>
    
      <category term="VGG" scheme="https://www.vincentqin.tech/tags/VGG/"/>
    
      <category term="GoogLeNet" scheme="https://www.vincentqin.tech/tags/GoogLeNet/"/>
    
      <category term="ResNet" scheme="https://www.vincentqin.tech/tags/ResNet/"/>
    
  </entry>
  
  <entry>
    <title>理解LSTM网络【译】</title>
    <link href="https://www.vincentqin.tech/posts/lstm/"/>
    <id>https://www.vincentqin.tech/posts/lstm/</id>
    <published>2017-10-23T13:26:20.000Z</published>
    <updated>2018-06-03T09:45:01.809Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/honghong.jpg" alt=""></p><p>本文是我对大神<a href="http://colah.github.io/about.html" target="_blank" rel="external">Christopher Olah</a>的<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="external">Understanding LSTM Networks</a>的译文。<br><a id="more"></a></p><h2 id="循环神经网络（Recurrent-Neural-Networks）"><a href="#循环神经网络（Recurrent-Neural-Networks）" class="headerlink" title="循环神经网络（Recurrent Neural Networks）"></a>循环神经网络（Recurrent Neural Networks）</h2><p>人们并非每时每刻都在大脑一片空白时开始思考。当我们读这篇文章的时候，我们会根据之前学习的知识来理解当前你在阅读的内容。我们不是把原来的知识丢的一干二净来重新学习，我们的记忆有一定的<strong>持久性</strong>。<br>传统的神经网络做不到这些，这是它的一大缺陷。比如说，可以想象有这样一种情况，我们想知道一部电影的每一帧画面正在发生什么。使用传统的神经网络很难通过理解电影之前的画面来推断以后将要发生的事件。（传统的神经网络不能处理带有时序的样本）<br>循环神经网络（Recurrent Neural Networks）尝试解决了以上问题。这种网络是一种带有循环结构的网络，可以使得信息得以持久保持。</p><center><img src="http://oofx6tpf6.bkt.clouddn.com/RNN-rolled.png" width="15%" rnn有循环结构=""></center><p>上图是一个RNN模块，$A$读取输入$x_i$，同时输出$h_t$，$A$这个循环允许信息从网络的当前步骤传递到下一步骤。<br>上述过程把RNN的过程讲的有些神秘感。但是，如果我们仔细想想，这也不比一个正常的神经网络难以理解。一个RNN可以看成是多个相同网络的拷贝，每一个拷贝都会向后续网络传递信息，下图我们把RNN展开。<br><img src="http://oofx6tpf6.bkt.clouddn.com/RNN-unrolled.png" alt="展开RNN"><br>这种链式的特性揭示了RNN与序列和列表有关，RNN是对这些数据最自然的表达。RNN目前已经被广泛使用！在过去的几年间，RNN在很多领域都有着出色的表现：语音识别，语言建模，翻译，图像描述…推荐大家阅读大神Andrej Karpathy的博文<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="external"> The Unreasonable Effectiveness of Recurrent Neural Networks</a>，来领略下RNN的诸多应用，这简直不能太棒！<br>以上的成功案例离不开使用<strong>长短期记忆</strong>（LSTM: Long Short Term Memory）网络，这是一种特殊的RNN网络并且能够应对更多任务，相比于标准的RNN网络，它具有更为出色的表现。几乎所有的RNN都是基于LSTM来实现的，接下来我们讨论下LSTM的奥义。</p><h2 id="长期依赖（Long-Term-Dependencies）问题"><a href="#长期依赖（Long-Term-Dependencies）问题" class="headerlink" title="长期依赖（Long-Term Dependencies）问题"></a>长期依赖（Long-Term Dependencies）问题</h2><p>RNNs的要求之一就是它能够连接之前的信息到当前的任务之上，例如利用之前的视频帧来理解当前帧的内容。如果RNNs能够做到这一点的话，它将会超级有用。但是它真的可以吗？看情况而定。<br>有时，我们仅仅需要离当前任务最近的几个任务信息。例如，我们有一个<strong>语言模型</strong>，它的目标是根据当前已有的词语去预测接下来将要出现的词语。如果我们尝试去预测“the clouds are in the sky”中的最后一个单词，我们不需要任何更多的语料，很明显最后一个单词将会是“sky”。在这种情况下，相关信息和当前需要预测词的位置的间隔是非常小的，这时RNNs可以去利用过去的信息。<br><img src="http://oofx6tpf6.bkt.clouddn.com/RNN-shorttermdepdencies.png" alt="短期记忆"><br>但是也有一种情况是，我们需要更多的信息才能够做预测。例如我们的<strong>语言模型</strong>需要预测下面句子的最后一个单词“I grew up in France… I speak fluent French.”。从相邻近的几个单词可以推断最后一个单词可能是一种语言，但如果我们想要知道到底是哪种语言的话，我们需要句子最前面的一个单词“France”。这会使得相关信息以及需要预测词的位置的间隔变得很大。<br>遗憾的是，随着间隔的逐渐增大，RNNs不能够去关联有用的信息。<br><img src="http://oofx6tpf6.bkt.clouddn.com/RNN-longtermdependencies.png" alt="长期记忆"><br>在理论上，RNNs能够解决长期依赖的问题。人们可以通过仔细选取参数来解决这类问题。但是实际上RNNs并不能去解决这个问题。<a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="external"> Hochreiter (1991) [German] and Bengio(1994)</a>等人深入研究了该问题为何如此艰难。<br>阿弥陀佛，LSTMs并没有上述问题。</p><h2 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h2><p>长短期记忆网络——经常被叫做“LSTM”——是RNN的这一种特殊的形式，它能够解决长期依赖的问题。LSTM是由<a href="https://www.researchgate.net/publication/13853244_Long_Short-term_Memory" target="_blank" rel="external">Hochreiter &amp; Schmidhuber (1997)</a>提出，并由很多后来者完善以及推广。LSTM能够在很多问题上取得优秀的结果，现如今被广泛引用。<br>LSTM被设计成防止长期依赖问题的发生。在实践中，LSTM的长期记忆是默认行为，而并非艰苦习得！所有的RNN都有链式重复的神经网络模块。在标准的RNN中，这些重复的模块仅仅有简单的结构，例如$tanh$层。<br><img src="http://oofx6tpf6.bkt.clouddn.com/LSTM3-SimpleRNN.png" alt="标准RNN中重复的简单模块"><br>当然，LSTM中也存在这样的链式结构，但是其中的重复模块就大为不同了。LSTM的重复的模块中包含4种不同的层，它们以一种特殊的结构交错着。<br><img src="http://oofx6tpf6.bkt.clouddn.com/LSTM3-chain.png" alt="LSTM中重复的模块中包含4种不同层"><br>看不懂，不用担心，细节即将展开。接下来，我们会一步步来讲解LSTM的网络结构。首先我们先明确几个会经常用到的表示方法。<br><img src="http://oofx6tpf6.bkt.clouddn.com/LSTM2-notation.png" alt=""><br>在以上的图示中，每条实线传输着整个向量，从一个节点的输出到其他节点的输入。粉红色的圆圈表示的是<strong>逐点操作</strong>，例如向量的加法，黄色的方形表示已经学习了的网络层。汇集的线表示<strong>串联</strong>，分叉的线表示<strong>复制</strong>操作，这些复制的内容流向不同的位置。</p><h2 id="LSTM背后的核心技术"><a href="#LSTM背后的核心技术" class="headerlink" title="LSTM背后的核心技术"></a>LSTM背后的核心技术</h2><p>LSTM的关键技术在于细胞（cell）状态，也就是图表中最上方水平穿行的直线。细胞状态可以理解成是一种传送带。它仅仅以少量的线性相交，贯穿整个链式结构上方。信息沿着这条传动带很容易保持不变。<br><img src="http://oofx6tpf6.bkt.clouddn.com/LSTM3-C-line.png" alt=""><br>LSTM有一种能向细胞增加或者移除信息的能力，这种经过精心设计的结构称作<strong>门</strong>（gates）。所谓的门就是一种让信息选择性通过的方法。它是由一个$sigmoid$层和一个逐点乘法单元构成。如下图：</p><center><img src="http://oofx6tpf6.bkt.clouddn.com/LSTM3-gate.png" width="15%" rnn有循环结构=""></center><p>$sigmoid$层的输出是$0$-$1$之间的数字，它描述了每个部分可以有多少量能够通过。$0$代表“啥都不能通过”，$1$代表“啥都能通过”！一个LSTM有三种这样的门结构，用来保证以及控制细胞状态。</p><h2 id="逐步理解LSTM"><a href="#逐步理解LSTM" class="headerlink" title="逐步理解LSTM"></a>逐步理解LSTM</h2><p>LSTM的第一步是来决定<strong>啥信息将要从细胞状态中丢弃</strong>。这个决定是由一个叫做“遗忘门”（“forget gate layer”）的$sigmoid$层来决定。它的输入是$h_{t-1}$和$x_t$，输出是一个介于$0$到$1$之间数值，给每个在状态$C_{t-1}$的数值。$1$表示“完全保留”，$0$表示“完全丢弃”。<br>让我们回到之前的的语言模型的例子中，我们还是基于以前的词语来预测后续的单词。在这样一个问题中，细胞状态可能会包含当前主语的性别信息，所以正确的<strong>介词</strong>将会被使用。当我们看到一个新的主语时，我们要遗忘掉旧的的主语。<br><img src="http://oofx6tpf6.bkt.clouddn.com/LSTM3-focus-f.png" alt=""></p><p>下一步就是决定<strong>啥新信息将要存储在细胞状态中</strong>。这包括两个方面。第一，一个叫做“输入门层”的$sigmoid$层来决定哪些值我们要更新；第二，一个$tanh$层创造了新的候选值$\tilde{C}_t$，这个值将会加入到新的状态中去。进一步，我们要把上述两个方面结合起来来更新细胞状态。<br>在我们的语言模型中，我们想要在新的主语对应的细胞状态中加入性别信息，去代替我们遗忘掉的那个旧主语状态。<br><img src="http://oofx6tpf6.bkt.clouddn.com/LSTM3-focus-i.png" alt=""></p><p>我们现在更新旧的细胞状态，从状态$C_{t-1}$到状态$C_{t}$。上述步骤已经详述了具体如何操作，我们现在就开始更新！<br>我们将旧的细胞状态乘以$f_t$，目的是忘记我们要忘记的旧状态。然后我们加上$i_t*\tilde{C}_t$，这就是我们创造新的候选值，这个值根据我们想要更新每个状态值的程度进行伸缩变化（这就是$i_t$的意义）。<br>在我们的语言模型中，这就是我们根据最开始确定的目标，丢弃旧主语性别以及增加新主语信息的地方。<br><img src="http://oofx6tpf6.bkt.clouddn.com/LSTM3-focus-C.png" alt=""></p><p>最后一步我们要决定到底输出什么信息。这个输出信息会基于细胞状态，但将会是一个经过过滤后的结果。首先，我们用一个$sigmoid$层去决定细胞状态的哪一部分将会被输出。然后，我们将细胞状态通过$tanh$（将其值规范到$-1$到$1$之间）。最后我们将这个值与$sigmoid$输出相乘，这样我们仅仅输出我们想要输出的部分。<br>还是对于之前提到的语言模型，因为它只看到了一个主语，它可能会输出一个与动词相关的信息。例如，可能输出这个主语是单数还是复数，所以我们会知道紧跟的动词是何种形式。<br><img src="http://oofx6tpf6.bkt.clouddn.com/LSTM3-focus-o.png" alt=""><br>至此，基本的LSTM介绍完毕。</p><h2 id="LSTM的变体"><a href="#LSTM的变体" class="headerlink" title="LSTM的变体"></a>LSTM的变体</h2><p>我们以上描述的均是最为普通的LSTM。但是并不是所有的LSTM都是以上那个样子。事实上，似乎每一篇涉及LSTMs的论文均对其做了细微的修改。其中的差别不大，以下列举几种LSTM的变体。</p><p>其中之一就是一种特别流行的LSTM变体，它由<a href="ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf" target="_blank" rel="external"> Gers &amp; Schmidhuber (2000)</a>提出，加入一种<strong>窥视孔连接</strong>（peephole connections）的结构。这使得细胞状态可以作为门层（译者：gete layers:$sigmoid$ layers &amp; $tanh$ layer）输入。<br><img src="http://oofx6tpf6.bkt.clouddn.com/LSTM3-var-peepholes.png" alt=""><br>以上的图示为每个门层加入了窥视孔连接，但是也有一些论文并不是所有的门层都加。</p><p>另外一种变体是加入了双遗忘门（coupled forget）以及输入门。我们同时考虑了何时遗忘以及应该加入何种新信息，而并非分别考虑。我们仅仅在我们需要就地输入信息的时候才会遗忘，同时我们仅仅在遗忘掉旧的信息的时候才会加入新的信息（译者：此时$f_t=0$，表示遗忘旧的细胞状态，同时加入新的输入$\tilde{C}_t$）。<br><img src="http://oofx6tpf6.bkt.clouddn.com/LSTM3-var-tied.png" alt=""></p><p>另外一种改动较大的变体是门控循环单元（Gated Recurrent Unit）即GRU。这个算法由<a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="external">Cho,et al.(2014)</a>提出，它把遗忘门和输入门结合起来构成一个“更新门”（update gate）。与此同时，它还将细胞状态和隐含状态合并起来，当然还有一些其他变化在此不一一赘述。最终的变体比标准的LSTM简单，这使得它很受欢迎。<br><img src="http://oofx6tpf6.bkt.clouddn.com/LSTM3-var-GRU.png" alt=""></p><p>以上均是最近比较劲爆的LSTM变体。当然也有很多其他形式的变体，如<a href="http://arxiv.org/pdf/1508.03790v2.pdf" target="_blank" rel="external">Yao,et al. (2015)</a>提出的<strong>深度门RNN</strong>（Depth Gated RNNs）。还有一些变体用完全不同的方式来解决长期依赖问题，例如<a href="http://arxiv.org/pdf/1402.3511v1.pdf" target="_blank" rel="external">Koutnik,et al.(2014)</a>提出的<strong>时钟频率驱动RNN</strong>（Clockwork RNNs ）。</p><p>列举了诸多LSTM变体，那么到底哪一种变体是最好的呢？其中的差异真的很重要吗？<a href="http://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="external">Greff, et al.(2015)</a>做了一个非常棒了比较，发现这些变体几乎都是一样一样的。<a href="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="external">Jozefowicz,et al.(2015)</a>测试了上万种RNN框架，发现了一些框架在特定任务上会比LSTM表现出色。（译者：没有一种算法一统江湖）</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>以上，我提到了人们利用RNNs得到了很多优秀的结果。在本质上说，几乎所有的RNNs都使用了LSTMs。LSTMs在诸多任务上表现优异。<br>在介绍LSTMs的过程中写了很多公式，这让它看起来很吓人。幸运的是，我们在文中通过一步步地探索，让LSTM看起来平易近人。<br>LSTMs是我们完成RNNs的重大成果。我们很自然地想：还有没有其他的重大成果？研究员们的共识是：当然有！下一个重大成果就是——<strong>注意力</strong>。这个观点是让RNNs的每一步都能够从更大的数据集中挑选信息。例如，如果你想利用RNNs去给一幅图像创造标题来描述它，这就可能会选择图像的一部分作为输入，然后根据这些输入来得到每个单词。事实上，<a href="http://arxiv.org/pdf/1502.03044v2.pdf" target="_blank" rel="external">Xu,et al.(2015)</a>就是这么做的——这可能你探究<strong>注意力</strong>这个领域的起点。还有诸多使用<strong>注意力</strong>取得的令人激动的研究结果，看起来还有更多需要探索。<br><strong>注意力</strong>并非RNN唯一令人激动的研究方向。例如，<a href="http://arxiv.org/pdf/1507.01526v1.pdf" target="_blank" rel="external">Kalchbrenner, et al. (2015)</a>提出的网格LSTM（Grid LSTMs）看似非常有前景。<a href="http://arxiv.org/pdf/1502.04623.pdf" target="_blank" rel="external">Gregor, et al.(2015)</a>，<a href="http://arxiv.org/pdf/1506.02216v3.pdf" target="_blank" rel="external">Chung, et al.(2015)</a>和<a href="http://arxiv.org/pdf/1411.7610v3.pdf" target="_blank" rel="external"> Bayer &amp; Osendorfer (2015)</a>等人的研究工作是在生成模型中使用RNNs，这些工作都看起来非常有趣。过去几年是RNNs异常火爆的时期，未来也会有更多更加有意义的成果出现。</p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>我非常感谢那些帮助我去理解LSTMs的大佬们，同时感谢对可视化进行评论并在这篇博文提供反馈的网友们。非常感激谷歌同事们的反馈，尤其感谢<a href="http://research.google.com/pubs/OriolVinyals.html" target="_blank" rel="external">Oriol Vinyals</a>，<a href="http://research.google.com/pubs/GregCorrado.html" target="_blank" rel="external">Greg Corrado</a>，<a href="http://research.google.com/pubs/JonathonShlens.html" target="_blank" rel="external">Jon Shlens</a>，<a href="http://people.cs.umass.edu/~luke/" target="_blank" rel="external">Luke Vilnis</a>以及<a href="http://www.cs.toronto.edu/~ilya/" target="_blank" rel="external">Ilya Sutskever</a>。我也由衷感谢很多同事朋友的帮助，包括<a href="https://www.linkedin.com/pub/dario-amodei/4/493/393" target="_blank" rel="external">Dario Amodei</a>和<a href="http://cs.stanford.edu/~jsteinhardt/" target="_blank" rel="external">Jacob Steinhardt</a>。值得特别感谢还有<a href="http://www.kyunghyuncho.me/" target="_blank" rel="external">Kyunghyun Cho</a>，这哥们对图表的绘制给了我极大的帮助。<br>在写这篇博文之前，我尝试在我讲授的神经网络课程中利用两系列研讨会的时间来解释LSTMs。感谢每一位参与者，感觉大家的反馈。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/honghong.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文是我对大神&lt;a href=&quot;http://colah.github.io/about.html&quot;&gt;Christopher Olah&lt;/a&gt;的&lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Understanding LSTM Networks&lt;/a&gt;的译文。&lt;br&gt;
    
    </summary>
    
      <category term="LSTM" scheme="https://www.vincentqin.tech/categories/LSTM/"/>
    
    
      <category term="CNN" scheme="https://www.vincentqin.tech/tags/CNN/"/>
    
      <category term="LSTM" scheme="https://www.vincentqin.tech/tags/LSTM/"/>
    
      <category term="RNN" scheme="https://www.vincentqin.tech/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>降维之PCA主成分分析原理</title>
    <link href="https://www.vincentqin.tech/posts/pca/"/>
    <id>https://www.vincentqin.tech/posts/pca/</id>
    <published>2017-10-02T05:22:25.000Z</published>
    <updated>2018-06-14T08:52:40.619Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/pca-cover.jpg" alt=""></p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在许多领域的研究与应用中，往往需要对反映事物的多个变量进行大量的观测，收集大量数据以便进行分析寻找规律。多变量大样本无疑会为研究和应用提供了丰富的信息，但也在一定程度上增加了数据采集的工作量，更重要的是在多数情况下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性，同时对分析带来不便。如果分别对每个指标进行分析，分析往往是孤立的，而不是综合的。盲目减少指标会损失很多信息，容易产生错误的结论。<br>因此需要找到一个合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量间存在一定的相关关系，因此有可能用较少的综合指标分别综合存在于各变量中的各类信息。主成分分析与因子分析就属于这类降维的方法。</p><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。能够对高维数据降维的算法包括：</p><ul><li>LASSO</li><li>主成分分析法</li><li>聚类分析</li><li>小波分析法</li><li>线性判别法</li><li>拉普拉斯特征映射</li></ul><h2 id="降维有什么作用"><a href="#降维有什么作用" class="headerlink" title="降维有什么作用"></a>降维有什么作用</h2><p>降维有什么作用呢？</p><ul><li>数据在低维下更容易处理、更容易使用</li><li>相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示</li><li>去除数据噪声</li><li>降低算法开销</li></ul><p>常见的降维算法有主成分分析（principal component analysis,PCA）、因子分析（Factor Analysis）和独立成分分析（Independent Component Analysis，ICA）。</p><center></center><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。<br>注意：PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。</p><h2 id="PCA原理"><a href="#PCA原理" class="headerlink" title="PCA原理"></a>PCA原理</h2><p><img src="http://oofx6tpf6.bkt.clouddn.com/projection.png" alt=""></p><p>最大化样本点在基上的投影，使得数据点尽量的分离。令第一主成分的方向是<script type="math/tex">u_1</script>，我们的目标就是将样本点在该方向上的投影最大化，即：</p><script type="math/tex; mode=display">\max \frac{1}{n}\sum_{i=1}^n<u_1,x_i>^2</script><script type="math/tex; mode=display"> \frac{1}{n}\sum_{i=1}^n<u_1,x_i> \rightarrow \frac{1}{n}\sum_{i=1}^n(x_1^Tu_1)^2=\frac{1}{n}\sum_{i=1}^n(x_1^Tu_1)^T(x_1^Tu_1)</script><script type="math/tex; mode=display">=\frac{1}{n}\sum_{i=1}^n(u_1^Tx_1x_1^Tu_1)=\frac{1}{n}u_1^T\left(\sum_{i=1}^nx_1x_1^T\right)u_1=\frac{1}{n}u_1^T\left(XX^T\right)u_1</script><p>其中的<script type="math/tex">X=[x_1,x_2,...,x_n]^T,x_i\in R^{m}</script>。那么优化函数就变成了：</p><script type="math/tex; mode=display">\max u_1^T\left(XX^T\right)u_1</script><p>以上式子是个二次型，可以证明<script type="math/tex">XX^T</script>是半正定矩阵，所以上式必然有最大值。</p><script type="math/tex; mode=display">\max u_1^T\left(XX^T\right)u_1=\max ||X^Tu_1||_2^2</script><h2 id="优化函数"><a href="#优化函数" class="headerlink" title="优化函数"></a>优化函数</h2><script type="math/tex; mode=display">max||Wx||_2</script><script type="math/tex; mode=display">s.t.  W^TW=I</script><p>解释：<strong>最大化方差同时最小化协方差</strong>（PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”）。最大化方差意味着，使得每个样本点在每个维度上与均值有很大差异，就是说非常有个性，有个性才能分辨出来；同时协方差越小的话表明样本之间的互相影响就非常小，如果协方差是0的话，表示两个字段完全独立。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/explain.gif" alt=""></p><p>寻找协方差矩阵的特征向量和特征值就等价于拟合一条能保留最大方差的直线或主成分。因为特征向量追踪到了主成分的方向，而最大方差和协方差的轴线表明了数据最容易改变的方向。根据上述推导，我们发现达到优化目标就等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将特征值按大小从上到下排列。协方差矩阵作为实对称矩阵，其主要性质之一就是可以正交对角化，因此就一定可以分解为特征向量和特征值。</p><h2 id="具体实施步骤"><a href="#具体实施步骤" class="headerlink" title="具体实施步骤"></a>具体实施步骤</h2><p>总结一下PCA的算法步骤，设有<script type="math/tex">m</script>条<script type="math/tex">n</script>维(字段数)数据，我们采用以下步骤对数据降维。</p><ol><li>将原始数据按列组成<script type="math/tex">n</script>行<script type="math/tex">m</script>列矩阵X. (行数代表字段数目，一个字段就是取每个样本的该维度的数值；列数代表样本数目)</li><li>将<script type="math/tex">X</script>的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</li><li>求出协方差矩阵<script type="math/tex">C=\frac{1}{m}XX^T</script></li><li>求出协方差矩阵的特征值及对应的特征向量</li><li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</li><li><script type="math/tex">Y=PX</script>即为降维到k维后的数据</li></ol><h2 id="去均值化的目的"><a href="#去均值化的目的" class="headerlink" title="去均值化的目的"></a>去均值化的目的</h2><p>下面两幅图是数据做中心化（centering）前后的对比，可以看到其实就是一个平移的过程，平移后所有数据的中心是<script type="math/tex">(0,0)</script>.</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/centering_1.jpg" alt=""></p><p>在做PCA的时候，我们需要找出矩阵的特征向量，也就是主成分（PC）。比如说找到的第一个特征向量是a = [1, 2]，a在坐标平面上就是从原点出发到点（1，2）的一个向量。如果没有对数据做中心化，那算出来的第一主成分的方向可能就不是一个可以“描述”（或者说“概括”）数据的方向了。还是看图比较清楚。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/centering_2.jpg" alt=""></p><p>黑色线就是第一主成分的方向。只有中心化数据之后，计算得到的方向才能比较好的“概括”原来的数据。</p><h2 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h2><p>PCA虽可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="external">PCA的数学原理 </a></li><li><a href="http://blog.csdn.net/oldmonkeyyu_s/article/details/45766543" target="_blank" rel="external"> K-L变换和PCA的区别</a></li><li><a href="http://blog.csdn.net/Dark_Scope/article/details/53150883" target="_blank" rel="external">从PCA和SVD的关系拾遗</a></li><li><a href="https://www.zhihu.com/question/37069477/answer/132387124" target="_blank" rel="external">数据什么时候需要做中心化和标准化处理</a></li><li><a href="http://blog.jobbole.com/109015/" target="_blank" rel="external">主成分分析（PCA）原理详解</a></li></ol><p>附上最近比较火的一首歌<strong><font color="red">Time</font></strong></p><center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="45" height="86" src="//music.163.com/outchain/player?type=2&id=33035611&auto=0&height=66"></iframe></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/pca-cover.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://www.vincentqin.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.vincentqin.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="PCA" scheme="https://www.vincentqin.tech/tags/PCA/"/>
    
      <category term="LDA" scheme="https://www.vincentqin.tech/tags/LDA/"/>
    
      <category term="降维" scheme="https://www.vincentqin.tech/tags/%E9%99%8D%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>SIFT和SURF特性提取总结</title>
    <link href="https://www.vincentqin.tech/posts/SIFT-and-SURF/"/>
    <id>https://www.vincentqin.tech/posts/SIFT-and-SURF/</id>
    <published>2017-10-01T02:18:01.000Z</published>
    <updated>2018-06-03T09:18:21.755Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/match_before_v2.png" alt=""></p><blockquote><p>SIFT（Scale-invariant feature transform）是一种检测局部特征的算法，该算法通过求一幅图中的特征点（interest points,or corner points）及其有关<strong>scale</strong> 和 <strong>orientation</strong> 的描述子得到特征并进行图像特征点匹配</p></blockquote><a id="more"></a><h2 id="什么是SIFT"><a href="#什么是SIFT" class="headerlink" title="什么是SIFT"></a>什么是SIFT</h2><p>先看看上图利用sift进行匹配的结果：</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/matches_adjust_contrast.png" alt=""></p><p>这个结果应该可以很好的解释sift的尺度、旋转以及光照不变性。接下来就介绍一下这个神奇的算法的奥义。我把代码放在了<a href="https://github.com/Vincentqyw/siftDemo" target="_blank" rel="external">Github</a>，感兴趣的同学自己下载下来试试。</p><h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3><p>SIFT特征具有尺度不变性，旋转不变性，光照不变性。</p><h3 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h3><h4 id="构建尺度空间"><a href="#构建尺度空间" class="headerlink" title="构建尺度空间"></a>构建尺度空间</h4><p>尺度空间的目的是模拟图像的多尺度特性。<br><strong>高斯卷积核是实现尺度变换的唯一线性核</strong>，于是 一副二维图像的尺度空间定义为：</p><script type="math/tex; mode=display">L(x,y,\sigma)=G(x,y,\sigma)*I(x,y)</script><p>其中的<script type="math/tex">G(x,y,\sigma)</script>是尺度可以发生变化的高斯函数<script type="math/tex">G(x,y,\sigma)=\frac{1}{2\pi{\sigma}^2}e^{-\frac{x^2+y^2}{2{\sigma}^2}}</script>。<script type="math/tex">(x,y)</script>表示空间坐标，<script type="math/tex">\sigma</script>是尺度系数，描述了图像的模糊程度。<br>为了能够更为有效的提取出特征点，提出了DOG（高斯差分尺度空间）的概念。通过不同尺度下的高斯差分核与图像卷积形成：</p><script type="math/tex; mode=display">D(x,y,\sigma)=(G(x,y,k\sigma)-G(x,y,\sigma))*I(x,y) =L(x,y,k\sigma)-L(x,y,\sigma)</script><p><img src="http://oofx6tpf6.bkt.clouddn.com/scale_space.png" alt=""></p><p>图像金字塔的建立：为了实现尺度不变特性，对于每一幅图像<script type="math/tex">I(x,y)</script>，分成<strong>子八度（octave）</strong>，第一个子八度的scale为原图大小，后面每个octave为上一个octave降采样的结果，即原图size的1/4（长宽分别减半），构成下一个子八度（高一层金字塔）。此时要强烈注意size和尺度空间的概念。size是图像大小，而尺度空间表示不同<script type="math/tex">\sigma</script>的图像的集合。那么尺度空间的集合是：</p><script type="math/tex; mode=display">2^{i-1}(\sigma, k*\sigma,k^2*\sigma,k^3*\sigma,...,k^{n-1}*\sigma)</script><p>其中的 <script type="math/tex">k=2^{1/S}</script>，<script type="math/tex">S</script>表示尺度金字塔每个octave的层数，<script type="math/tex">n</script>表示尺度金字塔的总层数，<script type="math/tex">i</script>表示的是在某个octave的第<script type="math/tex">i</script>层，<script type="math/tex">i\in[1,2,3,...n]</script>。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/DoG.jpg" alt=""></p><p>由图片size决定建几个塔，每塔几层图像(S一般为3-5层)。0塔的第0层是原始图像(或你double后的图像)，往上每一层是对其下一层进行Laplacian变换（高斯卷积，其中σ值渐大，例如可以是σ, k*σ, k*k*σ…），直观上看来越往上图片越模糊。塔间的图片是降采样关系，例如1塔的第0层可以由0塔的第3层down sample得到，然后进行与0塔类似的高斯卷积操作。</p><h4 id="在DoG空间找到关键点"><a href="#在DoG空间找到关键点" class="headerlink" title="在DoG空间找到关键点"></a>在DoG空间找到关键点</h4><p>为了寻找尺度空间的极值点，每一个采样点要和它所有的相邻点比较，看其是否比它的图像域和尺度域的相邻点大或者小。如图所示，中间的检测点和它同尺度的8个相邻点和上下相邻尺度对应的9×2个点共26个点比较，以确保在尺度空间和二维图像空间都检测到极值点。 一个点如果在DOG尺度空间本层以及上下两层的26个领域中是最大或最小值时，就认为该点是图像在该尺度下的一个特征点,如图所示。使用Laplacian of Gaussian能够很好地找到找到图像中的兴趣点，但是需要大量的计算量，所以使用Difference of Gaussian图像的极大极小值近似寻找特征点.DOG算子计算简单，是尺度归一化的LoG算子的近似。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/DoG_Space.jpg" alt=""></p><h4 id="去除不好的点"><a href="#去除不好的点" class="headerlink" title="去除不好的点"></a>去除不好的点</h4><blockquote><p>这一步本质上要去掉DoG局部曲率非常不对称的像素。通过拟和三维二次函数以精确确定关键点的位置和尺度（达到亚像素精度），同时去除低对比度的关键点和不稳定的边缘响应点(因为DoG算子会产生较强的边缘响应)，以增强匹配稳定性、提高抗噪声能力，在这里使用近似Harris Corner检测器。</p></blockquote><h4 id="给特征点赋值一个128维方向参数并描述"><a href="#给特征点赋值一个128维方向参数并描述" class="headerlink" title="给特征点赋值一个128维方向参数并描述"></a>给特征点赋值一个128维方向参数并描述</h4><p>前面的几个步骤确定了特征点到底在哪里，此步骤是为了<strong>描述特征点</strong>。<br>(x,y)处梯度的模值和方向公式为：</p><script type="math/tex; mode=display">m(x,y)=\sqrt{(L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2}</script><script type="math/tex; mode=display">\theta(x,y)=tan^{-1}\left(\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}\right)</script><blockquote><p>利用关键点邻域像素的梯度方向分布特性为每个关键点指定方向参数，使算子具备旋转不变性。</p></blockquote><p>其中L所用的尺度为每个关键点各自所在的尺度。至此，图像的关键点已经检测完毕，每个关键点有三个信息：<strong>位置，所处尺度、方向</strong>，由此可以确定一个SIFT特征区域。</p><p>在实际计算时，我们在以关键点为中心的邻域窗口内采样，并用直方图统计邻域像素的梯度方向。梯度直方图的范围是0～360度，其中每45度一个柱，总共8个柱, 或者每10度一个柱，总共36个柱。Lowe论文中还提到要使用高斯函数对直方图进行平滑，减少突变的影响。直方图的峰值则代表了该关键点处邻域梯度的主方向，即作为该关键点的方向。直方图中的峰值就是主方向，其他的达到最大值80%的方向可作为辅助方向。</p><p>计算keypoint周围的16*16的window中每一个像素的梯度，而且使用高斯下降函数降低远离中心的权重。图左部分的中央为当前关键点的位置，每个小格代表关键点邻域所在尺度空间的一个像素，利用公式求得每个像素的梯度幅值与梯度方向，箭头方向代表该像素的梯度方向，箭头长度代表梯度模值，然后用高斯窗口对其进行加权运算。</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/keypoints.jpg" alt=""></p><p>该图是8*8的区域计算得到2*2描述子向量的过程。但是在实际中使用的是在16*16的区域计算得到4*4的特征描述子，如下图：</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/descriptor.jpg" alt=""></p><p>这样就可以对每个feature形成一个4*4*8=128维的描述子，每一维都可以表示4*4个格子中一个的scale/orientation。将这个<strong>向量归一化之后，就进一步去除了光照的影响</strong>。</p><h3 id="sift的缺点"><a href="#sift的缺点" class="headerlink" title="sift的缺点"></a>sift的缺点</h3><p>SIFT在图像的不变特征提取方面拥有无与伦比的优势，但并不完美，仍然存在：</p><ol><li>实时性不高。</li><li>有时特征点较少。</li><li>对边缘光滑的目标无法准确提取特征点。</li></ol><p>PS: 论文见这里：<a href="http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf" target="_blank" rel="external">Distinctive Image Features from Scale-Invariant Keypoints</a>，这里是<a href="http://www.cs.ubc.ca/~lowe/home.html" target="_blank" rel="external">David Lowe大神</a>做的一个<a href="http://www.cs.ubc.ca/~lowe/keypoints/" target="_blank" rel="external">Demo Software: SIFT Keypoint Detector</a>.</p><h2 id="SURF-简介"><a href="#SURF-简介" class="headerlink" title="SURF 简介"></a>SURF 简介</h2><p>参考了好友整理的一篇文章<a href="http://simtalk.cn/2017/08/18/%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8C%B9%E9%85%8D/#ORB" target="_blank" rel="external">特征与匹配</a></p><ol><li>整体的思路就是将计算DOG的一整套东西来检测关键点的理论替换成了利用hessian矩阵来检测关键点，因为当Hessian矩阵的判别式取得局部极大值时，判定当前点是比周围邻域内其他点更亮或更暗的点，由此来定位关键点的位置。<br>上述过程要进行Hessian判别式的计算，可以采用box filter的方式进行加速。</li><li>构建尺度金字塔的方式不同，具体见下图：</li></ol><p><img src="http://oofx6tpf6.bkt.clouddn.com/diff.png" alt=""></p><ol><li>Sift特征点方向分配是采用在特征点邻域内统计其梯度直方图，取直方图bin值最大的以及超过最大bin值80%的那些方向作为特征点的主方向。而在Surf中，采用的是统计特征点圆形邻域内的harr小波特征。即在特征点的圆形邻域内，统计60度扇形内所有点的水平、垂直harr小波特征总和，然后扇形以0.2弧度大小的间隔进行旋转并再次统计该区域内harr小波特征值之后，最后将值最大的那个扇形的方向作为该特征点的主方向。该过程示意图如下：</li></ol><p><img src="http://oofx6tpf6.bkt.clouddn.com/direction.jpg" alt=""></p><ol><li>生成特征点描述子: 在Sift中，是取特征点周围4*4个区域块，统计每小块内8个梯度方向，用着4*4*8=128维向量作为Sift特征的描述子。surf算法中，也是在特征点周围取一个4*4的矩形区域块，但是所取得矩形区域方向是沿着特征点的主方向。每个子区域统计25个像素的水平方向和垂直方向的haar小波特征，这里的水平和垂直方向都是相对主方向而言的。该haar小波特征为水平方向值之后、垂直方向值之后、水平方向绝对值之后以及垂直方向绝对值之和4个方向。<br>把这4个值作为每个子块区域的特征向量，所以一共有4*4*4=64维向量作为Surf特征的描述子，比Sift特征的描述子减少了2倍。</li></ol><p><img src="http://oofx6tpf6.bkt.clouddn.com/diff_more.jpg" alt=""></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="http://blog.csdn.net/abcjennifer/article/details/7639681/" target="_blank" rel="external">SIFT特征提取分析</a></li><li><a href="http://blog.csdn.net/luoshixian099/article/details/47807103" target="_blank" rel="external">特征匹配-SURF原理与源码解析（一）</a></li><li><a href="http://simtalk.cn/2017/08/18/%E7%89%B9%E5%BE%81%E4%B8%8E%E5%8C%B9%E9%85%8D/#ORB" target="_blank" rel="external">特征与匹配</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/match_before_v2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;SIFT（Scale-invariant feature transform）是一种检测局部特征的算法，该算法通过求一幅图中的特征点（interest points,or corner points）及其有关&lt;strong&gt;scale&lt;/strong&gt; 和 &lt;strong&gt;orientation&lt;/strong&gt; 的描述子得到特征并进行图像特征点匹配&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="计算机视觉" scheme="https://www.vincentqin.tech/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="sift" scheme="https://www.vincentqin.tech/tags/sift/"/>
    
      <category term="surf" scheme="https://www.vincentqin.tech/tags/surf/"/>
    
      <category term="特征提取" scheme="https://www.vincentqin.tech/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法总结</title>
    <link href="https://www.vincentqin.tech/posts/summary-statistical-learning/"/>
    <id>https://www.vincentqin.tech/posts/summary-statistical-learning/</id>
    <published>2017-09-23T11:47:38.000Z</published>
    <updated>2018-06-03T09:10:32.867Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/Captain_America.jpg" alt=""></p><p>本文主要研究监督学习，所谓的监督学习就是在给定的，有限的，用于学习的训练数据集合（training data）出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个集合，即<strong>假设空间</strong>；我们根据一定的评价准则，从假设空间中选取一个最优的模型，使它对已知的训练数据以及未知的测试数据在给定评价准则下有最优的预测，最优模型的选取由<strong>算法</strong>实现。<br>所以统计学习方法有三个要素：<strong>模型</strong>、<strong>策略</strong>、<strong>算法</strong>。</p><a id="more"></a><h2 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h2><ul><li>监督学习</li><li>半监督学习</li><li>无监督学习</li><li>强化学习</li></ul><h2 id="输入空间、特征空间与输出空间"><a href="#输入空间、特征空间与输出空间" class="headerlink" title="输入空间、特征空间与输出空间"></a>输入空间、特征空间与输出空间</h2><ol><li>输入变量&amp;输出变量均连续-&gt; 回归问题</li><li>输出空间为有限个离散变量的预测问题-&gt; 分类问题</li><li>输入与输出均为变量序列的预测问题-&gt; 标注问题</li></ol><h2 id="风险函数"><a href="#风险函数" class="headerlink" title="风险函数"></a>风险函数</h2><ul><li>期望风险：模型关于联合分布的期望损失</li><li>经验风险：模型关于训练样本的平均损失<br>按照大数定律，当样本数据量区域无穷时，经验风险趋近于期望风险；<br>但是当样本容量很小时，经验风险的效果就不会太好，此时容易出现过拟合现象。<br>此时，结构风险就被提出。结构风险是在经验风险的基础上添加上表示模型复杂度的正则化项/罚项。<br>极大似然估计是经验风险最小化的一个特例。<br>最大后验概率估计是结构风险最小化的一个特例；</li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>监督学习里要学习的模型就是<strong>决策函数或者条件概率分布</strong>。</p><p>此时不得不提到生成方法以及判别方法。</p><ul><li>生成方法，由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布模型，即生成模型：<script type="math/tex; mode=display">P(Y|X)=\frac{P(X,Y)}{P(X)}</script></li><li>判别方法是由数据直接学习决策函数或者条件概率分布作为预测模型，即判别模型。</li></ul><h3 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h3><ul><li>生成模型常见的主要有：</li></ul><ol><li>高斯混合模型</li><li>朴素贝叶斯</li><li>混合高斯模型GMM</li><li>隐马尔可夫模型HMM</li><li>马尔可夫的随机场</li><li>KNN</li></ol><ul><li>常见的判别模型有：</li></ul><ol><li>支持向量机</li><li>传统的神经网络</li><li>线性判别分析</li><li>线性回归</li><li>条件随机场</li><li>最大熵模型</li><li>逻辑斯特回归</li></ol><p><img src="http://oofx6tpf6.bkt.clouddn.com/norm12.jpg" alt=""></p><h2 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h2><p>指定策略的目的就是为了挑选出假设空间中到底哪个模型才是我们真正需要的。此时会用到损失函数以及风险函数的概念。</p><ul><li><p>0-1 损失函数</p><script type="math/tex; mode=display">L(Y,f(X))=\left\{\begin{aligned}1,    && Y \neq f(X)\\0,    &&Y = f(X) \end{aligned}\right.</script></li><li><p>平法损失函数</p><script type="math/tex; mode=display">L(Y,f(X))=(Y-f(X)^2</script></li><li><p>绝对值损失函数</p></li></ul><script type="math/tex; mode=display">L(Y,f(X))=|Y-f(X)|</script><ul><li>对数损失函数<script type="math/tex; mode=display">L(Y,P(Y|X))=-logP(Y|X)</script></li></ul><p>损失函数越小的话代表模型越好。$(X,Y)$是随机变量符合联合分布概率$P(X,Y)$，所以损失函数的期望被定义为：</p><script type="math/tex; mode=display">R_{ref}(f)=E_p[L(Y,f(X))]=\int_{\mathcal{X} \times \mathcal{Y}}L(y,f(x))P(x,y)dxdy</script><p>以上是模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数或者<strong>期望损失</strong>（expected loss）。<br>但是呢，期望损失不易求解，我们一般用<strong>模型关于训练数据集的平均损失</strong>来逼近期望损失，即：</p><script type="math/tex; mode=display">R_{emp}(f)=E_p[L(Y,f(X))]=\frac{1}{N} \sum_{i=1}^NL(y_i,f(x_i))</script><p><strong>经验风险最小化</strong>的策略认为，经验风险最小的模型就是最优的模型，于是按照这种定义，我们有：</p><script type="math/tex; mode=display">f^*={argmin}_{f \in \mathcal{F} } R_{emp}</script><p>其中的$\mathcal{F}$是假设空间。<br>最大似然估计就是经验风险最小化的一个例子：当模型为条件概率，损失函数是对数损失时，经验风险最小化就等价于极大似然估计。<br>根据大数定理可知，当样本容量N趋近于无限时，经验风险趋近于期望风险。但是如果样本数量是有限时，此时会出现过拟合现象，那么这时候需要结构风险的帮助。<br>结构风险是为了防止过拟合而提出的策略，结构风险最小化等价于正则化（regularization）。其定义是</p><script type="math/tex; mode=display">R_{srm}(f)=\frac{1}{N} \sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)</script><p>其中的$J(f)$是衡量模型复杂度的项，也叫罚项。当模型越复杂时，$J(f)$越大；模型越简单时，$J(f)$越小。<br>最大后验概率估计（MAP）就是结构风险最小化的一个例子：当模型时条件概率，损失函数是对数损失函数，模型复杂度由先验概率表示时，结构风险最小化就等价于MAP。</p><p>监督学习的模型可以分为概率模型与非概率模型，由条件概率分布$P(Y|X)$或者决策函数$Y=f(X)$表示。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>算法是指学习模型的具体计算方法： <strong>find global optimization solution efficiently</strong>.</p><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><p><strong>模型选择的主要方式有：正则化与交叉验证</strong>。</p><p>在经验风险最小化时已经了解到正则化的由来，正则化就是针对过拟合现象提出的解决策略。</p><p><strong>过拟合</strong>是指学习模型时选择的模型包含的参数过多，以致于这一模型对已知数据的预测很好，而对未知数据的预测能力变得很差。 以下介绍两种正则化的范数：$L_1$ and $L_2$</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h4 id="L2范数正则"><a href="#L2范数正则" class="headerlink" title="L2范数正则"></a>L2范数正则</h4><script type="math/tex; mode=display">C=C_0+ \frac{\alpha}{2n}\sum_ww^2</script><p>对$w$以及$b$求导如下：</p><script type="math/tex; mode=display"> \frac{\partial C}{\partial w}= \frac{\partial C_0}{\partial w}+\frac{\lambda}{n}w \frac{\partial C}{\partial b}= \frac{\partial C_0}{\partial b}</script><p>由梯度下降法可知：</p><script type="math/tex; mode=display">w \rightarrow w-\eta\frac{\partial C}{\partial w}=\left(1-\frac{\eta\lambda}{n} \right)w-\eta\frac{\partial C_0}{\partial w}</script><p>系数$\left(1-\frac{\eta\lambda}{n} \right)$是小于1的，相比于原来的系数等于一，此时的效果相当于就是权值衰减（weight decay）。</p><p>注意到<strong>过拟合</strong>的时候，我们的假设函数要顾及到每一个数据点，模型就会尝试对所有数据点进行拟合，包括一些异常点；此时形成的拟合函数的波动性会非常大，可以看到此时的拟合参数会异常大。通过L2正则化处理可以使这些参数变小，解释如下：</p><blockquote><p>更小的权重，表示网络的复杂组更低，对数据拟合的刚刚好（奥卡姆剃须刀原理）</p></blockquote><h4 id="L1范数正则"><a href="#L1范数正则" class="headerlink" title="L1范数正则"></a>L1范数正则</h4><p>L1正则假设参数的先验分布是Laplace分布，可以保证模型的稀疏性，也就是某些参数等于0；<br>L2正则假设参数的先验分布是Gaussian分布，可以保证模型的稳定性，也就是参数的值不会太大或太小</p><p><img src="https://gss0.baidu.com/-fo3dSag_xI4khGko9WTAnF6hhy/zhidao/pic/item/359b033b5bb5c9ea238f2f6cdd39b6003bf3b3c4.jpg" alt="两种正则化的对比"></p><h4 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h4><p><a href="http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/#ed61992b37932e208ae114be75e42a3e6dc34cb3" target="_blank" rel="external">lasso and ridge regularization</a></p><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>就是将整个数据集切分成三个部分，分别是训练集、验证集和测试集。训练集用来训练模型，验证集用于模型 的选择，而测试集用于对学习方法的评估。</p><p>但是一般情况下，训练数据是不足的，那么此时可以重复的利用数据，进行反复训练以得到最优模型。</p><p>常见的方法有：</p><ul><li>S折交叉验证（S-fold cross validation）</li><li>留一交叉验证（S=N，当S为数据集的容量时，S折交叉验证就变成了留一交叉验证）</li></ul><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”</p><blockquote><p>举个别人博客中的例子，假如有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。我 们想知道罐中白球和黑球的比例，但我们不能把罐中的球全部拿出来数。现在我们可以每次任意从已经摇匀的罐中拿一个球出来，记录球的颜色，然后把拿出来的球 再放回罐中。这个过程可以重复，我们可以用记录的球的颜色来估计罐中黑白球的比例。假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少？很多人马上就有答案了：70%。而其后的理论支撑是什么呢？</p></blockquote><p>我们假设罐中白球的比例是$p$，那么黑球的比例就是$1-p$。因为每抽一个球出来，在记录颜色之后，我们把抽出的球放回了罐中并摇匀，所以每次抽出来的球的颜 色服从同一独立分布。这里我们把一次抽出来球的颜色称为一次抽样。题目中在一百次抽样中，七十次是白球的概率$P(Data|M)$，这里Data是所有的数据，M是所给出的模型，表示每次抽出来的球是白色的概率为$p$。如果第一抽样的结果记为<script type="math/tex">x_1</script>，第二抽样的结果记为<script type="math/tex">x_2</script>… 那么<script type="math/tex">Data = (x_1,x_2,...,x_{100})</script>。这样，</p><script type="math/tex; mode=display">P(Data|M)= P(x_1,x_2,...,x_{100}|M)= P(x_1|M)P(x_2|M)...P(x_{100}|M)= p^{70}(1-p)^{30}</script><p>那么p在取什么值的时候，$P(Data|M)$的值最大呢？将$p^{70}(1-p)^{30}$对$p$求导，并其等于零。</p><script type="math/tex; mode=display">70p^{69}(1-p)^{30}-p^{70}*30(1-p)^{29}=0</script><p>解方程可以得到<script type="math/tex">p=0.7</script>。　　　　</p><h3 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h3><p>最大熵原理使概率学习中的一个准则。学习概率模型时，在所有的可能概率模型（分布）中，熵最大的模型是最好的模型。最大熵原理也可以理解成在满足约束条件的模型中选择熵最大的模型！</p><script type="math/tex; mode=display">H(p)=-\sum_x{log(P(x))P(x)}</script><p>其中$X$服从的概率分布为$P(X)$，$X$服从均匀分布时，熵最大。当没有其他已知的约束时，$\Sigma{P(x)}=1$，此时按照最大熵原理，当$P(x_1)=P(x_2)=…=P(x_n)$时，熵最大；此时等概论，等概论意味着对事实的无知，因为没有更多可能的信息，所以此时的判断也是合情合理的。</p><h3 id="线性分类器"><a href="#线性分类器" class="headerlink" title="线性分类器"></a>线性分类器</h3><p>线性分类器有三大类：感知器准则函数、$SVM$、$Fisher$准则，而贝叶斯分类器不是线性分类器。</p><ul><li>感知器准则函数：代价函数$J=-(W*X+w_0)$，分类的准则是最小化代价函数。感知器是神经网络（$NN$）的基础，网上有很多介绍。</li><li>$SVM$：支持向量机也是很经典的算法，优化目标是最大化间隔（margin），又称最大间隔分类器，是一种典型的线性分类器。（使用核函数可解决非线性问题）</li><li>$Fisher$准则：更广泛的称呼是线性判别分析（$LDA$），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。</li></ul><h3 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h3><ul><li>召回率就是有多少正确的被你找回来了；准确率就是你找到的有多少是正确的；(一般情况下召回率和准确率呈负相关，所以可以用F值衡量整体效果)</li><li>TP(True Positive)是你判断为正确的，实际就是正确的；</li><li>FP(False Positive)是你判断是错误的，实际也是错误的；</li><li>TN(True Negative)是你判断为正确的，但实际是错误的；</li><li>FN(False Negative)是你判断是错误的，但实际是正确的；</li></ul><h2 id="朴素贝叶斯-Naive-Bayes"><a href="#朴素贝叶斯-Naive-Bayes" class="headerlink" title="朴素贝叶斯 Naive Bayes"></a>朴素贝叶斯 <script type="math/tex">Naive Bayes</script></h2><h3 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h3><p>朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类方法。NB的核心在于它假设向量的所有分量之间是独立的。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分。<br>对于由$P(X,Y)$独立产生的训练集$T={(x_1,y_1),(x_2,y_2),…, (x_N,y_N),}$而言通过朴素贝叶斯的方法学习这个联合概率分布。大致分两步：</p><ol><li>计算先验分布：<script type="math/tex; mode=display">P(Y=c_k),k=1,2...,K</script></li><li>条件概率分布：<script type="math/tex; mode=display">P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_k),k=1,2...,K</script>其中的$x \in  R^{n}$，$n$表示这个样本的维度。</li></ol><p>但是在计算条件概率时因为朴素贝叶斯做了条件独立性假设，那么该条件概率分布可以写成：</p><script type="math/tex; mode=display">P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_k)\\=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)</script><p>在实际分类时，对于给定的输入x，通过学习到的模型估计后验概率$P(Y=c_k|X=x)$将后验概率最大的类作为x的类的输出。</p><script type="math/tex; mode=display">P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}\=\frac{P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_{k}P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}</script><p>因为分母就是$P(X=x)$的概率，这是不变的。因此我们仅需要知道分子哪个大就可以，也就是：</p><script type="math/tex; mode=display">y={argmax}_{c_k} P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)</script><p>上式的意思是求解到底是哪些类别<script type="math/tex">c_k</script>能够让最大后验概率最大化。</p><h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><h4 id="极大似然估计-1"><a href="#极大似然估计-1" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><ol><li><p>计算先验概率</p><script type="math/tex; mode=display">P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N}</script></li><li><p>计算条件概率</p><script type="math/tex; mode=display">P(X=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}</script><p>其中</p><script type="math/tex; mode=display">j=1,2...n;l=1,2...S_j;k=1,2...K</script></li><li><p>对于给定的实例x，计算</p><script type="math/tex; mode=display">P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)</script></li><li><p>确定实例的类别</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/Captain_America.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;本文主要研究监督学习，所谓的监督学习就是在给定的，有限的，用于学习的训练数据集合（training data）出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个集合，即&lt;strong&gt;假设空间&lt;/strong&gt;；我们根据一定的评价准则，从假设空间中选取一个最优的模型，使它对已知的训练数据以及未知的测试数据在给定评价准则下有最优的预测，最优模型的选取由&lt;strong&gt;算法&lt;/strong&gt;实现。&lt;br&gt;所以统计学习方法有三个要素：&lt;strong&gt;模型&lt;/strong&gt;、&lt;strong&gt;策略&lt;/strong&gt;、&lt;strong&gt;算法&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://www.vincentqin.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.vincentqin.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="统计学习方法" scheme="https://www.vincentqin.tech/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
      <category term="概率论" scheme="https://www.vincentqin.tech/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
      <category term="SVM" scheme="https://www.vincentqin.tech/tags/SVM/"/>
    
      <category term="过拟合" scheme="https://www.vincentqin.tech/tags/%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    
      <category term="朴素贝叶斯" scheme="https://www.vincentqin.tech/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu上使用Git以及GitHub</title>
    <link href="https://www.vincentqin.tech/posts/git-notebook/"/>
    <id>https://www.vincentqin.tech/posts/git-notebook/</id>
    <published>2017-05-30T14:40:20.000Z</published>
    <updated>2018-06-03T08:54:25.370Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/github-octocat.png" alt=""></p><a id="more"></a><h2 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h2><p>在Ubuntu上安装Git的命令为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install git</div></pre></td></tr></table></figure><h2 id="配置GitHub"><a href="#配置GitHub" class="headerlink" title="配置GitHub"></a>配置GitHub</h2><p>安装git结束之后就是配置github用户资料，如下：<br>将其中的 “user_name”替换成自己 GitHub的用户名并且将”email_id” 换成你创建GitHub账号所用的邮箱.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">git config --global user.name &quot;user_name&quot;</div><div class="line"></div><div class="line">git config --global user.email &quot;email_id&quot;</div></pre></td></tr></table></figure></p><h2 id="建立本地仓库（repository）"><a href="#建立本地仓库（repository）" class="headerlink" title="建立本地仓库（repository）"></a>建立本地仓库（repository）</h2><p>在自己的电脑上建立本地仓库，这个仓库将会在后续推送到GitHub上，语句如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git init Mytest</div></pre></td></tr></table></figure></p><p>如果初始化成功，你将会得到以下提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Initialized empty Git repository in /home/user_name/Mytest/.git/</div></pre></td></tr></table></figure><p>其中的user_name为本地计算机用户名，因人而异。<br>Mytest是”init”创建的文件夹，然后进入该文件所在目录：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cd Mytest</div></pre></td></tr></table></figure></p><h2 id="创建README来描述这个仓库"><a href="#创建README来描述这个仓库" class="headerlink" title="创建README来描述这个仓库"></a>创建README来描述这个仓库</h2><p>该步骤可有可无，但是作为一个优秀的工程师还是写点东西比较好。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gedit README</div></pre></td></tr></table></figure><p>然后输入：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">This is a git repo</div></pre></td></tr></table></figure></p><h2 id="将仓库文件加入index（缓存）"><a href="#将仓库文件加入index（缓存）" class="headerlink" title="将仓库文件加入index（缓存）"></a>将仓库文件加入index（缓存）</h2><p>这一步尤其重要，我们将需要上载到GitHub的文件们添加到index。这些文件可以是文本文档，m/c/c++/PDF/jpg…几乎任何类型文件，一般而言我们可以把需要上载的文件拷贝到这个文件夹内，然后再用一个语句来把需要上传到文件添加到index，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">git add file1.txt</div><div class="line">git add file2.c</div><div class="line">git and file3.m</div><div class="line">...</div></pre></td></tr></table></figure></p><h2 id="提交到本地仓库"><a href="#提交到本地仓库" class="headerlink" title="提交到本地仓库"></a>提交到本地仓库</h2><p>当我们已经把文件添加／修改到index后，就可以进行提交了；利用如下语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git commit -m &quot;some_message&quot;</div></pre></td></tr></table></figure><p>其中some_message可以是任何字符，例如：”my first commit” 或者”edit in readme”等。上面代码的-m参数，就是用来指定这个mesage 的。</p><p>注意：git是分为三部分，一部分是文件，另外一个是缓存区，最后一个是本地库。当你修改了自己的文件后，你会git add xx将修改保存到缓存区，然后再用commit推送修改到本地库中。git push 将本地仓库修改推送到服务器上的仓库中commit是将本地修改保存到本地仓库中。</p><h2 id="在GitHub创建仓库"><a href="#在GitHub创建仓库" class="headerlink" title="在GitHub创建仓库"></a>在GitHub创建仓库</h2><p>这个仓库的名字要和本地的一致，该部分不做展开。然后连接到远程仓库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git remote add origin https://github.com/user_name/Mytest.git</div></pre></td></tr></table></figure><p>其中user_name就是自己的GitHub的用户名。</p><h2 id="推送到远程仓库"><a href="#推送到远程仓库" class="headerlink" title="推送到远程仓库"></a>推送到远程仓库</h2><p>最后的一步就是提交到远程仓库，用以下命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git push origin master</div></pre></td></tr></table></figure></p><p><a href="https://www.howtoforge.com/tutorial/install-git-and-github-on-ubuntu-14.04/" target="_blank" rel="external">原文地址</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/github-octocat.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Git" scheme="https://www.vincentqin.tech/categories/Git/"/>
    
    
      <category term="Linux" scheme="https://www.vincentqin.tech/tags/Linux/"/>
    
      <category term="Ubuntu" scheme="https://www.vincentqin.tech/tags/Ubuntu/"/>
    
      <category term="GitHub" scheme="https://www.vincentqin.tech/tags/GitHub/"/>
    
      <category term="Git" scheme="https://www.vincentqin.tech/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>机器学习修炼手册</title>
    <link href="https://www.vincentqin.tech/posts/machine-learning/"/>
    <id>https://www.vincentqin.tech/posts/machine-learning/</id>
    <published>2017-05-07T12:02:56.000Z</published>
    <updated>2018-06-03T08:50:35.910Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/ML.jpg" alt=""></p><p id="div-border-left-green">对机器学习的学习我开始于二年级的《数据挖掘》课，当时袁老师对数据挖掘中的常用的算法做了一些介绍，但是这仅仅是个入门教学，我并没有深入了解的其中的原理。到现在我才深刻的意识到ML的重要性，我就抽空看了一些这方面的资料，整理了这一份文档。</p><a id="more"></a><p>机器学习算法包括，<span id="inline-red">监督学习</span>(回归、分类)以及<span id="inline-red">非监督学习</span>(聚类)。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><script type="math/tex; mode=display">\bbox[5px,border:2px solid red]{    \theta_j:=\theta_j-\alpha\frac{\partial}{\partial \theta}J(\theta)}</script><p>其中$\alpha$为学习率一般为很小的数字(0.001-0.1)，$\theta$为我们需要求解的参数，$J(\theta)$为能量函数或者为损失函数，通过上述公式可知，梯度下降是沿着损失函数梯度的反方向寻找迭代寻找最优值的过程。梯度下降容易陷入局部最极小点，所以我们要采取一定的措施来阻止这种现象的发生。</p><h2 id="过拟合（Overfitting）"><a href="#过拟合（Overfitting）" class="headerlink" title="过拟合（Overfitting）"></a>过拟合（Overfitting）</h2><p></p><p id="div-border-left-red">如果训练样本的特征过多，我们学习的假设可能在训练集上表现地很好，但是在验证集上表现地就不尽人意</p><p></p><h3 id="避免过拟合"><a href="#避免过拟合" class="headerlink" title="避免过拟合"></a>避免过拟合</h3><ul><li>减少特征的大小</li><li>正则化<ul><li>在保证所有特征都保留的情况下，限制$\theta$的大小，即Small values for parameters $ \theta_0,\theta_1,\theta_2…\theta_n$</li><li>当特征量很多时，该方式仍然表现的很好</li></ul></li><li>交叉验证(Cross Validation)</li></ul><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><p>对于线性回归而言，其损失函数形式如下：</p><script type="math/tex; mode=display">    J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2</script><p>引入正则化之后的损失函数的形式为：</p><script type="math/tex; mode=display">    J(\theta)=\frac{1}{2m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_{j}^2\right)</script><h5 id="GD迭代求解参数"><a href="#GD迭代求解参数" class="headerlink" title="GD迭代求解参数"></a>GD迭代求解参数</h5><p><strong>Repeat</strong>{</p><script type="math/tex; mode=display">    \theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)x_0^{(i)}</script><script type="math/tex; mode=display">    \theta_j:=\theta_j-\alpha\frac{1}{m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\theta_j\right)</script><p>}<br>梯度下降法的学习率$\alpha$需要提前指定，并且还要制定收敛标准。</p><h5 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h5><script type="math/tex; mode=display">\theta=\left(x^Tx+\lambda\begin{bmatrix}{0}&{0}&{\cdots}&{0}\\{0}&{1}&{\cdots}&{0}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{0}&{0}&{\cdots}&{1}\\\end{bmatrix}_{(n+1)(n+1)}\right)^{-1}x^Ty</script><p>上式是对线性回归正则化后的矩阵解。可以证明的是当$\lambda&gt;0$时，求逆符号内部的式子总是可逆的。</p><h4 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h4><p>在没有加入正则化之前，逻辑回归的损失函数的形式是这样的：</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})\log\left(1-h_{\theta}(x^{(i)})\right)\right)</script><p>加入正则项之后的形式为：</p><script type="math/tex; mode=display">J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})\log\left(1-h_{\theta}(x^{(i)})\right)\right)+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2</script><h5 id="GD迭代求解参数-1"><a href="#GD迭代求解参数-1" class="headerlink" title="GD迭代求解参数"></a>GD迭代求解参数</h5><p><strong>Repeat</strong>{</p><script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)x_0^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{1}{m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\theta_j\right)</script><p>}</p><h2 id="SVM支持向量机"><a href="#SVM支持向量机" class="headerlink" title="SVM支持向量机"></a>SVM支持向量机</h2><p>支持向量机又被称作最大间距（Large Margin）分类器，损失函数的形式是：</p><script type="math/tex; mode=display">J(\theta)=C\sum_{i=1}^{m}\left(y^{(i)}cost_1\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})cost_0\left(h_{\theta}(x^{(i)})\right)\right)+\frac{1}{2}\sum_{j=1}^{n}\theta_j^2</script><p>其中：$h_{\theta}(x^{(i)})=\theta^Tx^{i}$，$cost_1$以及$cost_0$的形式如下图所示：</p><script type="math/tex; mode=display">\begin{cases}\text{we want } \theta^Tx\ge1,  & \text{if $y$ =1} \\[2ex]\text{we want } \theta^Tx\le-1,  & \text{if $y$ =0}\end{cases}</script><p>在考虑到soft margin时的损失函数是hinge损失，<a href="http://breezedeus.github.io/2015/07/12/breezedeus-svm-is-hingeloss-with-l2regularization.html" target="_blank" rel="external">SVM就等价于Hinge损失函数+L2正则</a>。此时损失函数为0时候就对应着非支持向量样本的作用，“从而所有的普通样本都不参与最终超平面的决定，这才是支持向量机最大的优势所在，对训练样本数目的依赖大大减少，而且提高了训练效率”。<br>以下是七月在线大神July写的一篇关于SVM的介绍，个人觉得不错。分享下：<a href="https://coding.net/s/f03e79be-8200-46bb-b714-7bb4ef70c391" target="_blank" rel="external">支持向量机通俗导论（理解SVM的三层境界）</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/ML.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p id=&quot;div-border-left-green&quot;&gt;对机器学习的学习我开始于二年级的《数据挖掘》课，当时袁老师对数据挖掘中的常用的算法做了一些介绍，但是这仅仅是个入门教学，我并没有深入了解的其中的原理。到现在我才深刻的意识到ML的重要性，我就抽空看了一些这方面的资料，整理了这一份文档。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://www.vincentqin.tech/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://www.vincentqin.tech/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Machine Learning" scheme="https://www.vincentqin.tech/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linux指令学习笔记</title>
    <link href="https://www.vincentqin.tech/posts/Linux-commands/"/>
    <id>https://www.vincentqin.tech/posts/Linux-commands/</id>
    <published>2017-05-06T09:04:20.000Z</published>
    <updated>2018-06-03T08:48:21.643Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/ubuntu.jpg" alt=""></p><p>今天看了基本的<span id="inline-purple">Linux</span>指令以及<span id="inline-red">makefile</span>的写法，大体总结了一下。<br><a id="more"></a></p><h2 id="常用指令及意义"><a href="#常用指令及意义" class="headerlink" title="常用指令及意义"></a>常用指令及意义</h2><ul><li><strong>root</strong> 表示根目录</li><li><strong>cd</strong> path 切换到path目录， cd / 切换到根目录</li><li><strong>cat</strong> file.txt 查看file.txt中的内容</li><li><strong>pwd</strong>查看当前所在目录</li><li><strong>rmdir</strong>删除目录</li><li><strong>rm</strong> 删除文件</li><li><strong>ls</strong> 列出文件名字， ls -l 列出文件列表</li><li><strong>cp</strong> 复制文件 cp file1.txt file2.txt (复制file1并重命名为file2)</li><li><strong>head</strong> file.txt -n 7 查看file.txt的头7行</li><li><strong>tail</strong> file.txt -n 7 查看file.txt的末尾7行</li><li><strong>wc</strong> file.txt 统计file.txt文件中字符数，返回3个数字：row_size, word_number, character_number; <blockquote><p>-l 统计行数。即 wc -l file.txt<br>-m 统计字符数。这个标志不能与 -c 标志一起使用。<br>-w 统计字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。<br>-L 打印最长行的长度。</p></blockquote></li><li><p><strong>mv</strong> 命令有2中功能</p><ul><li>移动文件夹：mv file.txt file</li><li>修改文件名: mv file1.txt file2.txt</li></ul></li><li><p><strong>chmod</strong> 修改用户权限，有3种用户，分别是：</p></li></ul><div class="table-container"><table><thead><tr><th>解释</th><th>u</th><th>g</th><th>o</th></tr></thead><tbody><tr><td>用户</td><td>user 作者</td><td>group小组</td><td>other其他 </td></tr><tr><td>读写运行</td><td>r w x</td><td>r w x</td><td>r w x </td></tr><tr><td>二进制</td><td>1 0 0</td><td>1 0 0</td><td>1 0 0 </td></tr><tr><td>十进制</td><td>4</td><td>4</td><td>4</td></tr></tbody></table></div><p>假如取消作者的写的权限则：<code>chmod u-w file.txt</code>，其中减号表示“去除”<br>假如添加作者的写的权限则：<code>chmod u+w file.txt</code>，其中减号表示“添加”<br>假如除了作者以外的人都没有读写权限：<code>chmod go-rfile.txt</code><br>同样可以用二进制设置权限 <code>chmod 444 file.txt</code>，表示：u,g,o都只有读的权限。</p><h2 id="远程连接服务器"><a href="#远程连接服务器" class="headerlink" title="远程连接服务器"></a>远程连接服务器</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssh username@server_IP_address</div></pre></td></tr></table></figure><p>之后会提示输入密码。</p><h2 id="远程拷贝文件"><a href="#远程拷贝文件" class="headerlink" title="远程拷贝文件"></a>远程拷贝文件</h2><p>和<code>cp</code>一样，我们改用<code>scp</code>。在本机键入如下命令，其中file是本地的文件，后面的一长串是目标主机的以及其相应目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">scp <span class="_">-f</span> file username@serverIP:/yourfolder</div></pre></td></tr></table></figure><h2 id="监视GPU状态"><a href="#监视GPU状态" class="headerlink" title="监视GPU状态"></a>监视GPU状态</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">watch -n 0.2 nvidia-smi</div></pre></td></tr></table></figure><h2 id="无法连接远程服务器"><a href="#无法连接远程服务器" class="headerlink" title="无法连接远程服务器"></a>无法连接远程服务器</h2><p>首先明确两点：</p><ul><li>make sure you have connected the internet.</li><li>make sure you have installed <code>openssh-server</code>, if not, you can install it by using the following comands:</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install openssh-server</div></pre></td></tr></table></figure><h2 id="脚本语言"><a href="#脚本语言" class="headerlink" title="脚本语言"></a>脚本语言</h2><p>就是将Linux命令集中在一起，组成一个文件，例如有一个test.sh脚本文件<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ls</div><div class="line">date</div><div class="line">cal</div></pre></td></tr></table></figure></p><ul><li>后缀名 .sh</li><li>运行 sh test.sh</li><li>变量赋值不用加空格, b=1, a=$b</li><li>输出 echo</li><li>字符串写不写双引号一样效果，但是还是推荐写双引号</li><li>大于号 -gt</li><li>小于号 -lt</li><li>等于号 -eq</li><li>大于等于 -ge</li><li>小于等于 -le</li><li>不等于 -ne</li></ul><h3 id="判断语句"><a href="#判断语句" class="headerlink" title="判断语句"></a>判断语句</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> [ expr ]</div><div class="line"><span class="keyword">then</span></div><div class="line">...</div><div class="line"><span class="keyword">else</span></div><div class="line">...</div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure><h3 id="循环语句"><a href="#循环语句" class="headerlink" title="循环语句"></a>循环语句</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> x <span class="keyword">in</span> 1 2 3 4 5 6</div><div class="line"><span class="keyword">do</span></div><div class="line"> <span class="built_in">echo</span> <span class="variable">$x</span></div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>例如：arr=(1 2 3 5 6 3 4)<br>注意在运行时候不能继续用sh (1979脚本)；应该改用bash (后期脚本，有数组的时候就用bash)。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">arr=(1 2 5 9 8 6 5 4 3 2)</div><div class="line">max=<span class="variable">$&#123;arr[0]&#125;</span></div><div class="line"><span class="keyword">for</span> i = <span class="variable">$&#123;arr[@]&#125;</span></div><div class="line"><span class="keyword">do</span> </div><div class="line">    <span class="keyword">if</span> [ <span class="variable">$i</span> <span class="_">-gt</span> <span class="variable">$max</span> ]</div><div class="line">    <span class="keyword">then</span> </div><div class="line">        max=<span class="variable">$i</span></div><div class="line">    <span class="keyword">fi</span></div><div class="line"><span class="keyword">done</span></div><div class="line"><span class="built_in">echo</span> max= <span class="variable">$max</span></div></pre></td></tr></table></figure><h2 id="全局变量"><a href="#全局变量" class="headerlink" title="全局变量"></a>全局变量</h2><ul><li>$USER</li><li>$HOME   可以用 ~ 代替</li><li>环境变量 $PATH,将一个路径加入这个全局变量:<br><code>PATH=$PATH:/home/vincent/tutorial</code> (注意所有的路径都是用冒号间隔开的，)</li></ul><h2 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h2><h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3><ul><li>把几个文件打包成file.zip, <code>zip file.zip *</code> (星号的意思是打包所有的文档)</li><li>把全部的子文件夹都递归打包 <code>zip file.zip -r files/*</code></li><li>利用tar命令： <code>tar -zcvf file.tar.gz files/</code></li></ul><h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><ul><li>利用tar命令： <code>tar -zxvf file.tar.gz</code></li></ul><h3 id="下载命令"><a href="#下载命令" class="headerlink" title="下载命令"></a>下载命令</h3><ul><li>wget</li><li>下载后重命名 <code>wget web_address -O file.tar.gz</code> 注意用<code>-O</code></li></ul><h2 id="makefile的写法"><a href="#makefile的写法" class="headerlink" title="makefile的写法"></a>makefile的写法</h2><p>当编译代码的时候，如果有很多子文件，gcc 后面的语句非常长，我们可以选择使用makefile来对其进行处理以加速编译速度并加强可读性。基本的语句是如下所示的格式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Target: dependencies</div><div class="line">(tab键) command</div></pre></td></tr></table></figure></p><ul><li>其中Target表示目标，例如最后想把所有的.c .o 文件们打包成main,那么Target就是main</li><li>dependencies表示依赖项们，即所有的.c .h .o</li><li>command为命令即如：gcc test.c -o test</li></ul><p><strong>例如我们想把tool.c 和main.c 打包成main.o的目标文件</strong>, 则makefile的写法为：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">main: main.c tool.o</div><div class="line">    gcc main.c tool.o -o main</div></pre></td></tr></table></figure></p><p>但是我们发现并没有tool.o文件所以，还要把tool.o怎么来的说明一下：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">tool.o: tool.c</div><div class="line">    gcc -c tool.c</div></pre></td></tr></table></figure></p><p><strong>注意</strong>：<code>gcc -c</code>表示直接把tool.c编译成目标文件tool.o<br>最后呢，如果代码开源的话一般不需要保留.o文件以及main，所以最后还需要把所有的.o以及main文件删除，我们需要在makefile文件的最后添加如下代码：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">clean: </div><div class="line">    rm *.o main</div></pre></td></tr></table></figure></p><p>最后这个makefile可以写成：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">main: main.c tool.o</div><div class="line">    gcc main.c tool.o -o main</div><div class="line">tool.o: tool.c</div><div class="line">    gcc -c tool.c</div><div class="line">clean: </div><div class="line">    rm *.o main</div></pre></td></tr></table></figure></p><p>如果编译器不是gcc，而是其他的编译器，这时候我们有必要做一下代换来提高代码的可移植性。令：CC=gcc， 在引用的时候 $(CC)<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">CC=gcc</div><div class="line">main: main.c tool.o</div><div class="line">    $(CC) main.c tool.o -o main</div><div class="line">tool.o: tool.c</div><div class="line">    $(CC) -c tool.c</div><div class="line">clean: </div><div class="line">    rm *.o main</div></pre></td></tr></table></figure></p><p>例如有test1.c，test2.c，它们分别实现了查找最大值和最小值的功能；然后我们把这两个函数的声明分别放在Max.h和Min.h里面，最后我们在主函数main.c里面包含这两个头文件，然后调用这个两个求最大最小值的函数。 </p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">CC=gcc</div><div class="line">main: main.c Max.o Min.o</div><div class="line">    $(CC) main.c Max.o Min.o -o main</div><div class="line">Max.o: <span class="built_in">test</span>1.c</div><div class="line">    $(CC) <span class="built_in">test</span>1.c -o Max</div><div class="line">Min.o: <span class="built_in">test</span>2.c</div><div class="line">    $(CC) <span class="built_in">test</span>2.c -o M<span class="keyword">in</span></div><div class="line">clean: </div><div class="line">    rm *.o main</div></pre></td></tr></table></figure><p>如果还有第三方的库文件，我们如何链接呢？<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">CC=gcc</div><div class="line">CFLAGS=-lm -Wall -g</div><div class="line">main: main.c Max.o Min.o</div><div class="line">    $(CC) $(CFLAGS) main.c Max.o Min.o -o main</div><div class="line">Max.o: <span class="built_in">test</span>1.c</div><div class="line">    $(CC) $(CFLAGS) <span class="built_in">test</span>1.c -o Max</div><div class="line">Min.o: <span class="built_in">test</span>2.c</div><div class="line">    $(CC) $(CFLAGS) <span class="built_in">test</span>2.c -o M<span class="keyword">in</span></div><div class="line">clean: </div><div class="line">    rm *.o main</div></pre></td></tr></table></figure></p><p>未完待续，需要学习的知识太多了，深深地感觉到了自己的无知:(</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/ubuntu.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;今天看了基本的&lt;span id=&quot;inline-purple&quot;&gt;Linux&lt;/span&gt;指令以及&lt;span id=&quot;inline-red&quot;&gt;makefile&lt;/span&gt;的写法，大体总结了一下。&lt;br&gt;
    
    </summary>
    
      <category term="Linux" scheme="https://www.vincentqin.tech/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.vincentqin.tech/tags/Linux/"/>
    
      <category term="Shell脚本" scheme="https://www.vincentqin.tech/tags/Shell%E8%84%9A%E6%9C%AC/"/>
    
      <category term="makefile" scheme="https://www.vincentqin.tech/tags/makefile/"/>
    
  </entry>
  
  <entry>
    <title>初试HCI光场数据集</title>
    <link href="https://www.vincentqin.tech/posts/new-hci-lightfield-datasets/"/>
    <id>https://www.vincentqin.tech/posts/new-hci-lightfield-datasets/</id>
    <published>2017-04-30T06:22:20.000Z</published>
    <updated>2018-06-03T10:02:42.815Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/buddha2.gif" width="80%"></p><div class="note success"><p>好的数据集是做出漂亮实验的必要条件.<br><span id="inline-red">声明</span>：<u>一切理解都是本人观点，如有疑问，还望在<strong>评论</strong>中留言。如需转载请与本人联系，谢谢合作</u>! 邮箱：<a href="/about#CONTACT INFORMATION">点我</a></p></div><a id="more"></a><h2 id="Wanner光场数据集"><a href="#Wanner光场数据集" class="headerlink" title="Wanner光场数据集"></a>Wanner光场数据集</h2><p>目前光场数据集有如下几种主流的数据集，</p><ol><li><a href="http://lightfield.stanford.edu/lfs.html" target="_blank" rel="external">斯坦福大学光场数据集</a>；</li><li><a href="http://lightfieldgroup.iwr.uni-heidelberg.de/?page_id=713" target="_blank" rel="external">Wanner(HCI)数据集</a>(Old 4D Light Field Benchmark)；</li><li><a href="http://hci-lightfield.iwr.uni-heidelberg.de/" target="_blank" rel="external">4D Light Field Dataset</a>(Konstanz大学与Heidelberg大学的HCI合作,New 4D Light Field Benchmark)。</li></ol><p>下面对Wanner数据集进行讨论。学习光场的同学应该很熟悉Wanner提供的数据集共有<strong>10</strong>个场景，分别是：</p><ol><li>Buddha</li><li>Buddha2</li><li>Couple</li><li>Cube</li><li>Mona</li><li>Medieval</li><li>Papillon</li><li>StillLife</li><li>Horses</li><li>rx_watch</li></ol><p>其中，1-8为仿真场景，9-10是由Raytrix拍摄的场景。他们的文件后缀为 .h5, 格式是HDF5，这是一种文件组织格式，可以很好的将数据组织在一起，具体不做展开。MATLAB 提供了一系列相应的读取该文件的函数，如：h5disp，hdf5info(新版本用h5info)，hdf5read等函数，如利用h5disp就可以得到HDF5文件的内容信息，如下图：</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/Buddha2Hd5.gif" alt=""></p><p>以下给出解码HDF5文件得到子孔径图像以及重排图像的代码：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">input_file       = <span class="string">'Buddha2.h5'</span>; <span class="comment">% file name</span></div><div class="line">input_folder     = <span class="string">''</span>;           <span class="comment">% your datasets folder</span></div><div class="line"></div><div class="line">[pathstr,name,ext] = fileparts([input_folder <span class="string">'/'</span> input_file]);</div><div class="line">file_path=[pathstr,name,ext];</div><div class="line"></div><div class="line">hinfo_data = hdf5info(file_path);</div><div class="line"><span class="keyword">if</span> strcmp(file_path,<span class="string">'Cube'</span>) || strcmp(file_path,<span class="string">'Couple'</span>)</div><div class="line">data = hdf5read(hinfo_data.GroupHierarchy.Datasets(<span class="number">3</span>));</div><div class="line"><span class="keyword">else</span></div><div class="line">data = hdf5read(hinfo_data.GroupHierarchy.Datasets(<span class="number">2</span>));</div><div class="line"><span class="keyword">end</span></div><div class="line">data = <span class="built_in">permute</span>(data, [<span class="number">3</span> <span class="number">2</span> <span class="number">1</span> <span class="number">5</span> <span class="number">4</span>]);</div><div class="line">data = im2double(data(:, :, :, :, <span class="keyword">end</span>:<span class="number">-1</span>:<span class="number">1</span>));</div><div class="line"><span class="comment">% parameters from input</span></div><div class="line">UV_diameter = <span class="built_in">size</span>(data, <span class="number">4</span>);                   <span class="comment">% angular resolution</span></div><div class="line">UV_radius    = <span class="built_in">floor</span>(UV_diameter/<span class="number">2</span>);           <span class="comment">% half angular resolution</span></div><div class="line">h                  = <span class="built_in">size</span>(data, <span class="number">1</span>);            <span class="comment">% spatial image height</span></div><div class="line">w                  = <span class="built_in">size</span>(data, <span class="number">2</span>);            <span class="comment">% spatial image width</span></div><div class="line">y_size=h;</div><div class="line">x_size=w;</div><div class="line">UV_size=UV_diameter^<span class="number">2</span>;</div><div class="line">LF_y_size   = h * UV_diameter;                 <span class="comment">% total image height</span></div><div class="line">LF_x_size   = w * UV_diameter;                 <span class="comment">% total image width</span></div><div class="line">LF_Remap    = <span class="built_in">reshape</span>(<span class="built_in">permute</span>(data, ...</div><div class="line">   [<span class="number">4</span> <span class="number">1</span> <span class="number">5</span> <span class="number">2</span> <span class="number">3</span>]), [LF_y_size LF_x_size <span class="number">3</span>]); <span class="comment">% the remap image</span></div><div class="line">IM_Pinhole = data(:,:,:,UV_radius+<span class="number">1</span>,UV_radius+<span class="number">1</span>); <span class="comment">% the pinhole image</span></div></pre></td></tr></table></figure><p>经过以上步骤可以得到相应的中心视角图像以及Remap（重排）之后的图像，从而进一步方便接下来的工作，例如基于该数据集的深度图像估计算法估计。</p><h2 id="HCI-4D光场数据集-4D-Light-Field-Benchmark"><a href="#HCI-4D光场数据集-4D-Light-Field-Benchmark" class="headerlink" title="HCI 4D光场数据集(4D Light Field Benchmark)"></a>HCI 4D光场数据集(4D Light Field Benchmark)</h2><p id="div-border-left-red">The 4D Light Field Benchmark was jointly created by the University of Konstanz and the HCI at Heidelberg University.</p><p>上周整理上一篇博客的时候，想再次查看HCI数据集是否更新，结果惊喜地看到它竟然更新了！激动之余，就连夜把数据以及代码下载了下来，看看这个数据集的庐山真面目。</p><h3 id="数据集概况"><a href="#数据集概况" class="headerlink" title="数据集概况"></a>数据集概况</h3><p>这个数据集共有4大类：</p><ul><li><p>Stratified（4）</p></li><li><p>training（4）</p></li><li><p>test（4）</p></li><li><p>additional（16）</p><p>​</p></li></ul><p><img src="http://oofx6tpf6.bkt.clouddn.com/scenes.png" alt=""></p><p>总结而言这个4D光场数据集提供了如下信息：</p><ul><li>9x9x512x512x3 light fields as individual PNGs（角度分辨率：9×9，空间分辨率：512×512）</li><li>Config files with camera settings and disparity ranges（相机配置文件以及视差范围）</li><li>Per center view (except for the 4 test scenes):（除了测试类外每类的中心视角图像）<ul><li>512×512 and 5120×5120 depth and disparity maps as PFMs（深度图像以及视差图：512×512低分辨率，以及5120×5120高分辨率）</li><li>512×512 and 5120×5120 evaluation masks as PNGs（png格式的评价掩膜：512×512低分辨率，以及5120×5120高分辨率）</li></ul></li><li>16组additional的每个视角的Ground Truth Depth图像（pfm格式）</li></ul><h3 id="数据集下载"><a href="#数据集下载" class="headerlink" title="数据集下载"></a>数据集下载</h3><p>开始下载吧！在<a href="http://hci-lightfield.iwr.uni-heidelberg.de/" target="_blank" rel="external">该页面</a>的<code>get the data</code>处填写自己的邮箱，然后点击<code>request download links</code>。接下来你的邮箱里就会出现这个数据集的下载链接，链接有点多，你可以选择性的下载或者全部下载。方便起见，我把邮件中提供的链接贴在了下面：</p><ul><li><p><a href="http://lightfield-analysis.net/benchmark/downloads/benchmark.zip" target="_blank" rel="external">Benchmark package with the 12 benchmark scenes</a></p></li><li><p><a href="http://lightfield-analysis.net/benchmark/downloads/full_data.zip" target="_blank" rel="external"><strong>Full package with all 28 scenes</strong></a>(这是全部的场景，共28类；注意不包含深度图像)</p></li></ul><ul><li>Packages per category:<ul><li><a href="http://lightfield-analysis.net/benchmark/downloads/stratified.zip" target="_blank" rel="external">stratified</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/test.zip" target="_blank" rel="external">test</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/training.zip" target="_blank" rel="external">training</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/additional.zip" target="_blank" rel="external">additional</a></li></ul></li></ul><ul><li>Stratified scenes:<ul><li><a href="http://lightfield-analysis.net/benchmark/downloads/backgammon.zip" target="_blank" rel="external">backgammon</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/dots.zip" target="_blank" rel="external">dots</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/pyramids.zip" target="_blank" rel="external">pyramids</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/stripes.zip" target="_blank" rel="external">stripes</a></li></ul></li></ul><ul><li>Test scenes:<ul><li><a href="http://lightfield-analysis.net/benchmark/downloads/bedroom.zip" target="_blank" rel="external">bedroom</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/bicycle.zip" target="_blank" rel="external">bicycle</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/herbs.zip" target="_blank" rel="external">herbs</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/origami.zip" target="_blank" rel="external">origami</a></li></ul></li></ul><ul><li>Training scenes:<ul><li><a href="http://lightfield-analysis.net/benchmark/downloads/boxes.zip" target="_blank" rel="external">boxes</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/cotton.zip" target="_blank" rel="external">cotton</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/dino.zip" target="_blank" rel="external">dino</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/sideboard.zip" target="_blank" rel="external">sideboard</a></li></ul></li></ul><ul><li><p>Additional scenes:</p><ul><li><a href="http://lightfield-analysis.net/benchmark/downloads/antinous.zip" target="_blank" rel="external">antinous</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/boardgames.zip" target="_blank" rel="external">boardgames</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/dishes.zip" target="_blank" rel="external">dishes</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/greek.zip" target="_blank" rel="external">greek</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/kitchen.zip" target="_blank" rel="external">kitchen</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/medieval2.zip" target="_blank" rel="external">medieval2</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/museum.zip" target="_blank" rel="external">museum</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/pens.zip" target="_blank" rel="external">pens</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/pillows.zip" target="_blank" rel="external">pillows</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/platonic.zip" target="_blank" rel="external">platonic</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/rosemary.zip" target="_blank" rel="external">rosemary</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/table.zip" target="_blank" rel="external">table</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/tomb.zip" target="_blank" rel="external">tomb</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/tower.zip" target="_blank" rel="external">tower</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/town.zip" target="_blank" rel="external">town</a></li><li><a href="http://lightfield-analysis.net/benchmark/downloads/vinyl.zip" target="_blank" rel="external">vinyl</a></li></ul></li></ul><ul><li><a href="http://lightfield-analysis.net/benchmark/downloads/additional_depth_disp_all_views.zip" target="_blank" rel="external">Depth and disparity maps for all views of the additional scenes</a></li></ul><h3 id="数据集初体验"><a href="#数据集初体验" class="headerlink" title="数据集初体验"></a>数据集初体验</h3><h4 id="测试代码下载"><a href="#测试代码下载" class="headerlink" title="测试代码下载"></a>测试代码下载</h4><p>在其官方给出的<a href="https://github.com/lightfield-analysis/matlab-tools" target="_blank" rel="external">代码页面</a>下载测试程序，下载完毕后将convert*.m以及lib文件夹其放置在与上述数据集同级目录。例如：TEST目录下同时包括：convert.m 以及 lib/， 同样也包含 additional/, stratified/, test/, 以及 training/。</p><h4 id="生成LF-mat"><a href="#生成LF-mat" class="headerlink" title="生成LF.mat"></a>生成LF.mat</h4><ul><li>convertAll. 对于每一个场景都声称一个<code>LF.mat</code>文件</li></ul><p>如果我们仅仅下载了几个场景我们可以利用如下函数得到相应的<code>LF.mat</code></p><ul><li>convertBlenderTo5D(‘FOLDER’)</li></ul><p>这个LF.mat中包含该场景的光场信息诸如：</p><ul><li>光场数据 (LF.LF)</li><li>真实值 (LF.depth/disp_high/lowres)</li><li>评价掩膜（mask）</li><li>中心视角图像</li></ul><p><span id="inline-red">注意</span>：生成LF.mat的过程用到的参数通过加载相应文件夹下parameters.cfg得到，并将其存储在了LF.parameters中；H变换矩阵存储在了LF.H中（可以参考论文<a href="http://www-personal.acfr.usyd.edu.au/ddan1654/PlenCal.pdf" target="_blank" rel="external">“Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras” </a>）；两个平面的距离存储在LF.f, 单位 [mm]； 相机焦距：LF.parameters.intrinsics.focal_length_mm.</p><h4 id="生成点云（Point-Cloud）"><a href="#生成点云（Point-Cloud）" class="headerlink" title="生成点云（Point Cloud）"></a>生成点云（Point Cloud）</h4><p>接下来我以<strong>additional</strong>文件下的<strong><code>antinous</code></strong>为例子展示如何利用深度图像（官方利用视差）与纹理图像生成点云。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">filename=<span class="string">'antinous'</span>;</div><div class="line">addpath(<span class="string">'lib'</span>);</div><div class="line"><span class="comment">% 得到antinous的LF.mat</span></div><div class="line">convertBlenderTo5D([<span class="string">'additional/'</span>,filename])</div><div class="line">load([<span class="string">'additional/'</span>,filename,<span class="string">'/LF.mat'</span>]);</div><div class="line"></div><div class="line">img=LF.LF(<span class="number">5</span>,<span class="number">5</span>,:,:,:); <span class="comment">%中心视角，用于着色</span></div><div class="line">r=img(:,:,<span class="number">1</span>);</div><div class="line">g=img(:,:,<span class="number">2</span>);</div><div class="line">b=img(:,:,<span class="number">3</span>);</div><div class="line"></div><div class="line"><span class="comment">% 深度图读取</span></div><div class="line">d=pfmread([<span class="string">'additional_depth_disp_all_views\',filename,'</span>\gt_disp_lowres_Cam025.pfm<span class="string">']);</span></div><div class="line">d=mat2gray(d);</div><div class="line"></div><div class="line">mkdir(['PointClouds-color/<span class="string">',filename]);建立一个文件夹存储图片</span></div><div class="line"></div><div class="line">[ X,Y,Z ] = getPointcloud(LF,'disp<span class="string">',d);</span></div><div class="line">ptCloud1 = pointCloud([X(:),Y(:),Z(:)],'color<span class="string">',[r(:) g(:) b(:)]);</span></div><div class="line"></div><div class="line">h1=figure(1);</div><div class="line">pcshow(ptCloud1);</div><div class="line"></div><div class="line">axis off</div><div class="line">set(gcf,'color<span class="string">',[1 1 1])</span></div><div class="line">set(gcf,'Position<span class="string">',[800,300,600,600], '</span>color<span class="string">','</span>w<span class="string">')</span></div><div class="line">view(90.6338,  88.5605);</div><div class="line">zoom(1.2)</div></pre></td></tr></table></figure><p>结果如下所示：</p><p><img src="http://oofx6tpf6.bkt.clouddn.com/PCs.gif" alt=""></p><p><span id="inline-red">注意</span>：生成点云这一步，低版本的MATLAB（如R2014a）由于没有加入相应的函数所以不能够生成点云，高版本（R2016b）可以正常生成。另外，在此提供另外一个函数<code>visualizeZ_3D</code>，该函数将depth map当做彩色图像的z向延伸，然后构图。<br><img src="http://p66ri5yke.bkt.clouddn.com/3d-demo.jpg" width="1200px"></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">&lt;!--The <span class="function"><span class="keyword">function</span> <span class="title">visualizeZ_3D</span>--&gt;</span></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">visualizeZ_3D</span><span class="params">(Z,im)</span></span></div><div class="line"></div><div class="line"><span class="keyword">if</span> (im == <span class="number">0</span>)</div><div class="line">    surf(Z, visualizeZ(Z), <span class="string">'EdgeColor'</span>, <span class="string">'none'</span>); imtight; axis image ij; <span class="comment">%view(-180, 91);</span></div><div class="line"><span class="keyword">else</span></div><div class="line">    surf(Z, im, <span class="string">'EdgeColor'</span>, <span class="string">'none'</span>); imtight; axis image ij; <span class="comment">%view(-180, 91);</span></div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="keyword">end</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">imtight</span></span></div><div class="line">axis image off;</div><div class="line">xMult = <span class="number">1</span>;</div><div class="line">yMult = <span class="number">1</span>;</div><div class="line">borderSize = <span class="number">0</span>;</div><div class="line">PLOTBASESIZE = <span class="number">500</span>;</div><div class="line">set(gca, <span class="string">'PlotBoxAspectRatio'</span>, [xMult yMult <span class="number">1</span>])</div><div class="line">set(gcf, <span class="string">'Position'</span>, get(gcf, <span class="string">'Position'</span>) .* [<span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>] + [<span class="number">0</span> <span class="number">0</span> PLOTBASESIZE*xMult PLOTBASESIZE*yMult]);</div><div class="line">set(gca, <span class="string">'Position'</span>, [borderSize borderSize <span class="number">1</span><span class="number">-2</span>*borderSize <span class="number">1</span><span class="number">-2</span>*borderSize]);</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/buddha2.gif&quot; width=&quot;80%&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;note success&quot;&gt;&lt;p&gt;好的数据集是做出漂亮实验的必要条件.&lt;br&gt;&lt;span id=&quot;inline-red&quot;&gt;声明&lt;/span&gt;：&lt;u&gt;一切理解都是本人观点，如有疑问，还望在&lt;strong&gt;评论&lt;/strong&gt;中留言。如需转载请与本人联系，谢谢合作&lt;/u&gt;! 邮箱：&lt;a href=&quot;/about#CONTACT INFORMATION&quot;&gt;点我&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;
    
    </summary>
    
      <category term="计算机视觉" scheme="https://www.vincentqin.tech/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="Light Field" scheme="https://www.vincentqin.tech/tags/Light-Field/"/>
    
      <category term="光场" scheme="https://www.vincentqin.tech/tags/%E5%85%89%E5%9C%BA/"/>
    
      <category term="计算成像" scheme="https://www.vincentqin.tech/tags/%E8%AE%A1%E7%AE%97%E6%88%90%E5%83%8F/"/>
    
      <category term="点云Point Cloud" scheme="https://www.vincentqin.tech/tags/%E7%82%B9%E4%BA%91Point-Cloud/"/>
    
  </entry>
  
  <entry>
    <title>实习季到了，大家又浮躁了起来</title>
    <link href="https://www.vincentqin.tech/posts/internship/"/>
    <id>https://www.vincentqin.tech/posts/internship/</id>
    <published>2017-04-16T12:48:20.000Z</published>
    <updated>2018-06-03T08:33:46.485Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://oofx6tpf6.bkt.clouddn.com/internship.png" alt=""></p><p id="div-border-left-red">实习季堪比就业季，今年的形势尤其严峻。伴随着忐忑的心情，我迎来了这个不得不面对的时期。</p><a id="more"></a><h1 id="阿里巴巴视觉算法工程师"><a href="#阿里巴巴视觉算法工程师" class="headerlink" title="阿里巴巴视觉算法工程师"></a>阿里巴巴视觉算法工程师</h1><h2 id="算法与视觉部分"><a href="#算法与视觉部分" class="headerlink" title="算法与视觉部分"></a>算法与视觉部分</h2><ul><li>BF，NN的区别</li><li>激活函数的种类</li><li>怎么防止过拟合</li><li>CUDA的内存模型</li><li>HMM是什么</li><li>SVM的优缺点</li><li>SVD分解的过程</li><li>PCA过程</li><li>光流法</li><li>模版匹配SSD与NCC的优缺点</li><li>有哪些形态学操作</li><li>相机畸变的参数到底有哪些</li><li>交叉熵的概念</li><li>Sift与Surf的区别</li><li>由前序遍历与中序遍历求后序遍历</li><li>深度优先遍历可能的顺序</li></ul><p><br></p><h1 id="腾讯基础研究实习生"><a href="#腾讯基础研究实习生" class="headerlink" title="腾讯基础研究实习生"></a>腾讯基础研究实习生</h1><p><img src="http://oofx6tpf6.bkt.clouddn.com/Tencent.png" alt=""></p><p id="div-border-left-green"> 上机考试包括很多数学方面的知识，比考研数学简单多了，但是范围很广，我想过了这么久大家都忘记了吧。概率论部分占了不少题目，尤其要注意后验概率以及假设检验的题目。基础研究没有编程题目！</p><h2 id="数学部分"><a href="#数学部分" class="headerlink" title="数学部分"></a>数学部分</h2><ul><li>特征值与特征向量：Ax=λx</li><li>假设检验，第一类错误与第二类错误</li><li>求解偏导数</li><li>切比雪夫不等式</li><li>F分布的性质</li></ul><h2 id="简答部分"><a href="#简答部分" class="headerlink" title="简答部分"></a>简答部分</h2><ul><li>假设检验来确定零件是否符合标准（可以查看概率论的部分例题）</li><li>神经网络以及SVM的对比，优缺点介绍</li><li>根据某项调查研究，来确定某结论的正确性；</li></ul><h2 id="现场面试部分"><a href="#现场面试部分" class="headerlink" title="现场面试部分"></a>现场面试部分</h2><h3 id="一面"><a href="#一面" class="headerlink" title="一面"></a>一面</h3><p>主要包括以下部分：</p><ul><li>自我介绍（1分钟内）</li><li>项目经历（占了60%时间）</li><li>编程题目（反转链表，可参考<strong>《剑指offer》</strong>）</li><li>意向，做工程还是做算法<br>(ps: 被腾讯挂掉了，惨啊)</li></ul><h3 id="一面（数据挖掘）"><a href="#一面（数据挖掘）" class="headerlink" title="一面（数据挖掘）"></a>一面（数据挖掘）</h3><p>猝不及防地又来了一波电话面试，我一脸懵逼的节奏，完全没有准备。我是小白有没有，面试官主要问了以下几个问题：</p><ul><li>解释方差，协方差以及样本方差的概念</li><li>解释过拟合以及过拟合的概念以及预防措施</li><li>解释TCP滑动窗口的概念（这是啥？）</li><li>求超级长数组的中位数</li><li>析构函数是否可以为虚函数（我是C++小白）</li><li>项目介绍</li></ul><p><br></p><h1 id="商汤算法实习生"><a href="#商汤算法实习生" class="headerlink" title="商汤算法实习生"></a>商汤算法实习生</h1><p><img src="http://oofx6tpf6.bkt.clouddn.com/sensetime.png" alt=""></p><h2 id="在线笔试"><a href="#在线笔试" class="headerlink" title="在线笔试"></a>在线笔试</h2><p id="div-border-left-green"> 本人申请的岗位是见习算法研究员，笔试1个小时，20道选择填空题，3道编程题。时间略紧。涉及面也非常广，数学，智力，概率统计，线代矩阵，图形学，机器学习，神经网络，C++，均有涉及。</p><h3 id="一、选择填空题-部分-："><a href="#一、选择填空题-部分-：" class="headerlink" title="一、选择填空题(部分)："></a>一、选择填空题(部分)：</h3><ol><li><p>S市A，B共有两个区，人口比例为3：5，据历史统计A的犯罪率为0.01%，B区为0.015%，现有一起新案件发生在S市，那么案件发生在A区的可能性有多大?  (概率题，考查贝叶斯公式，牛客网有)</p></li><li><p>A = [1, 2 ; 2, 1]，求A的k次方。(线代，对A进行对角化，求特征值以及特征方程)</p></li><li><p><strong>git</strong>常用命令，克隆到本地是（），提交到仓库区（），取回远程仓库的变化，并与本地分支合并（）,  推送所有分支到远程仓库（），显示有变更的文件（）(答：clone, commit, pull, push, status)</p></li><li><p>表示矩阵需要多少个数字，表示矩阵的投影需要多少个数字?</p></li><li><p>给出先序序列，中序序列，求后序序列。</p></li><li><p>一个关于继承和虚函数问题。</p></li><li><p>掷两个骰子，得到两个数字A,B，设 C = A+B，那么设 C 除以4 的余数为0，1，2，3的概率分别为p0, p1, p2, p3，求它们的大小关系。</p></li><li><p>图片分辨率为512x512，pad = 2, stride = 3, kernel_size = 9, group = 4, 求卷积后输出分辨率大小。</p></li><li><p>一个关于图形自由度的问题。（本人完全没概念，所以题目具体记不清了）</p></li><li><p>以下哪个不能使用迭代器？a) map, b) set, c) queue, d) vector.  (c)</p></li><li><p>有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是() （<a href="https://www.nowcoder.com/questionTerminal/104e95c6a13d464a86eb6b657cc545c0" target="_blank" rel="external">答案猛击这里</a>）</p></li><li><p>在一个无序数组中，求前k个最小数字，复杂度最小为？</p></li><li><p>根据以下程序：求func(500)的值。(经典问题，相当于求500的二进制中1的个数，《剑指offer》)</p><figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">func</span><span class="params">(x)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> countx = <span class="number">0</span>;</div><div class="line">    <span class="keyword">while</span>(x)</div><div class="line">    &#123;</div><div class="line">          countx ++;</div><div class="line">          x = x&amp;(x<span class="number">-1</span>);</div><div class="line">     &#125;</div><div class="line">    <span class="keyword">return</span> countx;</div><div class="line">&#125;</div></pre></td></tr></table></figure></li></ol><ol><li><p>在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（）<br>a) 增加训练集量<br>b) 减少神经网络隐藏层节点数<br>c) 删除稀疏的特征<br>d) SVM算法中使用高斯核/RBF核代替线性核</p></li><li><p>关于vector初始化的一个问题。</p></li><li><p>有4个车站连通情况如下，每辆车每天都会等概率随机从一个车站出发，然后在某个车站呆一夜，第二天再出发。求稳定之后，每个车站的车辆比例。</p></li></ol><p><img src="http://oofx6tpf6.bkt.clouddn.com/question.png" alt=""></p><p>( 根据马尔科夫链 平稳分布，π=πP( P为转移概率矩阵)，和π1+π2+π3+π4=1，同时π1=π4, π2 = π3。可以求得2:3:3:2 ）</p><h3 id="二、编程题"><a href="#二、编程题" class="headerlink" title="二、编程题:"></a>二、编程题:</h3><ol><li><p>连续子数组的最大和。（leetcode或剑指offer原题）</p></li><li><p>Minimum Window Substring .(leetcode 原题)</p></li></ol><p>特别鸣谢<a href="http://blog.csdn.net/smallplum123/article/details/69938232" target="_blank" rel="external">smallpum123</a>的商汤回忆版！</p><h2 id="现场面试"><a href="#现场面试" class="headerlink" title="现场面试"></a>现场面试</h2><blockquote><p>4月25日，从学校匆匆到了商汤科技进行面试，幸亏提前到了1个小时，要不然就被淋成落汤鸡了。</p></blockquote><p>一共有两个面试官依次面了我，这两名面试官的侧重点不同，第一位是大体了解面试对象，第二位面试官更加具体深入了解面试者。</p><ol><li>第一个面试官简单地聊了一段时间，首先是自我介绍，然后是项目经历，最常用的编程语言（我说的是Matlab），然后又问了我有没有用过Matlab的高级函数（bsxfun、ismember等），其他的没有很深入地讨论项目细节。（20 min）</li><li>接下来就是第二个面试官，还是重复前面的问题，自我介绍，项目经历，不过这次更加具体了。因为我的方向是做一些基于光场相机的深度图像估计研究的，面试官就问了我关于光场相机原理以及深度估计算法细节方面的东西；然后又问了我第二个基于GPU加速的项目，具体是如何加速代码的（该项目偏工程，没有具体展开）。项目的最后又问了我这些工程的代码量有多大，多少行的样子（我说最少得两、三千行吧）。</li><li>最后就是编程题目，面试官问我关于商汤在线评测代码书写的问题，我的回答是：对于<strong>连续子数组的最大和问题</strong>仅仅写了思路，没有写全代码。然后就是让我现场手写代码了，大概修改了4遍的样子，终于“调试”（所谓调试就是，现场测试代码一步一步写出结果）成功。（60 min）</li><li>当然，面试的最后通常面试官都会问面试者想要了解公司的事情，我就如实将我想要知道的事情想他请假了一下下，然后就没有然后了…</li></ol><p>经过大概一个半小时，面试结束。无论结果如何，我的心情瞬间轻松许多。还是静候佳音吧~</p><h2 id="电话面试"><a href="#电话面试" class="headerlink" title="电话面试"></a>电话面试</h2><p>由香港那边的负责人对我进行了远程电话面试，主要包括自我介绍以及项目介绍，重点在项目介绍上面。Ps：当时电话那头是两位面试官听着我的陈述，我竟然浑然不知。就这样过了40分钟，结束。等待的时间最为忐忑，我觉得自己表现平平，不知道给面试官留下了什么印象。</p><h2 id="顺利通过"><a href="#顺利通过" class="headerlink" title="顺利通过"></a>顺利通过</h2><p>经过一个漫长的劳动节并时逢校庆的假期，5月6号的下午收到了HR打来的电话，成功通过面试，现在心情还是特别激动。</p><p><br></p><h1 id="搜狐图像处理实习生"><a href="#搜狐图像处理实习生" class="headerlink" title="搜狐图像处理实习生"></a>搜狐图像处理实习生</h1><p><img src="http://oofx6tpf6.bkt.clouddn.com/qianfan.png" alt=""></p><p>初选简历过关之后进行面试。</p><h2 id="笔试（60min）"><a href="#笔试（60min）" class="headerlink" title="笔试（60min）"></a>笔试（60min）</h2><p>根据应聘的实习岗做不同的题目，因为我面的是千帆直播下的图像处理岗位，所以我的题目中有很多关于这方面的相关知识。以下是我的会议版本：</p><ol><li>do while 和while do 的区别</li><li>char const *p 与char * const p 的区别(答：p都是指向const char类型的指针, 不可以赋值给*p, 就是不可通过这个指针改变它指向的值; 第二个: char * const p是指向char的常指针, 指针需在声明时就初始化, 之后不可以改变它的指向)</li><li>创建并且初始化一个双向链表</li><li>代码实现二分查找</li><li>对一个WAV格式的文件头用适当的数据结构进行表示</li><li>队列与栈的区别，分别以什么数据结构表示</li><li>常见的视频压缩方法，视频格式，音频格式</li><li>汇编语言和C/C++混合编程有哪些方法</li><li>如何引用一个已经定义好的全局变量，并比较异同</li><li>gdb如何调试线程，多线程呢（ps:我根本不会这个题目）</li><li>解释“熵”的概念（答：熵，热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量。信息熵表示信息的丰富程度，定义为E=-plog(p)）</li><li>请解释1080p的含义（答：1080指的是分辨率1920*1080，p为扫描方式：逐行扫描）</li><li>请解释FPS的全称以及含义（答：Frames Per Second，帧率的意思）</li><li>解释“码率”的概念（答：即比特率，一秒钟处理的数据量大小，影响到视频的质量以及帧率）</li><li>MPEG标准中有哪些帧类型</li><li>有以下数据结构，请问最后输出结果的是？（注意共用体的大小）<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">typedef union&#123;</div><div class="line">    long i;</div><div class="line">    int j[3];</div><div class="line">    char k;</div><div class="line">&#125;DATA;</div><div class="line"></div><div class="line">struct data&#123;</div><div class="line">    int m;</div><div class="line">    DATA n;</div><div class="line">    double q;</div><div class="line">&#125;;</div><div class="line">data max;</div><div class="line">printf(&quot;%d/n&quot;,&amp;(sizeof(DATA)+sizeof(max));</div></pre></td></tr></table></figure></li></ol><p>还有其他的题目，记不太清了，但是主要就是以上的。难度适中，即有涉及到程序也有图像以及视频处理的相关知识。因为当时没有好好准备，猝不及防的给我了这些题目，感觉一脸懵逼。</p><h2 id="现场面试-1"><a href="#现场面试-1" class="headerlink" title="现场面试"></a>现场面试</h2><p>面试小哥很nice，人很好。首先是自我介绍，然后就是项目经历。基本上简历上的内容问了一遍，感觉还不错。问了我如果调试正在进行中的程序，如何用markdown语言引用代码，对Latex的了解等等。小哥面试结束后，以为女面试官姐姐再次对我的一些基本情况进行了询问，最后还送了一个可爱的小狐狸。<br>无论结果如何等结果吧，祝我好运！<br>ps：很幸运地被录为实习生，但是还是选择了商汤。</p><center><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="600" height="86" src="http://music.163.com/outchain/player?type=2&id=579954&auto=0&height=66"></iframe></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://oofx6tpf6.bkt.clouddn.com/internship.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p id=&quot;div-border-left-red&quot;&gt;实习季堪比就业季，今年的形势尤其严峻。伴随着忐忑的心情，我迎来了这个不得不面对的时期。&lt;/p&gt;
    
    </summary>
    
      <category term="实习" scheme="https://www.vincentqin.tech/categories/%E5%AE%9E%E4%B9%A0/"/>
    
    
      <category term="实习" scheme="https://www.vincentqin.tech/tags/%E5%AE%9E%E4%B9%A0/"/>
    
      <category term="腾讯基础研究" scheme="https://www.vincentqin.tech/tags/%E8%85%BE%E8%AE%AF%E5%9F%BA%E7%A1%80%E7%A0%94%E7%A9%B6/"/>
    
      <category term="阿里巴巴视觉算法" scheme="https://www.vincentqin.tech/tags/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95/"/>
    
      <category term="商汤科技" scheme="https://www.vincentqin.tech/tags/%E5%95%86%E6%B1%A4%E7%A7%91%E6%8A%80/"/>
    
  </entry>
  
</feed>
