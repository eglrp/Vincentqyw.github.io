<!DOCTYPE html>




<html class="theme-next gemini" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="u46QTaG_Dv3OZLpOBKYtqyuiNtIdnhSG5ASKoNvGBCM" />














  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/safari-pinned-tab.svg?v=5.1.3" color="#222">


  <link rel="manifest" href="/images/manifest.json">


  <meta name="msapplication-config" content="/images/browserconfig.xml" />



  <meta name="keywords" content="depth estimation,light field," />





  <link rel="alternate" href="/atom.xml" title="Vincent Qin" type="application/atom+xml" />






<meta name="description" content="本文将介绍光场领域进行深度估计的相关研究。In this post, I’ll introduce some depth estimation algorithms using Light field information. Here is some of the code.研究生阶段的研究方向是光场深度信息的恢复。再此做一些总结，以便于让大家了解光场数据处理的一般步骤以及深度估计的相关的知识">
<meta name="keywords" content="depth estimation,light field">
<meta property="og:type" content="article">
<meta property="og:title" content="Light Field Depth Estimation">
<meta property="og:url" content="https://www.vincentqin.tech/posts/light-field-depth-estimation/index.html">
<meta property="og:site_name" content="Vincent Qin">
<meta property="og:description" content="本文将介绍光场领域进行深度估计的相关研究。In this post, I’ll introduce some depth estimation algorithms using Light field information. Here is some of the code.研究生阶段的研究方向是光场深度信息的恢复。再此做一些总结，以便于让大家了解光场数据处理的一般步骤以及深度估计的相关的知识">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/street.jpg">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/2pp-v1.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/uvst2images.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/allviews.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/angular-patch.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/epi.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/lf-all-view.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/lf-acquisition.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/jeon-depth-step.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/2pp-epi-depth.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/epi-depth.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/tao2013.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/tao2013-results-step-by-step.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/tao2013-confidence-analysis.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/epinet-architecture.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/viewpoints-effect.png">
<meta property="og:image" content="http://oofx6tpf6.bkt.clouddn.com/epinet-evaluation.png">
<meta property="og:updated_time" content="2018-06-04T15:02:08.883Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Light Field Depth Estimation">
<meta name="twitter:description" content="本文将介绍光场领域进行深度估计的相关研究。In this post, I’ll introduce some depth estimation algorithms using Light field information. Here is some of the code.研究生阶段的研究方向是光场深度信息的恢复。再此做一些总结，以便于让大家了解光场数据处理的一般步骤以及深度估计的相关的知识">
<meta name="twitter:image" content="http://oofx6tpf6.bkt.clouddn.com/street.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.3',
    sidebar: {"position":"left","display":"hide","offset":10,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":false,"async":true,"transition":{"post_block":"bounceLeftIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://www.vincentqin.tech/posts/light-field-depth-estimation/"/>





  <title>Light Field Depth Estimation | Vincent Qin</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-97856334-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?c6e58e5665d2cb4a832117302943c909";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
	<a href="https://www.github.com/Vincentqyw" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
	    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Vincent Qin</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Keep Your Curiosity</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Timelines
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-collections">
          <a href="/collections" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-diamond"></i> <br />
            
            Collections
          </a>
        </li>
      
        
        <li class="menu-item menu-item-guest_comments">
          <a href="/guestbook" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-send"></i> <br />
            
            Messager
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            Schedule
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  
    
  

  <article class="post post-type-normal post-sticky" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.vincentqin.tech/posts/light-field-depth-estimation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Vincent Qin">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/qin.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Vincent Qin">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Light Field Depth Estimation</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-05-16T13:35:54+08:00">
                2018-05-16
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2018-06-04T23:02:08+08:00">
                2018-06-04
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/计算机视觉/" itemprop="url" rel="index">
                    <span itemprop="name">计算机视觉</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
            <!--noindex-->
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/posts/light-field-depth-estimation/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count hc-comment-count" data-xid="posts/light-field-depth-estimation/" itemprop="commentsCount"></span>
                </a>
              </span>
              <!--/noindex-->
            
          

          
          
             <span id="/posts/light-field-depth-estimation/" class="leancloud_visitors" data-flag-title="Light Field Depth Estimation">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><img src="http://oofx6tpf6.bkt.clouddn.com/street.jpg" alt=""></p>
<div class="note success"><p>本文将介绍光场领域进行深度估计的相关研究。<br>In this post, I’ll introduce some depth estimation algorithms using Light field information. Here is some of the <a href="https://github.com/Vincentqyw/Depth-Estimation-Light-Field" target="_blank" rel="external">code</a>.<br>研究生阶段的研究方向是光场深度信息的恢复。再此做一些总结，以便于让大家了解光场数据处理的一般步骤以及深度估计的相关的知识，光场可视化部分代码见<a href="https://github.com/Vincentqyw/light-field-Processing" target="_blank" rel="external">light-field-Processing</a>。如有任何疑问或者建议，请大家在评论区提出。</p></div>
<a id="more"></a>
<h1 id="什么是光场？"><a href="#什么是光场？" class="headerlink" title="什么是光场？"></a>什么是光场？</h1><p>提到光场，很多人对它的解释模糊不清，在此我对它的概念进行统一表述。它的故事可以追溯到1936年，那是一个春天，Gershun写了一本名为<strong>The Light Field</strong><sup><a href="#fn_1" id="reffn_1">1</a></sup>的鸿篇巨著（感兴趣的同学可以看看那个年代的论文），于是光场的概念就此诞生，但它并没有因此被世人熟知。经过了近六十年的沉寂，1991年Adelson<sup><a href="#fn_2" id="reffn_2">2</a></sup>等一帮帅小伙将光场表示成了如下的7维函数：</p>
<script type="math/tex; mode=display">
P(\theta,\phi,\lambda,t,V_x,V_y,V_z). \tag{1}</script><p>其中$(\theta,\phi)$表示球面坐标，$\lambda$表示光线的波长，$t$表示时间，$(V_x,V_y,V_z)$表示观察者的位置。<br>可以想象假如有这样一张由针孔相机拍摄的黑白照片，它表示：我们从<strong>某个时刻</strong>、<strong>单一视角</strong>观察到的<strong>可见光谱</strong>中某个<strong>波长</strong>的光线的平均。也就是说，它记录了通过$P$点的光强分布，光线方向可以由球面坐标$P(\theta,\phi)$或者笛卡尔坐标$P(x,y)$来表示。对于彩色图片而已，我们要添加光线的波长$\lambda$信息即变为$P(\theta,\phi,\lambda)$。按照同样的思路，彩色电影也就是增加了时间维度$t$，因此$P(\theta,\phi,\lambda,t)$。对于彩色全息电影而言，我们可以从任意空间位置$(V_x,V_y,V_z)$进行观看，于是其可以表达为最终的形式$P(\theta,\phi,\lambda,t,V_x,V_y,V_z)$。这个函数又被成为全光函数（Plenoptic Function）。<br>但是以上的七维的全光函数过于复杂，难以记录以及编程实现。所以在实际应用中我们对其进行简化处理。第一个简化是单色光以及时不变。可分别记录3原色以简化掉波长$\lambda$，可以通过记录不同帧以简化$t$，这样全光函数就变成了5D。第二个简化是Levoy<sup><a href="#fn_3" id="reffn_3">3</a></sup>等人（1996年）认为5D光场中还有一定的冗余，可以在自由空间（光线在传播过程中能量保持不变）中简化成4D。</p>
<h2 id="光场参数化表示"><a href="#光场参数化表示" class="headerlink" title="光场参数化表示"></a>光场参数化表示</h2><p>参数化表示要解决的问题包括：1. 计算高效；2. 光线可控；3. 光线均匀采样。目前比较常用的表示方式是双平面法（$2PP$）<sup><a href="#fn_3" id="reffn_3">3</a></sup>，利用双平面法可以将光场表示为：$L(u,v,s,t)$。其中$(u,v)$为第一个平面，$(s,t)$是第二个平面。那么一条有方向的直线可以表示为连接$uv$以及$st$平面上任意两点确定的线，如下图所示：</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/2pp-v1.png" alt=""></p>
<p>【注】：Levoy<sup><a href="#fn_3" id="reffn_3">3</a></sup>首先利用双平面法对光场进行表示，光线首先通过$uv$平面，然后再通过$st$平面。但是后来（同年）Gortler<sup><a href="#fn_4" id="reffn_4">4</a></sup>等人将其传播方向反了过来，导致后续研究者对此表述并不一致。与此同时，也有不少文献中也引入了$xy$坐标，例如著名的光场相机的缔造者N.G.博士的毕业论文。通常情况下，这指的是像平面坐标，即指的是由传感器得到的图像中像素的位置坐标。由于后续处理中都是针对图像而言，而对于光学结构以及光线的传播过程并不感兴趣。所以为了方便起见，我们在本文中统一采用Levoy<sup><a href="#fn_3" id="reffn_3">3</a></sup>的方式对<strong>光场图像</strong>进行表示，即$uv$表示角度分辨率，$xy$表示空间分辨率，即$L(u,v,x,y)$。同时在表示<strong>光场</strong>时用$L(u,v,s,t)$。有时候二者不做区分，注意即可。</p>
<h2 id="光场的可视化"><a href="#光场的可视化" class="headerlink" title="光场的可视化"></a>光场的可视化</h2><p>虽然光场由$7D$全光函数降维到$4D$，但是其结构还是很难直观想象。通过固定4D光场参数化表示$L(u,v,s,t)$中的某些变量，我们可以很容易地对光场进行可视化。我们通常认为$(u,v)$控制着某个视角的位置，即相机平面；而$(s,t)$控制着从某个视角观察到的图像。说简单点：$uv$控制角度分辨率，$st$控制空间分辨率（视野）。注意上式描述的是光线的表示方法，并没有涉及图像处理，所以没有$xy$。</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/uvst2images.png" width="600" alt="uvst2images"></p>
<p>接下来讲解，几种常见的可视化方式（图片来源<sup><a href="#fn_5" id="reffn_5">5</a></sup>）。首先是<strong>多视图法</strong>。很容易理解，对于最简单的情况，首先固定$u=u^*,v=v^*$，我们可以得到多视角的某个视图$L(u^*,v^*,s,t)$，如下图所示：</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/allviews.png" alt=""></p>
<p>第二种表示方法是<strong>角度域法</strong>，通过固定$s=s^*,t=t^*$可以得到某个宏像素$L(u,v,s^*,t^*)$，如下图所示：</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/angular-patch.png" alt=""></p>
<p>第三种表示方法是<strong>极线图法</strong>，通过固定$v=v^*,t=t^*$可以得到极线图：$L(u,v^*,s,t^*)$，如下图中水平方向的图所示；同理固定$u=u^*,s=s^*$可以得到极线图：$L(u^*,v,s^*,t)$，如下图中竖直方向的图所示：</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/epi.png" alt=""></p>
<p>最后，给出这几种方式的对应关系图（注意图中，$xy$对应于以上$st$，$st$对应于$uv$）。</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/lf-all-view.png" alt=""></p>
<h1 id="光场的获取"><a href="#光场的获取" class="headerlink" title="光场的获取"></a>光场的获取</h1><p>我们知道传统的相机只能采集来自场景某个方向的$2D$信息，那怎么才能够采集到光场信息呢？试想一下，当多个相机在多个不同视角同时拍摄时，这样我们就可以得到一个光场的采样（多视角图像）了。当然，这是容易想到的方法，目前已有多种获得光场的方式，如下表格中列举了其中具有代表性的方式<sup><a href="#fn_5" id="reffn_5">5</a></sup>。</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/lf-acquisition.png" alt=""></p>
<h1 id="光场深度估计算法分类"><a href="#光场深度估计算法分类" class="headerlink" title="光场深度估计算法分类"></a>光场深度估计算法分类</h1><p>由上可知，光场图像中包含来自场景的多视角信息，这使得深度估计成为可能。相较于传统的多视角深度估计算法而言，基于光场的深度估计算法无需进行相机标定，这大大简化的深度估计的流程。但是由于光场图像巨大导致了深度估计过程占用大量的计算资源。同时这些所谓的多个视角之间虚拟相机的基线过短，从而可能导致误匹配的问题。以下将对多种深度估计算法进行分类并挑选具有代表性的算法进行介绍。</p>
<h2 id="多视角立体匹配"><a href="#多视角立体匹配" class="headerlink" title="多视角立体匹配"></a>多视角立体匹配</h2><p>根据光场相机的成像原理，我们可以将光场图像想像成为多个虚拟相机在多个不同视角拍摄同一场景得到图像的集合，那么此时的深度估计问题就转换成为多视角立体匹配问题。以下列举几种基于多视角立体匹配算法的深度估计算法<sup><a href="#fn_8" id="reffn_8">8</a></sup> <sup><a href="#fn_9" id="reffn_9">9</a></sup> <sup><a href="#fn_10" id="reffn_10">10</a></sup> <sup><a href="#fn_20" id="reffn_20">20</a></sup> <sup><a href="#fn_21" id="reffn_21">21</a></sup>。</p>
<table>
    <tr>
        <td rowspan="6"> MVS-based<br>
        </td><td><b>Approach</b></td>
        <td><b>Main Feature</b></td>
    </tr>

    <tr>
        <td>Jeon et al. <sup><a href="#fn_8" id="reffn_8">8</a></sup></td>
        <td>Phase shift Sub-pixel</td>
    </tr>
    <tr>
        <td>Yu et al. <sup><a href="#fn_9" id="reffn_9">9</a></sup></td>
        <td>Line-assisted graph cut</td>
    </tr>
    <tr>
        <td>Heber et al. <sup><a href="#fn_10" id="reffn_10">10</a></sup> <sup><a href="#fn_2" id="reffn_20">20</a></sup></td>
        <td>PCA matching term</td>
    </tr>
    <tr>
        <td>Chen et al. <sup><a href="#fn_21" id="reffn_21">21</a></sup></td>
        <td>Scam: Bilateral Consistency Metric</td>
    </tr>

</table>

<p>在这里介绍Jeon等人<sup><a href="#fn_8" id="reffn_8">8</a></sup>提出的基于相移的亚像素多视角立体匹配算法。</p>
<h3 id="相移理论"><a href="#相移理论" class="headerlink" title="相移理论"></a>相移理论</h3><p>该算法的核心就是用到了相移理论，即空域的一个小的位移在频域为原始信号的频域表达与位移的指数的幂乘积，即如下公式：</p>
<script type="math/tex; mode=display">
\mathcal{F}\left\{I(x+\Delta x)\right\} = \mathcal{F}\left\{I(x)\right\}\exp^{2\pi j\Delta x}. \tag{2}</script><p>所以，经过位移后图像可以表示为：</p>
<script type="math/tex; mode=display">
I'(x)=I(x+\Delta x)={\mathcal{F}^{-1}\left\{\mathcal{F}\left\{I(x)\right\}\exp^{2 \pi j \Delta x}\right\}},\tag{3}</script><p>面对Lytro相机窄基线的难点，通过相移的思想能够实现亚像素精度的匹配，在一定程度上解决了基线短的问题。那么大家可能好奇的是，如何将这个理论用在多视角立体匹配中呢？带着这样的疑问，继续介绍该算法。</p>
<h3 id="匹配代价构建"><a href="#匹配代价构建" class="headerlink" title="匹配代价构建"></a>匹配代价构建</h3><p>为了能够使子视角图像之间进行匹配，作者设计了2中不同的代价量：Sum of Absolute Differences (SAD)以及Sum of Gradient Differences (GRAD)，最终通过加权的方式获得最终的匹配量$C$，它是位点$x$以及损失编号（可以理解成深度/视差层）$l$的函数，具体形式如下公式所示：</p>
<script type="math/tex; mode=display">
C(x,l) = \alpha C_A(x,l)+(1-\alpha)C_G(x,l),\tag{4}</script><p>其中$\alpha \in [0,1]$表示SAD损失量$C_A$以及SGD损失量$C_G$之间的权重。同时其中的$C_A$被定义为如下形式：</p>
<script type="math/tex; mode=display">
C_A(x,l) = \sum_{u \in V}\sum_{x \in R_x}{\min\left( | I(u_c,x)-I(u,x+\Delta x(u,l))|,\tau _1\right)},\tag{5}</script><p>其中的$R_x$表示在$x$点邻域的矩形区域；$\tau _1$是代价的截断值（为了增加算法鲁棒性）；$V$表示除了中心视角$u_c$之外的其余视角。上述公式通过比较中心视角图像$I(u_c,x)$与其余视角$I(u,x)$的差异来构建损失量，具体而言就是通过不断地在某个视角$I(u_i,x)$上$x$点的周围移动一个<strong>小的距离</strong>并于中心视角做差；重复这个过程直到比较完所有的视角(i=1…视角数目N)为止。此时会用到上节提及的相移理论以得到移动后的像素强度，注意上面提到的<strong>小的距离</strong>实际上就是公式中的$\Delta x$，它被定义为如下形式：</p>
<script type="math/tex; mode=display">
\Delta x(u,l) = lk(u-u_c),\tag{6}</script><p>其中k表示深度/视差层的单位（像素），$\Delta x$会随着任意视角与中心视角之间距离的增大而线性增加。同理，可以构造出第二个匹配代价量SGD，其基本形式如下所示：</p>
<script type="math/tex; mode=display">
C_G(x,l) = \sum_{u \in V}\sum_{x \in R_x}\beta (u){\min\left( Diff_x(u_c,u,x,l),\tau _2\right)}+ \\ \ \ \ (1-\beta (u)){\min\left( Diff_y(u_c,u,x,l),\tau _2\right)},\tag{7}</script><p>其中的$Diff_x(u_c,u,x,l)=|I_x(u_c,x)-I_x(u,x+\Delta x(u,l))|$表示子视角图像在x方向的上的梯度，同理$Diff_y$表示子孔径图像在y方向上的梯度；$\beta (u)$控制着这两个方向代价量的权重，它由任意视角与中心视角之间的相对距离表示：</p>
<script type="math/tex; mode=display">
\beta (u) = \frac{|u-u_c|}{|u-u_c|+|v-v_c|}.\tag{8}</script><p>至此，代价函数构建完毕。随后对于该代价函数利用边缘保持滤波器进行损失聚合，得到优化后的代价量。紧接着作者建立了一个多标签优化模型（GC求解）以及迭代优化模型对深度图进行优化，再此不做详细介绍。下面是其算法的分部结果：</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/jeon-depth-step.png" alt=""></p>
<h2 id="基于EPI的方法"><a href="#基于EPI的方法" class="headerlink" title="基于EPI的方法"></a>基于EPI的方法</h2><p><img src="http://oofx6tpf6.bkt.clouddn.com/2pp-epi-depth.png" width="100%"></p>
<p>不同于多视角立体匹配的方式，EPI的方式是通过分析光场数据结构的从而进行深度估计的方式。EPI图像中斜线的斜率就能够反映出场景的深度。上图中点P为空间点，平面$\Pi$为相机平面，平面$\Omega$为像平面。图中$\Delta u$与$\Delta x$的关系可以表示为如下公式<sup><a href="#fn_6" id="reffn_6">6</a></sup>：</p>
<script type="math/tex; mode=display">
\Delta x=- \frac{f}{Z}\Delta u,\tag{9}</script><p>假如固定相同的$\Delta u$，水平方向位移较大的EPI图中斜线所对应的视差就越大，即深度就越小。如下图所示，$\Delta x_2$&gt;$\Delta x_1$，那么绿色线所对应的空间点要比红色线所对应的空间点深度小。</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/epi-depth.png" width="100%"></p>
<p>以下列举几种基于EPI的深度估计算法<sup><a href="#fn_11" id="reffn_11">11</a></sup> <sup><a href="#fn_12" id="reffn_12">12</a></sup> <sup><a href="#fn_13" id="reffn_13">13</a></sup> <sup><a href="#fn_14" id="reffn_14">14</a></sup> <sup><a href="#fn_15" id="reffn_15">15</a></sup> <sup><a href="#fn_24" id="reffn_24">24</a></sup>。</p>
<table>
    <tr>
        <td rowspan="9"> EPI-based<br>

        </td><td><b>Approach</b></td>
        <td><b>Main Feature</b></td>
    </tr>

    <tr>
        <td>Kim et al. <sup><a href="#fn_11" id="reffn_11">11</a></sup></td>
        <td>Large scene reconstruction</td>
    </tr>

    <tr>
        <td>Li et al. <sup><a href="#fn_12" id="reffn_12">12</a></sup></td>
        <td>Sparse linear optimization</td>
    </tr>

    <tr>
        <td>Krolla et al. <sup><a href="#fn_13" id="reffn_13">13</a></sup></td>
        <td>Spherical light field</td>
    </tr>

    <tr>
        <td>Wanner et al. <sup><a href="#fn_14" id="reffn_14">14</a></sup> <sup><a href="#fn_26" id="reffn_26">26</a></sup></td>
        <td>Total variation(TV)</td>
    </tr>

    <tr>
        <td>Diebode et al. <sup><a href="#fn_15" id="reffn_15">15</a></sup></td>
        <td>Modified structure tensor</td>
    </tr>

    <tr>
        <td>Zhang et al. <sup><a href="#fn_24" id="reffn_24">24</a></sup></td>
        <td>Spinning Parallelogram Operator(SPO)</td>
    </tr>

</table>

<p>在以上表格中最具代表性的算法是由wanner<sup><a href="#fn_14" id="reffn_14">14</a></sup>提出的结构张量法得到EPI图中线的斜率，如下公式所示：</p>
<script type="math/tex; mode=display">
 J=
 \left[
 \begin{matrix}
   G_{\sigma}*(S_xS_x) & G_{\sigma}*(S_xS_y)  \\
   G_{\sigma}*(S_xS_y) & G_{\sigma}*(S_yS_y)
  \end{matrix}
  \right]=
  \left[
 \begin{matrix}
   J_{xx} & J_{xy}\\
   J_{xy} & J_{yy}
  \end{matrix}
  \right],
  \tag{10}</script><p>其中$S=S_{y^*,v^*}$为极线图。$S_x$以及$S_y$表示极线图在x以及y方向上的梯度，$G_{\sigma}$表示高斯平滑算子。最终极线图中局部斜线的斜率可以表示成如下形式：</p>
<script type="math/tex; mode=display">
 J=\left[
 \begin{matrix}
   \Delta x  \\
    \Delta v
  \end{matrix}
  \right]=
  \left[
 \begin{matrix}
  \sin \varphi\\
   \cos \varphi
  \end{matrix}
  \right],
  \tag{11}</script><p>其中$\varphi = \frac{1}{2}\arctan\left(\frac{J_{yy}-J_{xx}}{2J_{xy}}\right)$。因此深度可以由公式（9）推出：</p>
<script type="math/tex; mode=display">
Z=-f\frac{\Delta v}{\Delta x},  \tag{12}</script><p>通常情况下，可以用一种更加简单的形式，如视差对其进行表示：</p>
<script type="math/tex; mode=display">
d_{y^*,v^*}=-f/Z=\frac{\Delta x}{\Delta v}=\tan \phi .  \tag{13}</script><p>至此，利用上述公式可以从EPI中估计出视差。</p>
<h2 id="散焦及融合的方法"><a href="#散焦及融合的方法" class="headerlink" title="散焦及融合的方法"></a>散焦及融合的方法</h2><p>光场相机一个很重要的卖点是先拍照后对焦，这其实是根据光场剪切原理<sup><a href="#fn_31" id="reffn_31">31</a></sup>得到的。通过衡量像素在不同焦栈处的“模糊度”可以得到其对应的深度。以下列举几种基于散焦的深度估计算法<sup><a href="#fn_7" id="reffn_7">7</a></sup> <sup><a href="#fn_16" id="reffn_16">16</a></sup> <sup><a href="#fn_17" id="reffn_17">17</a></sup> <sup><a href="#fn_22" id="reffn_22">22</a></sup> <sup><a href="#fn_23" id="reffn_23">23</a></sup>。</p>
<table>
    <tr>
        <td rowspan="5"> Defocus-based<br>
        </td><td><b>Approach</b></td>
        <td><b>Main Feature</b></td>
    </tr>

    <tr>
        <td>Wang et al. <sup><a href="#fn_7" id="reffn_7">7</a></sup></td>
        <td>Occlusion-aware</td>
    </tr>

    <tr>
        <td>Tao et al. <sup><a href="#fn_16" id="reffn_16">16</a></sup></td>
        <td>Defocus cues & Correspondence cues</td>
    </tr>
    <tr>
        <td>Tao et al. <sup><a href="#fn_17" id="reffn_17">17</a></sup></td>
        <td>Angular Coherence</td>
    </tr>

    <tr>
        <td>Williem et al. <sup><a href="#fn_22" id="reffn_22">22</a></sup> <sup><a href="#fn_23" id="reffn_23">23</a></sup></td>
        <td>Angular Entropy(AD, AE, CAD, CAE)</td>
    </tr>

</table>

<p>这里介绍一个最具代表性的工作，由Tao等人<sup><a href="#fn_16" id="reffn_16">16</a></sup>在2013年提出，下图为其算法框架以及分部结果。</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/tao2013.png" alt=""><br><img src="http://oofx6tpf6.bkt.clouddn.com/tao2013-results-step-by-step.png" alt=""></p>
<p>该工作其实就做了2件事情：1. 设计两种深度线索并估计原始深度；2. 置信度分析及MRF融合。以下对其进行具体介绍。</p>
<h3 id="双线索提取"><a href="#双线索提取" class="headerlink" title="双线索提取"></a>双线索提取</h3><p>首先对光场图像进行重聚焦，然后得到一系列具有不同深度的焦栈。然后对该焦栈分别提取2个线索：散焦量以及匹配量。其中散焦量被定义为：</p>
<script type="math/tex; mode=display">
D_{\alpha}(x)=\frac{1}{|W_{D}|}{\sum _{x' \in W_D} {|\Delta _x{L}_{\alpha}(x')|}},\tag{14}</script><p>其中，$W_D$表示为当前像素领域窗口大小，$\Delta _x$表示水平方向拉式算子，$\overline{L}_{\alpha}(x)$为每个经过平均化后的重聚焦后光场图像，其表达式如下：</p>
<script type="math/tex; mode=display">
\overline{L}_{\alpha}(x)=\frac{1}{N_{u}}\sum _{u'} {L}_{\alpha}(x,u'),\tag{15}</script><p>其中$N_{u}$表示每一个角度域内像素的数目。然后匹配量被定义成如下形式：</p>
<script type="math/tex; mode=display">
{C}_{\alpha}(x)=\frac{1}{|W_{C}|}\sum _{x' \in W_C} {\sigma}_{\alpha}(x'),\tag{16}</script><p>其中，$W_C$表示为当前像素领域窗口大小，${\sigma}_{\alpha}(x)$表示每个宏像素强度的标准差，其表达式为：</p>
<script type="math/tex; mode=display">
{\sigma}_{\alpha}(x)^2=\frac{1}{N_{u}}\sum _{u'} \left({L}_{\alpha}(x,u')-\overline{L}_{\alpha}(x)\right)^2.\tag{17}</script><p>经过以上两个线索可以通过赢者通吃（Winner Takes All，WTA）得到两张原始深度图。注意：对这两个线索使用WTA时略有不同，通过最大化空间对比度可以得到散焦线索对应的深度，最小化角度域方差能够获得匹配量对应的深度。因此二者深度可以分别表示为如下公式：</p>
<script type="math/tex; mode=display">
\alpha ^{*}_D(x)=\mathop{\arg\max}_{\alpha} \ \ {D}_{\alpha}(x).\tag{18}</script><script type="math/tex; mode=display">
\alpha ^{*}_C(x)=\mathop{\arg\min}_{\alpha} \ \ {C}_{\alpha}(x).\tag{19}</script><h3 id="置信度分析及深度融合"><a href="#置信度分析及深度融合" class="headerlink" title="置信度分析及深度融合"></a>置信度分析及深度融合</h3><p><img src="http://oofx6tpf6.bkt.clouddn.com/tao2013-confidence-analysis.png" alt=""></p>
<p>上图中显示了两个线索随着深度层次而变化的曲线。接下来的置信度分析用<strong>主次峰值比例</strong>（Peak Ratio）来定义每种线索的置信度，可表示为如下公式，其中的$\alpha ^{*2}_D(x)$以及$\alpha ^{*2}_C(x)$分别表示曲线的次峰值对应的深度层次。</p>
<script type="math/tex; mode=display">
D_{conf}(x)=\frac{D_{\alpha ^{*}_D}(x)}{D_{\alpha ^{*2}_D}(x)}.\tag{20}</script><script type="math/tex; mode=display">
C_{conf}(x)=\frac{C_{\alpha ^{*}_C}(x)}{C_{\alpha ^{*2}_C}(x)}.\tag{21}</script><p>接下来对原始深度进行MRF置信度融合：</p>
<script type="math/tex; mode=display">
\mathop{minimize}_{Z} \ \ \sum_{source}\lambda _{source} \sum _i W_i|Z_i-Z_i^{source}|</script><script type="math/tex; mode=display">
+\lambda _{flat} \sum _{(x,y)}\left( \left |\frac{\partial Z_i}{\partial x}\right|_{(x,y)}+\left|\frac{\partial Z_i}{\partial y}\right|_{(x,y)}\right)</script><script type="math/tex; mode=display">
 + \lambda _{smooth} \sum _{(x,y)}|\Delta Z_i|_{(x,y)}.\tag{22}</script><p>其中，$source$控制着数据项，即优化后的深度要与原始深度尽量保持一致。第二项与第三项分别控制着平坦性（flatness）与平滑性（smoothness）。注意：<strong>平坦</strong>的意思是物体表面没有凹凸变化的沟壑，例如魔方任一侧面，无论是否拼好（忽略中间黑线）。而<strong>平滑</strong>则表示在平坦的基础上物体表面没有花纹，如拼好的魔方的一个侧面。另外的$W$是权重量，此处选用的是每个线索的置信度。</p>
<script type="math/tex; mode=display">
 \{Z_1^{source},Z_2^{source}\}=\{\alpha_C^{*},\alpha_D^{*}\}.\tag{23}</script><script type="math/tex; mode=display">
 \{W_1^{source},W_2^{source}\}=\{C_{conf},D_{conf}\}.\tag{24}</script><p>至此，该算法介绍完毕，其代码已经放在我的<a href="https://github.com/Vincentqyw/Depth-Estimation-Light-Field/tree/master/LF_DC" target="_blank" rel="external">Github</a>。</p>
<h2 id="学习的方法"><a href="#学习的方法" class="headerlink" title="学习的方法"></a>学习的方法</h2><p>目前而言，将深度学习应用于从双目或者单目中恢复深度已经不再新鲜，我在之前的<a href="https://www.vincentqin.tech/2017/12/06/depth-estimation-using-deeplearning-1/">博文1</a>&amp;<a href="https://www.vincentqin.tech/2017/12/10/depth-estimation-using-deeplearning-2/">博文2</a>中有过对这类算法的介绍。但是将其应用于光场领域进行深度估计的算法还真是寥寥无几。不过总有一些勇敢的践行者去探索如何将二者结合，以下列举几种基于学习的深度估计算法<sup><a href="#fn_18" id="reffn_18">18</a></sup> <sup><a href="#fn_19" id="reffn_19">19</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_28" id="reffn_28">28</a></sup> <sup><a href="#fn_29" id="reffn_29">29</a></sup> <sup><a href="#fn_30" id="reffn_30">30</a></sup>。</p>
<table>
    <tr>
        <td rowspan="5"> Learning-based<br>
           </td><td><b>Approach</b></td>
        <td><b>Main Feature</b></td>
    </tr>
    <tr>
        <td>Johannsen et al. <sup><a href="#fn_18" id="reffn_18">18</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup></td>
        <td>Sparse coding</td>
    </tr>
    <tr>
        <td>Heber et al. <sup><a href="#fn_19" id="reffn_19">19</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_28" id="reffn_28">28</a></sup></td>
        <td>CNN-based</td>
    </tr>

    <tr>
        <td>Jeon et al. <sup><a href="#fn_29" id="reffn_29">29</a></sup></td>
        <td>SAD, SGD, ZNCC, CT, Random Forests</td>
    </tr>

    <tr>
        <td>Shin et al. <sup><a href="#fn_30" id="reffn_30">30</a></sup></td>
        <td>4-Directions EPIs & CNN-based</td>
    </tr>

</table>

<p>在此，我将对截止目前（2018年5月29日）而言，在HCI新数据集上表现最好的<a href="https://arxiv.org/abs/1804.02379" target="_blank" rel="external">EPINET: A Fully-Convolutional Neural Network Using Epipolar Geometry for Depth from Light Field Images</a><sup><a href="#fn_30" id="reffn_30">30</a></sup>算法进行介绍，下图为该算法在各个指标上的表现情况。</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/epinet-architecture.png" alt=""></p>
<p>算法摘要：光场相机能够同时采集空间光线的空域以及角度域信息，因此可以根据这种特性恢复出空间场景的涉深度。在本文中，作者提出了一种基于CNN的快速准确的光场深度估计算法。作者在设计网络时将光场的几何结构加入考虑，同时提出了一种新的数据增强算法以克服训练数据不足的缺陷。作者提出的算法能够在HCI 4D-LFB上在多个指标上取得Top1的成绩。作者指出，光场相机存在优势的同时也有诸多缺点，例如：基线超级短且空间&amp;角度分辨率有一定的权衡关系。目前已有很多工作去克服这些问题，这样一来，深度图像的精度提升了，但是带来的后果就是计算量超级大，无法快速地估计出深度。因此作者为了解决精度以及速度之间权衡关系设计了该算法（感觉很有意义吧）。</p>
<p>上面表格中提到的诸如Johannsen<sup><a href="#fn_18" id="reffn_18">18</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup>以及Heber<sup><a href="#fn_19" id="reffn_19">19</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup> <sup><a href="#fn_28" id="reffn_28">28</a></sup>等人设计的算法仅仅考虑到了一个极线方向，从而容易导致低置信度的深度估计。为了解决他们算法中存在的问题，作者通过一种多流网络将不同的极线图像分别进行编码去预测深度。因为，每个极线图都有属于自己的集合特征，将这些极线图放入网络训练能够充分地利用其提供的信息。</p>
<h3 id="光场图像几何特征"><a href="#光场图像几何特征" class="headerlink" title="光场图像几何特征"></a>光场图像几何特征</h3><p>由于光场图像可以等效成多个视角图像的集合，这里的视角数目通常要比传统的立体匹配算法需要的视角数目多得多。所以，如果利用全部的视角做深度估计将会相当耗时，所以在实际情况下并不需要用到全部的视角。作者的思路就是想办法尽量减少实际要使用的视角数目，所以作者探究了不同角度域方向光场图像的特征。中心视角图像与其余视角的关系可以表示成如下公式：</p>
<script type="math/tex; mode=display">
L(x,y,0,0)=L(x+d(x,y)*u,y+d(x,y)*v,u,v),\tag{25}</script><p>其中$d(x,y)$表示中心视角到其相应相邻视角之间的视差（disparity）。令角度方向为$\theta$（$\tan \theta=v/u$），我们可以将上式改写成如下公式：</p>
<script type="math/tex; mode=display">
L(x,y,0,0)=L(x+d(x,y)*u,y+d(x,y)*u \tan \theta,u,u \tan \theta).\tag{26}</script><p>作者选择了四个方向$\theta$: 0<sup>o</sup>，45<sup>o</sup>，90<sup>o</sup>，135<sup>o</sup>，同时假设光场图像总视角数为$(2N+1)\times(2N+1)$。</p>
<h3 id="网络设计"><a href="#网络设计" class="headerlink" title="网络设计"></a>网络设计</h3><p>如本节开始的图所示的网络结构，该网络的开始为多路编码网络（类似于Flownet以及<a href="https://www.cs.toronto.edu/~urtasun/publications/luo_etal_cvpr16.pdf" target="_blank" rel="external">Efficient Deep Learning for Stereo Matching</a><sup><a href="#fn_32" id="reffn_32">32</a></sup>），其输入为4个不同方向视角图像集合，每个方向对应于一路网络，每一路都可以对其对应方向上图像进行编码提取特征。每一路网络都由3个全卷积模块组成，因为全卷积层对逐点稠密预测问题卓有成效，所以作者将每一个全卷积模块定义为这样的卷积层的集合：<strong>Conv-ReLU-Conv-BN-ReLU</strong>，这样的话就可以在局部块中预逐点预测视差。为了解决基线短的问题，作者设计了非常小的卷积核：$2\times 2$，同时stride = 1，这样的话就可以测量$\pm 4$的视差。为了验证这种多路网络的有效性，作者同单路的网络做了对比试验，其结果如下表所示，可见多路网络相对于单路网络有10%的误差降低。</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/viewpoints-effect.png" width="60%"></p>
<p>在完成多路编码之后，网络将这些特征串联起来组成更维度更高的特征。后面的融合网络包含8个卷积块，其目的是寻找经多路编码之后特征之间的相关性。注意除了最后一个卷积块之外，其余的卷积块全部相同。为了推断得到亚像素精度的视差图，作者将最后一个卷积块设计为<strong>Conv-ReLU-Conv</strong>结构。</p>
<p>最后，图像增强方式包括视角偏移（从9*9视角中选7*7，可扩展3*3倍数据），图像旋转（90<sup>o</sup>，180<sup>o</sup>，270<sup>o</sup>），图像缩放（[0.25,1]），色彩值域变化（[0.5,2]），随机灰度变化，gamma变换（[0.8,1.2]）以及翻转，最终扩充了288倍。</p>
<p>以下为其各个指标上的性能表现：</p>
<p><img src="http://oofx6tpf6.bkt.clouddn.com/epinet-evaluation.png" alt=""></p>
<p>以上介绍了目前已有的深度估计算法不同类别中具有代表性的算法，它们不一定是最优的，但绝对是最容易理解其精髓的。到目前为止，光场领域已经有一大波人做深度估计的工作，利用传统的方式其精度很难再往上提高。随着深度学习的大热，已经有一批先驱开始用深度学习做深度估计，虽然在仿真数据上可以表现得很好，但实际场景千变万化，即使是深度学习的策略也不敢保证对所有的场景都有效。路漫漫其修远兮，深度估计道路阻且长。我认为以后的趋势应该是从EPI图像下手，然后利用CNN提feature（或者响应）；此时可供选择的工具有<a href="http://www.cvlibs.net/datasets/kitti/eval_scene_flow.php?benchmark=stereo" target="_blank" rel="external">KITTI Stereo</a>/<a href="http://hci-lightfield.iwr.uni-heidelberg.de/" target="_blank" rel="external">HCI新数据集算法比较</a>/<a href="http://vision.middlebury.edu/stereo/" target="_blank" rel="external">Middlebury Stereo</a>中较好的几种算法，我们需要总结其算法优势并迁移到光场领域中来。GPU这个Powerful的计算工具一定要用到光场领域中来，发挥出多线程的优势。否则传统的CPU对于动辄上百兆的数据有心无力。这样一来，深度图像不仅仅可以从精度上得以提高，而且深度估计的速度也会更快。至此，本文介绍到此结束。</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><blockquote id="fn_1">
<sup>1</sup>. Gershun, A. “<a href="http://p9kx5cva1.bkt.clouddn.com/1.Gershun-1939-Journal_of_Mathematics_and_Physics.pdf" target="_blank" rel="external">The Light Field</a>.” Studies in Applied Mathematics 18.1-4(1939):51–151.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. Adelson, Edward H, and J. R. Bergen. “<a href="http://p9kx5cva1.bkt.clouddn.com/2.The%20plenoptic%20function%20and%20the%20elements%20of%20early%20vision.pdf" target="_blank" rel="external">The plenoptic function and the elements of early vision</a>. “ Computational Models of Visual Processing (1991):3-20.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_3">
<sup>3</sup>. Levoy, Marc. “<a href="http://p9kx5cva1.bkt.clouddn.com/3.Light_Field_Rendering.pdf" target="_blank" rel="external">Light field rendering</a>.” Conference on Computer Graphics and Interactive Techniques ACM, 1996:31-42.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_4">
<sup>4</sup>. Gortler, Steven J., et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/4.The%20lumigraph.pdf" target="_blank" rel="external">The Lumigraph</a>.” Proc Siggraph 96(1996):43-54.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_5">
<sup>5</sup>. Wu, Gaochang, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/5.Light-Field-Image-Processing-An%20Overview.pdf" target="_blank" rel="external">Light Field Image Processing: An Overview</a>.” IEEE Journal of Selected Topics in Signal Processing PP.99(2017):1-1.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_6">
<sup>6</sup>. Wanner, Sven, and B. Goldluecke. “<a href="http://p9kx5cva1.bkt.clouddn.com/6.Variational%20Light%20Field%20Analysis%20for%20Disparity%20Estimation%20and%20Super-Resolution.pdf" target="_blank" rel="external">Variational Light Field Analysis for Disparity Estimation and Super-Resolution</a>.” IEEE Transactions on Pattern Analysis &amp; Machine Intelligence 36.3(2013):1.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_7">
<sup>7</sup>. Wang, Ting Chun, A. A. Efros, and R. Ramamoorthi. “<a href="http://p9kx5cva1.bkt.clouddn.com/7.Occlusion-aware%20Depth%20Estimation%20Using%20Light-field%20Cameras.pdf" target="_blank" rel="external">Occlusion-Aware Depth Estimation Using Light-Field Cameras</a>.” IEEE International Conference on Computer Vision IEEE, 2016:3487-3495.<a href="#reffn_7" title="Jump back to footnote [7] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_8">
<sup>8</sup>. Jeon, Hae Gon, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/8.Accurate%20Depth%20Map%20Estimation%20from%20a%20Lenslet%20Light%20Field%20Camera.pdf" target="_blank" rel="external">Accurate depth map estimation from a lenslet light field camera</a>.” Computer Vision and Pattern Recognition IEEE, 2015:1547-1555.<a href="#reffn_8" title="Jump back to footnote [8] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_9">
<sup>9</sup>. Yu, Zhan, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/9.Line%20Assisted%20Light%20Field%20Triangulation%20and%20Stereo%20Matching.pdf" target="_blank" rel="external">Line Assisted Light Field Triangulation and Stereo Matching</a>.” IEEE International Conference on Computer Vision IEEE, 2014:2792-2799.<a href="#reffn_9" title="Jump back to footnote [9] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_10">
<sup>10</sup>. Heber, Stefan, and T. Pock. “<a href="http://p9kx5cva1.bkt.clouddn.com/10.Shape%20from%20Light%20Field%20meets%20Robust%20PCA.pdf" target="_blank" rel="external">Shape from Light Field Meets Robust PCA</a>.” Computer Vision – ECCV 2014. 2014:751-767.<a href="#reffn_10" title="Jump back to footnote [10] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_11">
<sup>11</sup>. Kim, Changil, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/11.scene-reconstruction-from-high-spatio-angular-resolution-light-fields-siggraph-2013-compressed-kim-et-al.pdf" target="_blank" rel="external">Scene reconstruction from high spatio-angular resolution light fields</a>.” Acm Transactions on Graphics 32.4(2017):1-12.<a href="#reffn_11" title="Jump back to footnote [11] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_12">
<sup>12</sup>. Li, J., M. Lu, and Z. N. Li. “<a href="http://p9kx5cva1.bkt.clouddn.com/12.Continuous%20Depth%20Map%20Reconstruction%20From%20Light%20Fields.pdf" target="_blank" rel="external">Continuous Depth Map Reconstruction From Light Fields</a>.” IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society 24.11(2015):3257.<a href="#reffn_12" title="Jump back to footnote [12] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_13">
<sup>13</sup>. Krolla, Bernd, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/13.Spherical%20light%20field.pdf" target="_blank" rel="external">Spherical Light Fields</a>.” British Machine Vision Conference 2014.<a href="#reffn_13" title="Jump back to footnote [13] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_14">
<sup>14</sup>. Wanner, Sven, C. Straehle, and B. Goldluecke. “<a href="http://p9kx5cva1.bkt.clouddn.com/14.Globally%20consistent%20multi-label%20assignment%20on%20the%20ray%20space%20of%204D%20light%20field.pdf" target="_blank" rel="external">Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields</a>.” IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2013:1011-1018.<a href="#reffn_14" title="Jump back to footnote [14] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_15">
<sup>15</sup>. Diebold, Maximilian, B. Jahne, and A. Gatto. “<a href="http://p9kx5cva1.bkt.clouddn.com/15.Heterogeneous%20Light%20Fields.pdf" target="_blank" rel="external">Heterogeneous Light Fields</a>.” Computer Vision and Pattern Recognition IEEE, 2016:1745-1753.<a href="#reffn_15" title="Jump back to footnote [15] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_16">
<sup>16</sup>. Tao, M. W, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/16.Depth%20from%20Combining%20Defocus%20and%20Correspondence%20Using%20Light-Field%20Cameras.pdf" target="_blank" rel="external">Depth from Combining Defocus and Correspondence Using Light-Field Cameras</a>.” IEEE International Conference on Computer Vision IEEE Computer Society, 2013:673-680.<a href="#reffn_16" title="Jump back to footnote [16] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_17">
<sup>17</sup>. Tao, Michael W., et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/17.Depth%20from%20Shading,%20Defocus,%20and%20Correspondence%20Using%20Light-Field%20Angular%20Coherence.pdf" target="_blank" rel="external">Depth from shading, defocus, and correspondence using light-field angular coherence</a>.” Computer Vision and Pattern Recognition IEEE, 2015:1940-1948.<a href="#reffn_17" title="Jump back to footnote [17] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_18">
<sup>18</sup>. Johannsen, Ole, A. Sulc, and B. Goldluecke. “<a href="http://p9kx5cva1.bkt.clouddn.com/18.Variational%20separation%20of%20light%20field%20layers.pdf" target="_blank" rel="external">Variational Separation of Light Field Layers</a>.” (2015).<a href="#reffn_18" title="Jump back to footnote [18] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_19">
<sup>19</sup>. Heber, Stefan, and T. Pock. “<a href="http://p9kx5cva1.bkt.clouddn.com/19.Heber_Convolutional_Networks_for_CVPR_2016_paper.pdf" target="_blank" rel="external">Convolutional Networks for Shape from Light Field</a>.” Computer Vision and Pattern Recognition IEEE, 2016:3746-3754.<a href="#reffn_19" title="Jump back to footnote [19] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_20">
<sup>20</sup>. Heber, Stefan, R. Ranftl, and T. Pock. “<a href="http://p9kx5cva1.bkt.clouddn.com/20.Variational%20Shape%20from%20Light%20Field.pdf" target="_blank" rel="external">Variational Shape from Light Field</a>.” Energy Minimization Methods in Computer Vision and Pattern Recognition. Springer Berlin Heidelberg, 2013:66-79.<a href="#reffn_20" title="Jump back to footnote [20] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_21">
<sup>21</sup>. Chen, Can, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/21.Light%20Field%20Stereo%20Matching%20Using%20Bilateral%20Statistics%20of%20Surface%20Cameras-Can_CVPR14_stereo.pdf" target="_blank" rel="external">Light Field Stereo Matching Using Bilateral Statistics of Surface Cameras</a>.” IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2014:1518-1525.<a href="#reffn_21" title="Jump back to footnote [21] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_22">
<sup>22</sup>. Williem W, Kyu P I. “<a href="http://p9kx5cva1.bkt.clouddn.com/22.Williem_Robust_Light_Field_CVPR_2016_paper.pdf" target="_blank" rel="external">Robust light field depth estimation for noisy scene with occlusion</a>.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016:4396-4404.<a href="#reffn_22" title="Jump back to footnote [22] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_23">
<sup>23</sup>. Williem W, Park I K, Lee K M. “<a href="http://p9kx5cva1.bkt.clouddn.com/23.TPAMI2017_Williem.pdf" target="_blank" rel="external">Robust light field depth estimation using occlusion-noise aware data costs</a>.” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017(99):1-1.<a href="#reffn_23" title="Jump back to footnote [23] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_24">
<sup>24</sup>. Zhang S, Sheng H, Li C, et al. “<a href="http://p9kx5cva1.bkt.clouddn.com/24.Robust%20Depth%20Estimation%20for%20Light%20Field%20via%20Spinning%20Parallelogram%20Operator.pdf" target="_blank" rel="external">Robust depth estimation for light field via spinning parallelogram operator</a>.” Computer Vision and Image Understanding, 2016, 145:148-159.<a href="#reffn_24" title="Jump back to footnote [24] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_25">
<sup>25</sup>. Johannsen O, Sulc A, Goldluecke B. “<a href="http://p9kx5cva1.bkt.clouddn.com/25.What%20Sparse%20Light%20Field%20Coding%20Reveals%20about%20Scene%20Structure.pdf" target="_blank" rel="external">What sparse light field coding reveals about scene structure</a>.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016(1/3/4):3262-3270.<a href="#reffn_25" title="Jump back to footnote [25] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_26">
<sup>26</sup>. Wanner S, Goldluecke B. “<a href="http://p9kx5cva1.bkt.clouddn.com/26.Reconstructing%20reflective%20and%20transparent%20surfaces%20from%20epipolar%20plane%20images.pdf" target="_blank" rel="external">Reconstructing reflective and transparent surfaces from epipolar plane images</a>.” In German Conference on Pattern Recognition (Proc. GCPR), 2013:1-10.<a href="#reffn_26" title="Jump back to footnote [26] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_27">
<sup>27</sup>. Heber S, Yu W, Pock T. “<a href="http://p9kx5cva1.bkt.clouddn.com/27.U-shaped%20Networks%20for%20Shape%20from%20Light%20Field.pdf" target="_blank" rel="external">U-shaped networks for shape from light field</a>.” British Machine Vision Conference, 2016, 37:1-12.<a href="#reffn_27" title="Jump back to footnote [27] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_28">
<sup>28</sup>. Heber S, Yu W, Pock T. “<a href="http://p9kx5cva1.bkt.clouddn.com/28.Neural%20EPI-volume%20Networks%20for%20Shape%20from%20Light%20Field.pdf" target="_blank" rel="external">Neural EPI-Volume networks for shape from light field</a>.” IEEE International Conference on Computer Vision (ICCV), IEEE Computer Society, 2017:2271-2279.<a href="#reffn_28" title="Jump back to footnote [28] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_29">
<sup>29</sup>. Jeon H G, Park J, Choe G, et.al. “<a href="http://p9kx5cva1.bkt.clouddn.com/29.Depth%20from%20a%20Light%20Field%20Image%20with%20Learning-based%20Matching%20Costs.pdf" target="_blank" rel="external">Depth from a Light Field Image with Learning-based Matching Costs</a>.” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018.<a href="#reffn_29" title="Jump back to footnote [29] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_30">
<sup>30</sup>. Shin C, Jeon H G, Yoon Y. “<a href="http://p9kx5cva1.bkt.clouddn.com/30.EPINET%20A%20fully-Convolutional%20Neural%20Network%20Using%20Epipolar%20Geometry%20for%20Depth%20from%20Light%20Field%20Images.pdf" target="_blank" rel="external">EPINET: A Fully-Convolutional Neural Network for Light Field Depth Estimation Using Epipolar Geometry</a>.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.<a href="#reffn_30" title="Jump back to footnote [30] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_31">
<sup>31</sup>. Ng, Ren. “<a href="http://p9kx5cva1.bkt.clouddn.com/31.Digital%20light%20field%20photography.pdf" target="_blank" rel="external">Digital light field photography</a>.” 2006, 115(3):38-39.<a href="#reffn_31" title="Jump back to footnote [31] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_32">
<sup>32</sup>. Luo, Wenjie, A. G. Schwing, and R. Urtasun. “<a href="http://p9kx5cva1.bkt.clouddn.com/32.Efficient%20deep%20learning%20for%20stereo%20matching.pdf" target="_blank" rel="external">Efficient Deep Learning for Stereo Matching</a>.” IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2016:5695-5703.<a href="#reffn_32" title="Jump back to footnote [32] in the text."> &#8617;</a>
</blockquote>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Thanks for Your Kindly Donation.</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>Donate</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechat_pay.png" alt="Vincent Qin WeChat Pay"/>
        <p>WeChat Pay</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/ali_pay.png" alt="Vincent Qin Alipay"/>
        <p>Alipay</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    Vincent Qin
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="https://www.vincentqin.tech/posts/light-field-depth-estimation/" title="Light Field Depth Estimation">https://www.vincentqin.tech/posts/light-field-depth-estimation/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/depth-estimation/" rel="tag"><i class="fa fa-star"></i> depth estimation</a>
          
            <a href="/tags/light-field/" rel="tag"><i class="fa fa-star"></i> light field</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/posts/cv-books/" rel="prev" title="CV Related References">
                CV Related References <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
      
          <div onclick="showGitalk();" id="gitalk_title" class="gitalk_title">Click Gitalk Comments</div>
      
      <div id="gitalk_container" style="display:none;"></div>
  
	
  
    <div class="comments" id="comments">
      <div id="hypercomments_widget"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/qin.png"
                alt="Vincent Qin" />
            
              <p class="site-author-name" itemprop="name">Vincent Qin</p>
              <p class="site-description motion-element" itemprop="description">Keep Your Curiosity.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">72</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/Vincentqyw" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:qin123yw@163.com" target="_blank" title="Email">
                    
                      <i class="fa fa-fw fa-envelope"></i>Email</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://coding.net/vincentqin" target="_blank" title="Coding">
                    
                      <i class="fa fa-fw fa-code"></i>Coding</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://weibo.com/273224402" target="_blank" title="Weibo">
                    
                      <i class="fa fa-fw fa-weibo"></i>Weibo</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.vincentqin.tech/images/qcode.jpg" target="_blank" title="Wechat">
                    
                      <i class="fa fa-fw fa-weixin"></i>Wechat</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/i_vincent/activities" target="_blank" title="Zhihu">
                    
                      <i class="fa fa-fw fa-quora"></i>Zhihu</a>
                </span>
              
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-battery-1"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://github.com/tensorboy" title="Tensorboy" target="_blank">Tensorboy</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://simtalk.cn/" title="Simshang" target="_blank">Simshang</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://sttomato.github.io" title="Tomato" target="_blank">Tomato</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://newdee.cf/" title="Newdee" target="_blank">Newdee</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://qianli666.com/" title="QianLi" target="_blank">QianLi</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://cs-people.bu.edu/yfhu/" title="WhoIf" target="_blank">WhoIf</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://yulunzhang.com/" title="Yulun" target="_blank">Yulun</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://yqchen.com/" title="Yanqin" target="_blank">Yanqin</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://sanglongbest.github.io/" title="YangLiu" target="_blank">YangLiu</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.dandyweng.com/" title="DandyWeng" target="_blank">DandyWeng</a>
                  </li>
                
              </ul>
            </div>
          

          <div id="days"></div>
</script>
<script language="javascript">
function show_date_time(){
window.setTimeout("show_date_time()", 1000);
BirthDay=new Date("09/15/2016 9:00:00");
today=new Date();
timeold=(today.getTime()-BirthDay.getTime());
sectimeold=timeold/1000
secondsold=Math.floor(sectimeold);
msPerDay=24*60*60*1000
e_daysold=timeold/msPerDay
daysold=Math.floor(e_daysold);
e_hrsold=(e_daysold-daysold)*24;
hrsold=setzero(Math.floor(e_hrsold));
e_minsold=(e_hrsold-hrsold)*60;
minsold=setzero(Math.floor((e_hrsold-hrsold)*60));
seconds=setzero(Math.floor((e_minsold-minsold)*60));
document.getElementById('days').innerHTML="Running "+daysold+"DAYS "+hrsold+":"+minsold+":"+seconds+"";
}
function setzero(i){
if (i<10)
{i="0" + i};
return i;
}
show_date_time();
</script>

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#什么是光场？"><span class="nav-number">1.</span> <span class="nav-text">什么是光场？</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#光场参数化表示"><span class="nav-number">1.1.</span> <span class="nav-text">光场参数化表示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#光场的可视化"><span class="nav-number">1.2.</span> <span class="nav-text">光场的可视化</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#光场的获取"><span class="nav-number">2.</span> <span class="nav-text">光场的获取</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#光场深度估计算法分类"><span class="nav-number">3.</span> <span class="nav-text">光场深度估计算法分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#多视角立体匹配"><span class="nav-number">3.1.</span> <span class="nav-text">多视角立体匹配</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#相移理论"><span class="nav-number">3.1.1.</span> <span class="nav-text">相移理论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#匹配代价构建"><span class="nav-number">3.1.2.</span> <span class="nav-text">匹配代价构建</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于EPI的方法"><span class="nav-number">3.2.</span> <span class="nav-text">基于EPI的方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#散焦及融合的方法"><span class="nav-number">3.3.</span> <span class="nav-text">散焦及融合的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#双线索提取"><span class="nav-number">3.3.1.</span> <span class="nav-text">双线索提取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#置信度分析及深度融合"><span class="nav-number">3.3.2.</span> <span class="nav-text">置信度分析及深度融合</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习的方法"><span class="nav-number">3.4.</span> <span class="nav-text">学习的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#光场图像几何特征"><span class="nav-number">3.4.1.</span> <span class="nav-text">光场图像几何特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网络设计"><span class="nav-number">3.4.2.</span> <span class="nav-text">网络设计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        

<div class="copyright">&copy; 2016 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="heart">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Vincent Qin</span>
  	
  
</div>








  <div class="footer-custom">Hosted by <a href="https://pages.coding.me" id="coding">Coding Pages</a></div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user-circle-o"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








  <div style="display: none;">
    <script src="//s95.cnzz.com/z_stat.php?id=1273219530&web_id=1273219530" language="JavaScript"></script>
  </div>



        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  






	<script type="text/javascript">
	_hcwp = window._hcwp || [];

	_hcwp.push({widget:"Bloggerstream", widget_id: 98388, selector:".hc-comment-count", label: "{\%COUNT%\}" });

	
	_hcwp.push({widget:"Stream", widget_id: 98388, xid: "posts/light-field-depth-estimation/"});
	

	(function() {
	if("HC_LOAD_INIT" in window)return;
	HC_LOAD_INIT = true;
	var lang = ("en" || navigator.language || navigator.systemLanguage || navigator.userLanguage).substr(0, 2).toLowerCase();
	var hcc = document.createElement("script"); hcc.type = "text/javascript"; hcc.async = true;
	hcc.src = ("https:" == document.location.protocol ? "https" : "http")+"://w.hypercomments.com/widget/hc/98388/"+lang+"/widget.js";
	var s = document.getElementsByTagName("script")[0];
	s.parentNode.insertBefore(hcc, s.nextSibling);
	})();
	</script>


















  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
  <script>
    function showGitalk(){
        $("#gitalk_title").attr("style","display:none");
        $("#gitalk_container").attr("style","").addClass("gitalk_container");

        var gitalk = new Gitalk({
            id: window.location.pathname,
            clientID: 'fbf87ffe2080934209fc',
            clientSecret: '23048ae0f682b6e34844f41d2b84d171bb7b2854',
            repo: 'gitalk',
            owner: 'Vincentqyw',
            admin: ['Vincentqyw'],
            distractionFreeMode: true,
            body: location.href
        });

        gitalk.render('gitalk_container');
    }

    

  </script>

  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("RQs4FrsjFMNDD6sbzzHlN3ei-gzGzoHsz", "LQziYhODk8GWxR5yDnx4IT0Q");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.3"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.3"></script>


  

  
  

  <!--������ƭ-->
  <script type="text/javascript" src="/js/src/breakdown.js"></script>

  <!-- ҳ������С���� -->
  <script type="text/javascript" src="/js/src/love.js"></script>

  <!-- add gitter on sidebar -->

  <script>
    ((window.gitter = {}).chat = {}).options = {
      room: 'vincentqin-blog-chat/Lobby'
    };
  </script>
  <script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async defer></script>


</body>

</html>
