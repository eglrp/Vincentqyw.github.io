<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[笔记：清华-谷歌人工智能研讨会(Tsinghua-Google AI Symposium)]]></title>
      <url>%2Fposts%2Ftsinghua-google-ai%2F</url>
      <content type="text"><![CDATA[6 月 28 日，在清华大学人工智能研究院成立仪式暨清华-谷歌 AI 学术研讨会开幕式上，清华大学副校长尤政宣布成立清华大学人工智能研究院。张钹院士担任新研究院的院长，与此同时，谷歌 AI 负责人 Jeff Dean 也成为了清华大学计算机学科顾问委员会委员。 毕业季申请了一下Tsinghua和google主办的研讨会（Tsinghua-Google AI Symposium），有幸被选上参会。在此，我就此时研讨会进行一下总结。会议共分为两天进行。会上聚集了多个学术界以及产业界的大牛，其中包括Bo Zhang，Jeff Dean，Fei Fei Li，Bill Freeman等大佬。他们分别在其专业领域进行了主题演讲（Keynote）。 Bo Zhang（张钹院士）出任清华谷歌AI研究院院长的张钹是清华大学计算机系教授，中科院院士。 1958 年毕业于清华大学自动控制系，同年留校任教至今。他在 2011 年获汉堡大学授予的自然科学荣誉博士，并现任微软亚洲研究院技术顾问。 （上图）张钹院士进行名为“Towards A Real Artifical Intelligence”的主题报告 目前的AI并不能进行理解，它不是真正意义上的AI。例如，一个目标识别系统就仅仅是个机械的分类器，它与人类的感知能力大相径庭。因为像这样的分类器仅仅能够区分物体，它并不能真正进行理解以及判定目标。因此呢，目前的AI能够在完美的、静态的、具有明确信息的单任务场景中胜任。为了对其进行拓展，我们需要建立真正意义上的AI。 张钹院士主要参与人工智能、人工神经网络、机器学习等理论研究，以及这些理论应用于模式识别、知识工程与机器人等技术研究。在这些领域，张钹院士已发表 200 多篇学术论文和 5 篇（或章节）专著（中英文版），且专著获得国家教委高等学校出版社颁发的优秀学术专著特等奖。 张钹院士的科研成果分别获得 ICL 欧洲人工智能奖、国家自然科学三等奖、国家科技进步三等奖、国家教委科技进步一、二等奖、电子工业部科技进步一等奖以及国防科工委科技进步一等奖奖励。此外，张钹院士还参与创建智能技术与系统国家重点实验室，于 1990‐1996 年担任该实验室主任。 在过去 30 多年中，张钹院士提出问题求解的商空间理论，在商空间数学模型的基础上，提出了多粒度空间之间相互转换、综合与推理的方法。此外，张院士还提出了问题分层求解的计算复杂性分析以及降低复杂性的方法。该理论与相应的新算法已经应用于不同领域，如统计启发式搜索、路径规划的拓扑降维法、基于关系矩阵的时间规划以及多粒度信息融合等，这些新算法均能显著降低计算复杂性。在人工神经网络上，张院士提出基于规划和基于点集覆盖的学习算法。这些自顶向下的结构学习方法比传统的自底向上的搜索方法在许多方面具有显著优越性。 根据 Google Scholar，张院士引用量最高的研究论文主要关注于神经网络的稳定性分析、神经网络的建模方法等，它们都是在深度学习崛起之前做的研究，由此可见张院士是较早研究这种层级表征模型的研究者。 Jeff Dean：快点用深度学习解决问题吧！ （上图）Jeff Dean进行名为“Deep Learning to Solve Challenging Problems”的主题报告 Jeff Dean 大神无需过多介绍，想必大家都了解。这次他又多了一个新身份：受聘新成立的清华AI研究院学科顾问委员会委员，所以他也罕见的一身正装范儿。 在此次报告中，我将介绍我们团队正在做的工作：建立高性能，大规模机器学习系统。此次报告涉及特定硬件（Tensor Processing Units，张量处理单元）的设计初衷以及细节，同样地也会介绍诸如TensorFlow这样的软件架构如何被设计成为能够允许ML研究者方便地进行算法实现以解决困难问题的利器。同样也会讨论目前涌现的能够解决困难问题的机器学习算法，它能够让我们无须手动调节参数以及设计算法，取而代之的是让机器去学习这些东西，我们可利用该算法建立自适应以及灵活的机器学习系统。 过去6年来，Google Brain团队一直在研究人工智能中的难题，构建用于机器学习研究的大型计算机系统，并与Google的许多团队合作，将其研究和系统应用于众多Google产品当中。他们已经在计算机视觉，语音识别，语言理解，机器翻译，医疗保健，机器人控制等领域取得了重大进展。谷歌在人工智能领域最终目标是三点：利用人工智能和机器学习让谷歌的产品更加实用（Making products more useful）；帮助企业和外部开发者利用人工智能和机器学习进行创新（Helping others innovate）；为研究人员提供更好的工具，解决人类面临的重大挑战。 演讲从深度学习热潮的兴起讲起：从2010年开始，深度学习的热度稳步上升，如今Arxiv上发表的机器学习论文增长趋势已经超过了摩尔定律。深度学习在图像和语音识别为代表的一系列任务中取得了越来越卓越的成果，这个概念和技术并不是全新的，但为什么在过去的几年当中实现了极大的突破？这一切都得益于计算力的提升，在有充分计算力的情况下，深度学习解决问题的精度将大幅超越传统方法。 在2008年美国工程院列出的14大“21世纪重大工程难题”中，有5项都能用到深度学习和机器学习，甚至用深度学习和机器学习去解决，包括环境问题、城市基础设施，健康医疗，以及人脑的逆向工程。Jeff Dean本人还添加了两项，他认为不受语言限制获取信息和交流，以及构建灵活通用的AI系统也十分重要，而这两点也需要深度学习。 （上图）2008：21世纪重大工程难题 接下来，Jeff Dean重点介绍了一些Google Brain团队已经完成的研究和计算机系统工作，着眼于如何使用深度学习来解决具有挑战性的问题，来证明深度学习的有效性： 提高城市基础设施方面，Waymo的自动驾驶已经离实际应用越来越近。 在健康信息学方面，谷歌用深度学习分析糖尿病视网膜图像，算法的准确率已经超越了人类医生；不仅如此，使用深度学习视网膜图像分析来预测心血管疾病突发风险，获得人体解剖学和疾病变化之间的联系，这是人类医生此前完全不知道的诊断和预测方法，不仅能帮助科学家生成更有针对性的假设，还可能代表了科学发现的新方向。此外，谷歌还与顶级医学院合作使用深度学习分析电子病例，预测患者预后等情况，已经取得了不错的初步成果。 促进跨语言的交流和信息共享，有谷歌的神经机器翻译（GNMT），GNMT在多个语种的翻译上平均质量提高50%到80%以上，超过了过去十年的进展，而且谷歌还开放了基于TensorFlow的源代码。Jeff Dean特别提到，谷歌的目标是一百多种语言对之间相互翻译，这是一个非常复杂的工程问题，使用同一个基于神经网络的模型去翻译不同的语种，在工程上大大简化了工作量。 在人脑逆向工程方面，谷歌和马克思普朗克研究所等机构合作，从理解大脑神经网络的图像入手，重构生物神经网络。目前，使用马克思普朗克研究所的数据，研究人员已经生成了大约6000亿个体素。他们还提出了一种模拟生成神经网络的算法“Flood Filling Networks”，可以使用原始数据，利用此前的预测，自动跟踪神经传导。 其他还有使用深度学习预测分子性质，制作更好的药物，开发碳封存方法，管理氮循环……这些问题都能够在更好的科学工具的帮助下实现。而这个帮助科学工具开发的工具，就是谷歌深度学习开源框架TensorFlow：TensorFlow的目标是成为每个人都可以使用的机器学习平台，成为通用的平台，成为最好的平台，去更好的促进行业交流和创新。 TensorFlow是目前全球最受欢迎的深度学习框架，在中国也有强劲的开发者生态。此前一位参与TensorFlow开发的中国开发者声称，他认为谷歌推广TensorFlow不是为了赚钱，而是很纯粹的为了技术。 “2017年以前，谷歌并没有在中国展开太多活动。尽管谷歌知道中国市场很大，但很多业务无法展开。即使谷歌的云业务服务器能在中国大陆运行，但是由于阿里巴巴等本土竞争对手也在销售便宜的云计算产品，这使得谷歌难以盈利。但是，我们所有的中国开发者都在等待谷歌来中国，推出更多TensorFlow技术和产品。” 谷歌当然明白这一点，而包括这次研讨会在内的众多高校活动，将进一步把TensorFlow的用户人群拓展到学生里面。最后，这位谷歌AI的总负责人号召大家都使用深度学习：“深度神经网络和机器学习取得的重大突破，正在解决世界上一些最为重大的挑战；如果你还没有考虑使用深度学习，我几乎可以肯定你应该马上这么做！” Feifei Li：机器理解人类，提供医疗环境智能 李飞飞进行名为“Illuminating the Dark Space: Towards Ambient Intelligence in AI-assisted Healthcare”的主题讲座 今天给大家分享的实际上是最近五六年以来一次比较新的探索，虽然大家知道我做的很多研究很多都是计算机领域机器学习的基础科学，但是在应用方面，我们一直坚信“以人为本”的AI，需要对人类的福祉有所帮助。最重视的一个应用领域是医疗健康领域，所以，此次讲座我将给大家分享一下，最近两年我们在医疗健康领域的一些探索，还希望听取大家的意见。 接下来就是此次报告的正文。 在开始之前，我想向众多的合作方、学生、博士后，特别是在过去的五六年时间里，与我们在 AI 和医疗健康领域一起工作的临床医生表示感谢。除了以上与我列举的这部分人员合作之外，我们还与世界各地的医院合作，包括斯坦福大学Lucile Packard儿童医院以及斯坦福大学医学院。此外，我们还与犹他州山间麦凯迪医院、旧金山Unlock高级中心合作。刚刚，我们还与上海交通大学以及瑞金医院开展了一项令人兴奋的研究合作。 李飞飞团队成员 何为Ambient Intelligence？在中国和美国，医疗健康都是最受关注的问题。不断提高的成本，是全球医疗健康的主要问题之一。虽然医疗成本不断上涨，但质量并不见得会一定提高。那我们又该如何提高医疗质量呢？削减成本是目前主要的研究和提高的方向。但幸运的是，在过去的十年里，推进医疗方面的工作已取得了很大的成就。 我们已经看到药物和疫苗上的改进。我们看到了医疗影像的改进，医疗设备等方面巨大的进步。正如我的同事 Jeff Dean 在上午的分享中提到的那样，大数据和人工智能正推动医疗健康特别是诊断方面的进一步发展。此外，精密医学、药物发现相关的治疗选择，正基于机器学习、人工智能取得了新的进展。但是，在过去几年里我关注的医疗健康领域里，有一个往往被多数人忽略的领域，即医疗健康服务的物理空间。如果你考虑到了“医疗”这个词，那么“疗”这个词则非常重要。因为，物理空间指的是临床医生、护士、医生为治疗病人的地方。我们需要在一定的物理环境下通过与患者的互动来提升医疗服务的水准。 因此，在这段时间里我们在斯坦福大学研究的方向是，赋予医疗物理空间“Ambient Intelligence”的属性。让我先来定义下“Ambient Intelligence”的概念。需要说明的是，我们并非第一个想到这个概念，而这个概念也并非特属于医疗领域。一个可接受的定义是：未来将是一个环境满足需求的世界，多数情况下我们无需思考，智能也会萦绕空间，就像这个房间里的灯光。你感受不到科技的存在，但它就在那里，帮助我们更好地做一些事情，这就是我们所说的“Ambient Intelligence”。 (这里有相关视频介绍（需翻墙）：https://www.youtube.com/watch?reload=9&amp;v=5RTkhfVIW40) 那么，为什么我们需要变得智能？为什么我们需要提高医疗健康的服务质量？这是因为，执行和操作是临床医生在医疗服务中的一大痛点。在医疗领域，我们通过数百年知识的积累，需要在各种程序中完成预期的操作，而实际上，符合预期的操作并不总是发生。当出现小毛病、疏忽或错误时，就会涉及医疗成本。而这种成本，往往关乎人类的生命。 事实上，如果与一年内车祸死亡的人数相比的话，医疗事故引发的死亡人数远远高于前者。所以，这对我们而言是一个非常重要的问题。如果 AI 可以用来帮助解决这个问题，那么这会是一个以人为本的应用。在美国，国家医学研究所每隔几年就会针对医疗服务中出现的人为错误进行深度研究。这是我们思考的起点。 为什么临床医生会在医疗中犯错误呢？这一切都是靠人的主观意识完成的。在一个高度复杂的环境下，治疗到什么样的程度也是非常复杂，中间有很多步骤和程序，也有很多的不确定性和不可预测性。 而且，错误或疏忽等都会导致这些问题的发生。所以，当潜在的错误都可以预测时， 便意味着以上医疗问题都能得以解决。例如，病人可能会从床上掉下来，就需要通过行为活动传感器以检查患者是否坠落。再或者检查是否需要进行手部卫生的处理，与之相关的传感器就被发明出来，试图解决这个问题。此外，还有许多不同类型的本地化解决方案试图缩小医疗健康质量与服务之间的差距。 这关键就在于高度本地化。 每当出现一个错误或潜在的缺陷，就需要一个新的解决方案，且不具备可扩展性。这些本地化解决方案有很多不同的情景无法预测、监控。 那么我们能做些什么呢？ 有另一种方式可以考虑改善医疗健康的质量。大概五年前，我和斯坦福的同事们就开始跟进一种新的技术浪潮——自动驾驶技术，而这种新技术似乎与医疗健康毫无关系。但事实上，它们是高度相关的。 先来看看自动驾驶汽车是如何工作的。 这是一款配备了智能传感器的汽车，它能够感知从行人，到汽车、物体、路标等的道路环境。而且，一旦它能感知环境，就会将信息输入到后台，你就能利用机器学习算法做出决定和预测，辅助汽车驾驶。所以，我们受到这种思维以及“Ambient Intelligence”概念的启发，想要将 AI 注入到医疗服务的物理空间中，以便我们能够协助执行预期的步骤。 这是一个医院单元的示意图：由许多传感器覆盖，可以观察不同的医疗服务情况。首先，我们需要通过传感器的性能来改造物理空间，如果是一家（设备）传统的医院，它可能就没有现代化的传感器以帮助收集并将这些潜在的信息传递给算法。接下来，一旦我们收集了数据，我们需要辨认出在这个环境里的活动，无论是手术室、病人康复室，还是在养老院里。 而辨认出该医疗活动的关键因素在于对人类活动的理解进行可视化。现在，如果你来自计算机视觉领域，那么你可以将医疗应用与计算机视觉的基础科学联系起来。事实上，多年以来，理解人类活动一直是计算机视觉的核心问题。所以，我将展示一些可以帮医疗服务环境提升的基础科学研究。最终我们希望整个医疗数据可以整合到整个医疗生态体系中。 让AI注入医疗服务的每一环节接下来的演讲中，通过展示我们最近的一些工作，我将分享到以下三个研究方向：感知、人类活动识别，以及医疗生态体系。 感知首先是感知，即将传感器集成到物理空间，并构建一个数据基础架构的过程。我们最近发表了一篇论文，讨论了我们在试点中所做的工作。我想问在座的各位：在医疗服务环境中，基于“Ambient Intelligence”的感应系统最重要的部分是什么？ 一是隐私，这是非常重要的。患者需要隐私，临床医生也需要隐私。 二是通过空间进行感知。刚才提到的本地化解决方案，其部分问题是因为太过于本地化了，很难在空间上扩展。 三是根据时间进行感知。如果人类来观测活动，他们往往会感到厌倦。 所以，我们想利用机器并且将其变得可扩展。在过去的数十年里，现代传感器已经有了很大的发展。那些曾经玩过 Xbox 视频游戏的玩家，应该都知道深度传感器，它可以用来保护隐私。 如何通过深度传感器收集人类活动的数据？ 在我们的两所试点医院（犹他州儿童医院和承认重症监护病房）中，我们进行了深度传感器的试用。这些深度传感器被安装在医院的病房中。例如，在儿童医院，我们安装了将近30个不同的传感器，目的就是为了通过传感器获取更多的数据以理解人类的行为。 还有一种传感器，它与前者相互补，主要作用于生理信息，即热传感器。通过深度传感器可以看到病人轮廓；而通过热传感器收集信息，你不仅可以看到病人的轮廓，你实际上还能看到其他关键的物体，如氧气管。这对病人而言是非常重要的。所以，在我们的试点研究中，我们同样也会用到热传感器。实际上，我们正在与旧金山的一所养老院合作，在养老院里安装了热传感器和深度传感器，以帮助医生监测老人的行为，帮助他们独立生活。 其实，将传感器投放在医疗环境中，数据基础架构的建设就已经面临着巨大挑战。例如，持续的数据源就意味着大量数据的涌入。如果我们使用传感器的原始分辨率，就会出现需要处理海量数据的问题。因此，我们进行了一些自适应抽样以减少要处理的数据。这些都是我们必须面临着的技术挑战。但我们依然保持着：对人类行为识别的计算机视觉研究的专注，也希望应对医疗环境下的种种挑战，为计算机视觉的基础科学研究做出贡献。 人类活动识别视觉智能，指的是在动态物理世界中发生的过程。谈到动态这个概念，有很多的信息、事物转瞬即逝。这意味着：我们有时会处理之前从未见过的情况。例如，在医疗环境这种复杂的情况下，这名患者在地板上睡了会儿，在床上又睡了会儿。这并非是我们通常利用数据进行训练的场景。所以，这种问题有待解决。 在医疗场景中，我们还要处理物理空间的限制问题。比如一般计算机视觉处理的都是类似 YouTube 用户上传的视频，但是医院的空间有限，因此传感器的装设位置也受到限制，拍摄到的都是各种角度的画面，非常具有挑战性。同样重要的是，我们还会面临计算效率的问题，因为我们希望为临床医师提供实时反馈，因此计算效率极为重要。 人类活动识别是目前计算机视觉领域最受关注的方向之一，目前也已经一些公开的数据集，而且也有很多非常不错的工作。我想介绍的是，我们是如何把我们的工作和医疗健康应用相结合的。第一个是发表在 ECCV 16 上的一篇论文，论述了如何处理不同视野角度的问题，这只是最基础的。我们用到了很多深度学习结构，比如这个用来做图像分类的 Vanilla CNN 网络。 比如，我们希望检测临床医师在进出病人的房间前后是否都有洗手，就需要面临很多的挑战。首先，由于我们的传感器大都安在天花板上，因此画面的视角和正常的 YouTube 视频画面的视角非常不一样。此外，人是运动的，因此我们安装了很多传感器，来对人进行追踪。 我这里简单介绍下视角问题。我们使用了 Vanilla CNN 网络来做分类，唯一的变化就是我们增加了一个转换网络（transformer network），来解决训练数据的视角问题。然后，为了解决多个传感器的追踪问题，我们将不同的个体进行 ground projection，然后将整个 3D 空间的投影结合起来，进行联合优化，以此来追踪不同的个体。 为什么我们会选择手部卫生作为第一个应用案例呢？因为不注意手部卫生是病人死亡的重要元凶之一。实际上，因为每年死于医院获得性感染的病人是交通事故致死人数的三倍。 而大多数的医院获得性感染都是没有注意手部卫生导致的。这是医疗系统里的一个顽疾，解决这个问题的唯一办法是派人到医院里监督医生和护士，督促他们洗手。但是这种方法非常低效，不仅不能做到实施监督，也非常耗费时间，而且人也可能会犯错误。因此，通过使用深度学习和智能传感器系统来对医务人员进行追踪，我们取得了非常好的结果。和人类检查员相比，我们的方法观察到与事实更接近。 我们的系统可以追踪医务人员的行动轨迹，而这些数据对医疗系统来说非常宝贵，不仅仅可以用来追踪医务人员洗手了没，还可以用来优化工作流程。结合智能传感器和计算机视觉识别系统，我们在手部卫生检测领域取得了鼓舞人心的结果。下一步，我们将会把反馈信号实时传递到周遭环境中，以此来督促大家洗手。不过，这还远远不够，我们还需要理解各种不同的行为，观察它们，最终帮助医生和护士优化治疗和护理流程，这一点很重要。这就引出了我们的下一项工作：密集多标签活动识别。 比如，在 ICU 里面经常会涉及到测血压、绑止血带、用医用究竟喷雾消毒等等一系列的动作，我们希望最终能够映射所有的医疗活动，帮助医生和护士更好地照顾病人。为了做到这一点，仅仅用 CNN 是不够的，特别是对于静态帧分类。我们真正想做的，是将其扩展到时域，利用视频数据来识别人类活动。很多人都对此领域做出过相关贡献，这里我就不展开了。 但是总的来说，利用视频来进行活动识别的工作仍然很少，大部分工作都是活动分类，比如为潜水视频打一个单一的标签，或者是活动检测，为视频中不同活动分配相应的标签。不过，时域理解仍然处在比较初始的水平，比如很多帧都没有标签，而且大多数的测试视频都只有一种或少数几种活动类型。在医疗健康领域，活动要密集的多，因此我们需要识别更多不同种类的活动。 为了解决这个问题，我们开发了基于 RNN 序列模型的网络，使用 multilevel loss 来预测同时发生的活动。我们在 MultiHUMOS 数据集上进行了计算机视觉基准测试，与Vanilla LSTM 或者Stream CNN 相比，我们的算法在多活动标记领域取得了领先的成果。我们正试着在 InterMountain 医院的 ICU 里部署我们的模型，来观察病人的活动。我们选择从病人的四种活动开始，比如上床、下床等动作。知晓病人的活动水平，对医务人员提供更好的医疗服务至关重要。 最后，我还想介绍下我们在减少训练数据量方面的工作，这在养老院的应用中非常有用。人口老龄化是世界性的问题，我们真的需要做一些事情来帮助老人们。 我们和很多老年病患交谈过，并确定了十几种和老年人健康息息相关的行为。我们希望最终使用计算机视觉系统来识别他们的行为，帮助患者和医生。举例来说，跌倒对老年人说是一个大问题，它甚至有可能夺走老人的生命。我们正在尝试解决跌倒检测的问题，但是在这个领域，我们不可能收集到大量的训练数据，因此我们的一个想法是使用自监督学习系统，比如我们去年发布在 CVPR 上的一项工作。另外，我们还尝试了迁移学习的想法。 医疗生态系统最后，融入整个医疗生态系统也非常重要。为此，我们与斯坦福大学的其他小组展开合作，在为医院赋能时，我们不仅考虑智能传感器本身，还与病理学、放射学、医疗文献、图片等相结合。比如，我们和皮肤科医生一起研究烧伤患者的图像分割，又比如，我们也一直在寻找手术视频来识别手术中的活动。 总的来说，这是一个非常新兴的研究领域，它使用计算机视觉和机器学习算法来改善医疗保健服务，并帮助医生和护士观察病人活动，提高护理质量，从挽救更多的生命。从感知到人类活动识别到生态系统，以及建立大型合作关系，未来还有许多工作要做。 Bill Freeman：视觉信息促进语音辨识第二天的主题演讲人是谷歌研究科学家、MIT教授Bill Freeman，题目是“Look to Listen: Using Vision to Improve Speech Understanding”。 人类拥有卓越的人声辨识能力，甚至在嘈杂的多人环境中识别出特定的一个人。但，计算机很难做到这点。我们最近的一项研究工作就是让计算机具备这种能力，我们借鉴了人类在这个过程中利用到的线索：“看”着说话人讲话。我们算法的输入时两个或者多于两个人的说话视频，输出就是被挑选出的某个说话人的声音。这个技术我们把它命名为“Looking to Listen”，可以应用于多个领域例如：语音识别、翻译以及辅助听力等。 主题报告＂Look to Listen: Using Vision to Improve Speech Understanding＂ 人类在识别和理解人类语音方面有着极强的能力，哪怕是好几个人同时间在嘈杂的环境中说话，也能分清楚谁在说什么。对于计算机而言，这个任务还很艰巨。 最近，Freeman教授的团队通过让计算机“看”，也即观察说话者来辅助语音识别，大幅提升了计算机语言识别的性能。实际上，这也是人类在语音识别时常常采用的方法。他们的研究论文“Looking to Listen at the Cocktail Party”，已经被SIGGRAPH 2018接收。这项研究的起点，是MIT的研究人员发现，视频信息实际上可以充当一种“视觉麦克风”。一袋放在桌上的薯片，在旁边播放音乐，观察高速摄像头拍摄下的薯片包装袋，能发现包装袋在颤动，从而推理出音频信息。 在此基础上，Freeman教授带领的Google Research团队，通过计算生成视频，使用视觉信息，加强其中特定人物的语音，同时抑制其他的所有声音。这个方法适用于带有单个音频轨道的普通视频，用户需要的只是选择他们想要听的视频中人物的脸部，或者根据上下文在算法上选择这样的人物就行了。 他们设计了一种算法，输入有两个及更多人同时说话的视频，算法能够输出其中被选定的那个人的音频，非常清晰 “鸡尾酒效应”论文提出的基于神经网络的多数据流架构 他们把这种技术成为Looking to Listen，在语音识别、会议转录和视频会议等场景中，有着巨大的应用潜力。(墙Video: https://www.youtube.com/watch?v=rVQVAPiJWKU) 除了“从看到听”，在更早一些的时候，Freeman教授的团队还做了“从听到看”的研究，也即从声音中学习画面（Learning Sight from Sound）。在一项工作中，他们表明环境声音可以用作学习视觉模型的监督信号。他们训练了一个卷积神经网络来预测与视频帧相关的声音的统计汇总，网络学会了关于某个物体（对象）和场景有关声音信息的表示。结果发现，具有类似声音特征的视频，比如海边和河边，虽然视觉信息非常不同，但在网络学会的声音信号空间中，却是十分类似的。 通过这个过程，网络学会了关于某个物体（对象）和场景有关声音信息的表示。实验结果显示，这种方法的性能与其他最先进的无监督学习方法相当。图像是声音的补充，从一种模态（比如图像）中能够得到一些很难或者无法从另一种模态（比如语音）分析中得到的信息。反之也一样。通过这样将视觉和语音信号相结合，能够彼此促进。此外，如果能够确定哪些视觉信号能在训练过程中帮助检测特定的声音信号，将进一步提升语音识别的效果。 大合影 距离较远，只能拍成这样了 参考 Serena Yeung. http://ai.stanford.edu/~syyeung/ Yeung S, Downing N L, Fei-Fei L, et al. Bedside Computer Vision-Moving Artificial Intelligence from Driver Assistance to Patient Safety[J]. The New England journal of medicine, 2018, 378(14): 1271. William T. Freeman. https://billf.mit.edu/publications/all Ambient Sound Provides Supervision for Visual Learning. http://andrewowens.com/ambient/index.html Jeff Dean主页：https://ai.google/research/people/jeff Feifei Li主页：http://vision.stanford.edu/feifeili/ 电子书： http://g.cheerue.com/#/index 特别感谢：新智元、人工智能学家、机器之心等公众号的支持！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[NEWS]]></title>
      <url>%2Fposts%2Fnews%2F</url>
      <content type="text"><![CDATA[是时候回归了！ 最近评论系统HyperComments竟然开始收费了，于是我不得不改用新的评论系统LiveRe Gitalk/valine。这样一来，原来的评论都看不到了，由此给大家带来的不便，特此道歉！ Hello everyone, the comment system HyperComments is charging recently, so I had to switch to the new comment system LiveRe. As a result, the original comments are invisible. I deeply apologize for this inconveniences!]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Light Field Depth Estimation]]></title>
      <url>%2Fposts%2Flight-field-depth-estimation%2F</url>
      <content type="text"><![CDATA[本文将介绍光场领域进行深度估计的相关研究。In this post, I’ll introduce some depth estimation algorithms using Light field information. Here is some of the code.研究生阶段的研究方向是光场深度信息的恢复。再此做一些总结，以便于让大家了解光场数据处理的一般步骤以及深度估计的相关的知识，光场可视化部分代码见light-field-Processing。如有任何疑问或者建议，请大家在评论区提出。 什么是光场？提到光场，很多人对它的解释模糊不清，在此我对它的概念进行统一表述。它的故事可以追溯到1936年，那是一个春天，Gershun写了一本名为The Light Field1的鸿篇巨著（感兴趣的同学可以看看那个年代的论文），于是光场的概念就此诞生，但它并没有因此被世人熟知。经过了近六十年的沉寂，1991年Adelson2等一帮帅小伙将光场表示成了如下的7维函数： P(\theta,\phi,\lambda,t,V_x,V_y,V_z). \tag{1}其中$(\theta,\phi)$表示球面坐标，$\lambda$表示光线的波长，$t$表示时间，$(V_x,V_y,V_z)$表示观察者的位置。可以想象假如有这样一张由针孔相机拍摄的黑白照片，它表示：我们从某个时刻、单一视角观察到的可见光谱中某个波长的光线的平均。也就是说，它记录了通过$P$点的光强分布，光线方向可以由球面坐标$P(\theta,\phi)$或者笛卡尔坐标$P(x,y)$来表示。对于彩色图片而言，我们要添加光线的波长$\lambda$信息即变为$P(\theta,\phi,\lambda)$。按照同样的思路，彩色电影也就是增加了时间维度$t$，因此$P(\theta,\phi,\lambda,t)$。对于彩色全息电影而言，我们可以从任意空间位置$(V_x,V_y,V_z)$进行观看，于是其可以表达为最终的形式$P(\theta,\phi,\lambda,t,V_x,V_y,V_z)$。这个函数又被成为全光函数（Plenoptic Function）。但是以上的七维的全光函数过于复杂，难以记录以及编程实现。所以在实际应用中我们对其进行简化处理。第一个简化是单色光以及时不变。可分别记录3原色以简化掉波长$\lambda$，可以通过记录不同帧以简化$t$，这样全光函数就变成了5D。第二个简化是Levoy3等人（1996年）认为5D光场中还有一定的冗余，可以在自由空间（光线在传播过程中能量保持不变）中简化成4D。 光场参数化表示参数化表示要解决的问题包括：1. 计算高效；2. 光线可控；3. 光线均匀采样。目前比较常用的表示方式是双平面法（$2PP$）3，利用双平面法可以将光场表示为：$L(u,v,s,t)$。其中$(u,v)$为第一个平面，$(s,t)$是第二个平面。那么一条有方向的直线可以表示为连接$uv$以及$st$平面上任意两点确定的线，如下图所示： 【注】：Levoy3首先利用双平面法对光场进行表示，光线首先通过$uv$平面，然后再通过$st$平面。但是后来（同年）Gortler4等人将其传播方向反了过来，导致后续研究者对此表述并不一致。与此同时，也有不少文献中也引入了$xy$坐标，例如著名的光场相机的缔造者N.G.博士的毕业论文。通常情况下，这指的是像平面坐标，即指的是由传感器得到的图像中像素的位置坐标。由于后续处理中都是针对图像而言，而对于光学结构以及光线的传播过程并不感兴趣。所以为了方便起见，我们在本文中统一采用Levoy3的方式对光场图像进行表示，即$uv$表示角度分辨率，$xy$表示空间分辨率，即$L(u,v,x,y)$。同时在表示光场时用$L(u,v,s,t)$。有时候二者不做区分，注意即可。 光场的可视化虽然光场由$7D$全光函数降维到$4D$，但是其结构还是很难直观想象。通过固定4D光场参数化表示$L(u,v,s,t)$中的某些变量，我们可以很容易地对光场进行可视化。我们通常认为$(u,v)$控制着某个视角的位置，即相机平面；而$(s,t)$控制着从某个视角观察到的图像。说简单点：$uv$控制角度分辨率，$st$控制空间分辨率（视野）。注意上式描述的是光线的表示方法，并没有涉及图像处理，所以没有$xy$。 接下来讲解，几种常见的可视化方式（图片来源5）。首先是多视图法。很容易理解，对于最简单的情况，首先固定$u=u^*,v=v^*$，我们可以得到多视角的某个视图$L(u^*,v^*,s,t)$，如下图所示： 第二种表示方法是角度域法，通过固定$s=s^*,t=t^*$可以得到某个宏像素$L(u,v,s^*,t^*)$，如下图所示： 第三种表示方法是极线图法，通过固定$v=v^*,t=t^*$可以得到极线图：$L(u,v^*,s,t^*)$，如下图中水平方向的图所示；同理固定$u=u^*,s=s^*$可以得到极线图：$L(u^*,v,s^*,t)$，如下图中竖直方向的图所示： 最后，给出这几种方式的对应关系图（注意图中，$xy$对应于以上$st$，$st$对应于$uv$）。 光场的获取我们知道传统的相机只能采集来自场景某个方向的$2D$信息，那怎么才能够采集到光场信息呢？试想一下，当多个相机在多个不同视角同时拍摄时，这样我们就可以得到一个光场的采样（多视角图像）了。当然，这是容易想到的方法，目前已有多种获得光场的方式，如下表格中列举了其中具有代表性的方式5。 光场深度估计算法分类由上可知，光场图像中包含来自场景的多视角信息，这使得深度估计成为可能。相较于传统的多视角深度估计算法而言，基于光场的深度估计算法无需进行相机标定，这大大简化的深度估计的流程。但是由于光场图像巨大导致了深度估计过程占用大量的计算资源。同时这些所谓的多个视角之间虚拟相机的基线过短，从而可能导致误匹配的问题。以下将对多种深度估计算法进行分类并挑选具有代表性的算法进行介绍。 多视角立体匹配根据光场相机的成像原理，我们可以将光场图像想像成为多个虚拟相机在多个不同视角拍摄同一场景得到图像的集合，那么此时的深度估计问题就转换成为多视角立体匹配问题。以下列举几种基于多视角立体匹配算法的深度估计算法8 9 10 20 21。 MVS-based Approach Main Feature Jeon et al. 8 Phase shift Sub-pixel Yu et al. 9 Line-assisted graph cut Heber et al. 10 20 PCA matching term Chen et al. 21 Scam: Bilateral Consistency Metric 在这里介绍Jeon等人8提出的基于相移的亚像素多视角立体匹配算法。 相移理论该算法的核心就是用到了相移理论，即空域的一个小的位移在频域为原始信号的频域表达与位移的指数的幂乘积，即如下公式： \mathcal{F}\left\{I(x+\Delta x)\right\} = \mathcal{F}\left\{I(x)\right\}\exp^{2\pi j\Delta x}. \tag{2}所以，经过位移后图像可以表示为： I'(x)=I(x+\Delta x)={\mathcal{F}^{-1}\left\{\mathcal{F}\left\{I(x)\right\}\exp^{2 \pi j \Delta x}\right\}},\tag{3}面对Lytro相机窄基线的难点，通过相移的思想能够实现亚像素精度的匹配，在一定程度上解决了基线短的问题。那么大家可能好奇的是，如何将这个理论用在多视角立体匹配中呢？带着这样的疑问，继续介绍该算法。 匹配代价构建为了能够使子视角图像之间进行匹配，作者设计了2中不同的代价量：Sum of Absolute Differences (SAD)以及Sum of Gradient Differences (GRAD)，最终通过加权的方式获得最终的匹配量$C$，它是位点$x$以及损失编号（可以理解成深度/视差层）$l$的函数，具体形式如下公式所示： C(x,l) = \alpha C_A(x,l)+(1-\alpha)C_G(x,l),\tag{4}其中$\alpha \in [0,1]$表示SAD损失量$C_A$以及SGD损失量$C_G$之间的权重。同时其中的$C_A$被定义为如下形式： C_A(x,l) = \sum_{u \in V}\sum_{x \in R_x}{\min\left( | I(u_c,x)-I(u,x+\Delta x(u,l))|,\tau _1\right)},\tag{5}其中的$R_x$表示在$x$点邻域的矩形区域；$\tau _1$是代价的截断值（为了增加算法鲁棒性）；$V$表示除了中心视角$u_c$之外的其余视角。上述公式通过比较中心视角图像$I(u_c,x)$与其余视角$I(u,x)$的差异来构建损失量，具体而言就是通过不断地在某个视角$I(u_i,x)$上$x$点的周围移动一个小的距离并于中心视角做差；重复这个过程直到比较完所有的视角(i=1…视角数目N)为止。此时会用到上节提及的相移理论以得到移动后的像素强度，注意上面提到的小的距离实际上就是公式中的$\Delta x$，它被定义为如下形式： \Delta x(u,l) = lk(u-u_c),\tag{6}其中k表示深度/视差层的单位（像素），$\Delta x$会随着任意视角与中心视角之间距离的增大而线性增加。同理，可以构造出第二个匹配代价量SGD，其基本形式如下所示： C_G(x,l) = \sum_{u \in V}\sum_{x \in R_x}\beta (u){\min\left( Diff_x(u_c,u,x,l),\tau _2\right)}+ \\ \ \ \ (1-\beta (u)){\min\left( Diff_y(u_c,u,x,l),\tau _2\right)},\tag{7}其中的$Diff_x(u_c,u,x,l)=|I_x(u_c,x)-I_x(u,x+\Delta x(u,l))|$表示子视角图像在x方向的上的梯度，同理$Diff_y$表示子孔径图像在y方向上的梯度；$\beta (u)$控制着这两个方向代价量的权重，它由任意视角与中心视角之间的相对距离表示： \beta (u) = \frac{|u-u_c|}{|u-u_c|+|v-v_c|}.\tag{8}至此，代价函数构建完毕。随后对于该代价函数利用边缘保持滤波器进行损失聚合，得到优化后的代价量。紧接着作者建立了一个多标签优化模型（GC求解）以及迭代优化模型对深度图进行优化，再此不做详细介绍。下面是其算法的分部结果： 基于EPI的方法 不同于多视角立体匹配的方式，EPI的方式是通过分析光场数据结构的从而进行深度估计的方式。EPI图像中斜线的斜率就能够反映出场景的深度。上图中点P为空间点，平面$\Pi$为相机平面，平面$\Omega$为像平面。图中$\Delta u$与$\Delta x$的关系可以表示为如下公式6： \Delta x=- \frac{f}{Z}\Delta u,\tag{9}假如固定相同的$\Delta u$，水平方向位移较大的EPI图中斜线所对应的视差就越大，即深度就越小。如下图所示，$\Delta x_2$&gt;$\Delta x_1$，那么绿色线所对应的空间点要比红色线所对应的空间点深度小。 以下列举几种基于EPI的深度估计算法11 12 13 14 15 24。 EPI-based Approach Main Feature Kim et al. 11 Large scene reconstruction Li et al. 12 Sparse linear optimization Krolla et al. 13 Spherical light field Wanner et al. 14 26 Total variation(TV) Diebode et al. 15 Modified structure tensor Zhang et al. 24 Spinning Parallelogram Operator(SPO) 在以上表格中最具代表性的算法是由wanner14提出的结构张量法得到EPI图中线的斜率，如下公式所示： J= \left[ \begin{matrix} G_{\sigma}*(S_xS_x) & G_{\sigma}*(S_xS_y) \\ G_{\sigma}*(S_xS_y) & G_{\sigma}*(S_yS_y) \end{matrix} \right]= \left[ \begin{matrix} J_{xx} & J_{xy}\\ J_{xy} & J_{yy} \end{matrix} \right], \tag{10}其中$S=S_{y^*,v^*}$为极线图。$S_x$以及$S_y$表示极线图在x以及y方向上的梯度，$G_{\sigma}$表示高斯平滑算子。最终极线图中局部斜线的斜率可以表示成如下形式： J=\left[ \begin{matrix} \Delta x \\ \Delta v \end{matrix} \right]= \left[ \begin{matrix} \sin \varphi\\ \cos \varphi \end{matrix} \right], \tag{11}其中$\varphi = \frac{1}{2}\arctan\left(\frac{J_{yy}-J_{xx}}{2J_{xy}}\right)$。因此深度可以由公式（9）推出： Z=-f\frac{\Delta v}{\Delta x}, \tag{12}通常情况下，可以用一种更加简单的形式，如视差对其进行表示： d_{y^*,v^*}=-f/Z=\frac{\Delta x}{\Delta v}=\tan \phi . \tag{13}至此，利用上述公式可以从EPI中估计出视差。 散焦及融合的方法光场相机一个很重要的卖点是先拍照后对焦，这其实是根据光场剪切原理31得到的。通过衡量像素在不同焦栈处的“模糊度”可以得到其对应的深度。以下列举几种基于散焦的深度估计算法7 16 17 22 23。 Defocus-based Approach Main Feature Wang et al. 7 Occlusion-aware Tao et al. 16 Defocus cues & Correspondence cues Tao et al. 17 Angular Coherence Williem et al. 22 23 Angular Entropy(AD, AE, CAD, CAE) 这里介绍一个最具代表性的工作，由Tao等人16在2013年提出，下图为其算法框架以及分部结果。 该工作其实就做了2件事情：1. 设计两种深度线索并估计原始深度；2. 置信度分析及MRF融合。以下对其进行具体介绍。 双线索提取首先对光场图像进行重聚焦，然后得到一系列具有不同深度的焦栈。然后对该焦栈分别提取2个线索：散焦量以及匹配量。其中散焦量被定义为： D_{\alpha}(x)=\frac{1}{|W_{D}|}{\sum _{x' \in W_D} {|\Delta _x{L}_{\alpha}(x')|}},\tag{14}其中，$W_D$表示为当前像素领域窗口大小，$\Delta _x$表示水平方向拉式算子，$\overline{L}_{\alpha}(x)$为每个经过平均化后的重聚焦后光场图像，其表达式如下： \overline{L}_{\alpha}(x)=\frac{1}{N_{u}}\sum _{u'} {L}_{\alpha}(x,u'),\tag{15}其中$N_{u}$表示每一个角度域内像素的数目。然后匹配量被定义成如下形式： {C}_{\alpha}(x)=\frac{1}{|W_{C}|}\sum _{x' \in W_C} {\sigma}_{\alpha}(x'),\tag{16}其中，$W_C$表示为当前像素领域窗口大小，${\sigma}_{\alpha}(x)$表示每个宏像素强度的标准差，其表达式为： {\sigma}_{\alpha}(x)^2=\frac{1}{N_{u}}\sum _{u'} \left({L}_{\alpha}(x,u')-\overline{L}_{\alpha}(x)\right)^2.\tag{17}经过以上两个线索可以通过赢者通吃（Winner Takes All，WTA）得到两张原始深度图。注意：对这两个线索使用WTA时略有不同，通过最大化空间对比度可以得到散焦线索对应的深度，最小化角度域方差能够获得匹配量对应的深度。因此二者深度可以分别表示为如下公式： \alpha ^{*}_D(x)=\mathop{\arg\max}_{\alpha} \ \ {D}_{\alpha}(x).\tag{18} \alpha ^{*}_C(x)=\mathop{\arg\min}_{\alpha} \ \ {C}_{\alpha}(x).\tag{19}置信度分析及深度融合 上图中显示了两个线索随着深度层次而变化的曲线。接下来的置信度分析用主次峰值比例（Peak Ratio）来定义每种线索的置信度，可表示为如下公式，其中的$\alpha ^{*2}_D(x)$以及$\alpha ^{*2}_C(x)$分别表示曲线的次峰值对应的深度层次。 D_{conf}(x)=\frac{D_{\alpha ^{*}_D}(x)}{D_{\alpha ^{*2}_D}(x)}.\tag{20} C_{conf}(x)=\frac{C_{\alpha ^{*}_C}(x)}{C_{\alpha ^{*2}_C}(x)}.\tag{21}接下来对原始深度进行MRF置信度融合： \mathop{minimize}_{Z} \ \ \sum_{source}\lambda _{source} \sum _i W_i|Z_i-Z_i^{source}| +\lambda _{flat} \sum _{(x,y)}\left( \left |\frac{\partial Z_i}{\partial x}\right|_{(x,y)}+\left|\frac{\partial Z_i}{\partial y}\right|_{(x,y)}\right) + \lambda _{smooth} \sum _{(x,y)}|\Delta Z_i|_{(x,y)}.\tag{22}其中，$source$控制着数据项，即优化后的深度要与原始深度尽量保持一致。第二项与第三项分别控制着平坦性（flatness）与平滑性（smoothness）。注意：平坦的意思是物体表面没有凹凸变化的沟壑，例如魔方任一侧面，无论是否拼好（忽略中间黑线）。而平滑则表示在平坦的基础上物体表面没有花纹，如拼好的魔方的一个侧面。另外的$W$是权重量，此处选用的是每个线索的置信度。 \{Z_1^{source},Z_2^{source}\}=\{\alpha_C^{*},\alpha_D^{*}\}.\tag{23} \{W_1^{source},W_2^{source}\}=\{C_{conf},D_{conf}\}.\tag{24}至此，该算法介绍完毕，其代码已经放在我的Github。 学习的方法目前而言，将深度学习应用于从双目或者单目中恢复深度已经不再新鲜，我在之前的博文1&amp;博文2中有过对这类算法的介绍。但是将其应用于光场领域进行深度估计的算法还真是寥寥无几。不过总有一些勇敢的践行者去探索如何将二者结合，以下列举几种基于学习的深度估计算法18 19 25 27 28 29 30。 Learning-based Approach Main Feature Johannsen et al. 18 25 Sparse coding Heber et al. 19 27 28 CNN-based Jeon et al. 29 SAD, SGD, ZNCC, CT, Random Forests Shin et al. 30 4-Directions EPIs & CNN-based 在此，我将对截止目前（2018年5月29日）而言，在HCI新数据集上表现最好的EPINET: A Fully-Convolutional Neural Network Using Epipolar Geometry for Depth from Light Field Images30算法进行介绍，下图为该算法在各个指标上的表现情况。 算法摘要：光场相机能够同时采集空间光线的空域以及角度域信息，因此可以根据这种特性恢复出空间场景的涉深度。在本文中，作者提出了一种基于CNN的快速准确的光场深度估计算法。作者在设计网络时将光场的几何结构加入考虑，同时提出了一种新的数据增强算法以克服训练数据不足的缺陷。作者提出的算法能够在HCI 4D-LFB上在多个指标上取得Top1的成绩。作者指出，光场相机存在优势的同时也有诸多缺点，例如：基线超级短且空间&amp;角度分辨率有一定的权衡关系。目前已有很多工作去克服这些问题，这样一来，深度图像的精度提升了，但是带来的后果就是计算量超级大，无法快速地估计出深度。因此作者为了解决精度以及速度之间权衡关系设计了该算法（感觉很有意义吧）。 上面表格中提到的诸如Johannsen18 25以及Heber19 27 28等人设计的算法仅仅考虑到了一个极线方向，从而容易导致低置信度的深度估计。为了解决他们算法中存在的问题，作者通过一种多流网络将不同的极线图像分别进行编码去预测深度。因为，每个极线图都有属于自己的集合特征，将这些极线图放入网络训练能够充分地利用其提供的信息。 光场图像几何特征由于光场图像可以等效成多个视角图像的集合，这里的视角数目通常要比传统的立体匹配算法需要的视角数目多得多。所以，如果利用全部的视角做深度估计将会相当耗时，所以在实际情况下并不需要用到全部的视角。作者的思路就是想办法尽量减少实际要使用的视角数目，所以作者探究了不同角度域方向光场图像的特征。中心视角图像与其余视角的关系可以表示成如下公式： L(x,y,0,0)=L(x+d(x,y)*u,y+d(x,y)*v,u,v),\tag{25}其中$d(x,y)$表示中心视角到其相应相邻视角之间的视差（disparity）。令角度方向为$\theta$（$\tan \theta=v/u$），我们可以将上式改写成如下公式： L(x,y,0,0)=L(x+d(x,y)*u,y+d(x,y)*u \tan \theta,u,u \tan \theta).\tag{26}作者选择了四个方向$\theta$: 0o，45o，90o，135o，同时假设光场图像总视角数为$(2N+1)\times(2N+1)$。 网络设计如本节开始的图所示的网络结构，该网络的开始为多路编码网络（类似于Flownet以及Efficient Deep Learning for Stereo Matching32），其输入为4个不同方向视角图像集合，每个方向对应于一路网络，每一路都可以对其对应方向上图像进行编码提取特征。每一路网络都由3个全卷积模块组成，因为全卷积层对逐点稠密预测问题卓有成效，所以作者将每一个全卷积模块定义为这样的卷积层的集合：Conv-ReLU-Conv-BN-ReLU，这样的话就可以在局部块中预逐点预测视差。为了解决基线短的问题，作者设计了非常小的卷积核：$2\times 2$，同时stride = 1，这样的话就可以测量$\pm 4$的视差。为了验证这种多路网络的有效性，作者同单路的网络做了对比试验，其结果如下表所示，可见多路网络相对于单路网络有10%的误差降低。 在完成多路编码之后，网络将这些特征串联起来组成更维度更高的特征。后面的融合网络包含8个卷积块，其目的是寻找经多路编码之后特征之间的相关性。注意除了最后一个卷积块之外，其余的卷积块全部相同。为了推断得到亚像素精度的视差图，作者将最后一个卷积块设计为Conv-ReLU-Conv结构。 最后，图像增强方式包括视角偏移（从9*9视角中选7*7，可扩展3*3倍数据），图像旋转（90o，180o，270o），图像缩放（[0.25,1]），色彩值域变化（[0.5,2]），随机灰度变化，gamma变换（[0.8,1.2]）以及翻转，最终扩充了288倍。 以下为其各个指标上的性能表现： 以上介绍了目前已有的深度估计算法不同类别中具有代表性的算法，它们不一定是最优的，但绝对是最容易理解其精髓的。到目前为止，光场领域已经有一大波人做深度估计的工作，利用传统的方式其精度很难再往上提高。随着深度学习的大热，已经有一批先驱开始用深度学习做深度估计，虽然在仿真数据上可以表现得很好，但实际场景千变万化，即使是深度学习的策略也不敢保证对所有的场景都有效。路漫漫其修远兮，深度估计道路阻且长。我认为以后的趋势应该是从EPI图像下手，然后利用CNN提feature（或者响应）；此时可供选择的工具有KITTI Stereo/HCI新数据集算法比较/Middlebury Stereo中较好的几种算法，我们需要总结其算法优势并迁移到光场领域中来。GPU这个Powerful的计算工具一定要用到光场领域中来，发挥出多线程的优势。否则传统的CPU对于动辄上百兆的数据有心无力。这样一来，深度图像不仅仅可以从精度上得以提高，而且深度估计的速度也会更快。至此，本文介绍到此结束。 References 1. Gershun, A. “The Light Field.” Studies in Applied Mathematics 18.1-4(1939):51–151. &#8617; 2. Adelson, Edward H, and J. R. Bergen. “The plenoptic function and the elements of early vision. “ Computational Models of Visual Processing (1991):3-20. &#8617; 3. Levoy, Marc. “Light field rendering.” Conference on Computer Graphics and Interactive Techniques ACM, 1996:31-42. &#8617; 4. Gortler, Steven J., et al. “The Lumigraph.” Proc Siggraph 96(1996):43-54. &#8617; 5. Wu, Gaochang, et al. “Light Field Image Processing: An Overview.” IEEE Journal of Selected Topics in Signal Processing PP.99(2017):1-1. &#8617; 6. Wanner, Sven, and B. Goldluecke. “Variational Light Field Analysis for Disparity Estimation and Super-Resolution.” IEEE Transactions on Pattern Analysis &amp; Machine Intelligence 36.3(2013):1. &#8617; 7. Wang, Ting Chun, A. A. Efros, and R. Ramamoorthi. “Occlusion-Aware Depth Estimation Using Light-Field Cameras.” IEEE International Conference on Computer Vision IEEE, 2016:3487-3495. &#8617; 8. Jeon, Hae Gon, et al. “Accurate depth map estimation from a lenslet light field camera.” Computer Vision and Pattern Recognition IEEE, 2015:1547-1555. &#8617; 9. Yu, Zhan, et al. “Line Assisted Light Field Triangulation and Stereo Matching.” IEEE International Conference on Computer Vision IEEE, 2014:2792-2799. &#8617; 10. Heber, Stefan, and T. Pock. “Shape from Light Field Meets Robust PCA.” Computer Vision – ECCV 2014. 2014:751-767. &#8617; 11. Kim, Changil, et al. “Scene reconstruction from high spatio-angular resolution light fields.” Acm Transactions on Graphics 32.4(2017):1-12. &#8617; 12. Li, J., M. Lu, and Z. N. Li. “Continuous Depth Map Reconstruction From Light Fields.” IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society 24.11(2015):3257. &#8617; 13. Krolla, Bernd, et al. “Spherical Light Fields.” British Machine Vision Conference 2014. &#8617; 14. Wanner, Sven, C. Straehle, and B. Goldluecke. “Globally Consistent Multi-label Assignment on the Ray Space of 4D Light Fields.” IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2013:1011-1018. &#8617; 15. Diebold, Maximilian, B. Jahne, and A. Gatto. “Heterogeneous Light Fields.” Computer Vision and Pattern Recognition IEEE, 2016:1745-1753. &#8617; 16. Tao, M. W, et al. “Depth from Combining Defocus and Correspondence Using Light-Field Cameras.” IEEE International Conference on Computer Vision IEEE Computer Society, 2013:673-680. &#8617; 17. Tao, Michael W., et al. “Depth from shading, defocus, and correspondence using light-field angular coherence.” Computer Vision and Pattern Recognition IEEE, 2015:1940-1948. &#8617; 18. Johannsen, Ole, A. Sulc, and B. Goldluecke. “Variational Separation of Light Field Layers.” (2015). &#8617; 19. Heber, Stefan, and T. Pock. “Convolutional Networks for Shape from Light Field.” Computer Vision and Pattern Recognition IEEE, 2016:3746-3754. &#8617; 20. Heber, Stefan, R. Ranftl, and T. Pock. “Variational Shape from Light Field.” Energy Minimization Methods in Computer Vision and Pattern Recognition. Springer Berlin Heidelberg, 2013:66-79. &#8617; 21. Chen, Can, et al. “Light Field Stereo Matching Using Bilateral Statistics of Surface Cameras.” IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2014:1518-1525. &#8617; 22. Williem W, Kyu P I. “Robust light field depth estimation for noisy scene with occlusion.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016:4396-4404. &#8617; 23. Williem W, Park I K, Lee K M. “Robust light field depth estimation using occlusion-noise aware data costs.” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017(99):1-1. &#8617; 24. Zhang S, Sheng H, Li C, et al. “Robust depth estimation for light field via spinning parallelogram operator.” Computer Vision and Image Understanding, 2016, 145:148-159. &#8617; 25. Johannsen O, Sulc A, Goldluecke B. “What sparse light field coding reveals about scene structure.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016(1/3/4):3262-3270. &#8617; 26. Wanner S, Goldluecke B. “Reconstructing reflective and transparent surfaces from epipolar plane images.” In German Conference on Pattern Recognition (Proc. GCPR), 2013:1-10. &#8617; 27. Heber S, Yu W, Pock T. “U-shaped networks for shape from light field.” British Machine Vision Conference, 2016, 37:1-12. &#8617; 28. Heber S, Yu W, Pock T. “Neural EPI-Volume networks for shape from light field.” IEEE International Conference on Computer Vision (ICCV), IEEE Computer Society, 2017:2271-2279. &#8617; 29. Jeon H G, Park J, Choe G, et.al. “Depth from a Light Field Image with Learning-based Matching Costs.” IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2018. &#8617; 30. Shin C, Jeon H G, Yoon Y. “EPINET: A Fully-Convolutional Neural Network for Light Field Depth Estimation Using Epipolar Geometry.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. &#8617; 31. Ng, Ren. “Digital light field photography.” 2006, 115(3):38-39. &#8617; 32. Luo, Wenjie, A. G. Schwing, and R. Urtasun. “Efficient Deep Learning for Stereo Matching.” IEEE Conference on Computer Vision and Pattern Recognition IEEE Computer Society, 2016:5695-5703. &#8617;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CV Related References]]></title>
      <url>%2Fposts%2Fcv-books%2F</url>
      <content type="text"><![CDATA[视觉 Multiple View Geometry in Computer Vision：计算机视觉中的多视图几何【69.52 MB】 凸优化 Selected Applications of Convex Optimization：凸优化应用讲义 CVX:凸优化问题求解 矩阵 Iterative Methods for Sparse Linear Systems：稀疏线性系统求解 The Matrix Cookbook：矩阵分析 Markov Random Field Image Modelling：马尔科夫随机场模型 Numerical Recipes：数值运算【20.41 MB】 机器学习算法 Machine Learning：机器学习中文版-周志华【85.68 MB】 C4.5 K-means SVM Apriori EM PageRank kNN Naive-Bayes CART Deep Learning：深度学习中文版【30.90 MB】 Building Machine Learning Projects with TensorFlow【13.42 MB】 统计学习方法-李航【17.56 MB】 OpenCV OpenCV Guide：Opencv简明教程 会议 CVPapers - Computer Vision Resource 中国计算机学会推荐国际会议及期刊目录 To be continued, welcome to commit.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[立体视觉综述：Stereo Vision Overview]]></title>
      <url>%2Fposts%2Fstereo-vision-overview%2F</url>
      <content type="text"><![CDATA[本文主要翻译自Mattoccia的双目视差估计综述，对于刚刚接触立体深度估计方向的小伙伴会有帮助；如果你是专家也可以做一下复习，如有错误请在评论中指出。文中主要介绍以下几个方面的内容： Introduction to stereo vision Overview of a stereo vision system Algorithms for visual correspondence Computational optimizations Hardware implementation Applications 什么是立体视觉 是一个能够从双目或者多目相机中提取深度图像的技术 在计算机视觉领域很火爆的研究话题 这与以下几个方面的相关：双目立体视觉系统、稠密立体算法、立体视觉应用 偏好能够实时或者硬件实现 单目相机 如图所示的是单目摄像机的拍摄原理，右侧实际场景可以抽象成左侧的模型。可以发现场景中的P点与Q点会同时汇聚在成像平面中的一点，同样遮挡问题出现在PQ连线的所有点。 双目相机对于双目相机，$O_R$和$O_T$分别是左右相机的光学中心，对于在参考相机像平面上被汇聚的两点（p和q），在目标相机像平面上会被区分开来，那么我们可以找到双目或者多目相机中匹配的点利用三角相似原理来估计深度。那么我们怎么寻找相对应的点呢？一个直观的想法就是固定两幅图中的一幅，然后在另外一幅图中 进行2D范围的搜索匹配点。 但实际情况这样做的代价非常大。不过多亏了有极线约束，我们可以在图像的1D范围上进行搜索。以下将要对极线约束进行解释。 极线约束(对极几何) 对于参考图像R而言，现实场景中的P与Q点在其像平面${\pi}_R$上被投影成为一个点p=q。 极线约束规定，属于（红色）视线的点对应位于目标图像T的图像平面${\pi}_T$上的绿线上。 我们可以在维基百科上找到更为详细的介绍，具体描述可见下图。特别感谢@岳麓吹雪同学的帮忙，以下是他已经整理好的译文。下图是针孔相机模型图。两个针孔相机看向空间点，实际相机的像面位于焦点中心后面，生成了一幅关于透镜的焦点中心对称的图像。这个问题可以简化为在焦点中心前方放置一个虚拟像面来生成正立图像，而不需要对称变换得到。$O_L$和$O_R$表示两个相机透镜中心，$X$表示两个相机共同的目标点，$X_L$和$X_R$是点$X$在两像面上的投影。 epipolar points极点每一个相机的透镜中心是不同的，会投射到另一个相机像面的不同点上。这两个像点用$e_L$和$e_R$表示，被称为epipolar points极点。两个极点$e_L$、$e_R$分别与透镜中心$O_L$、$O_R$在空间中位于一条直线上。 epipolar plane极面将$X$、$O_L$和$O_R$三点形成的面称为epipolar plane极面。 epipolar line极线直线$O_LX$被左侧相机看做一个点，因为它和透镜中心位于一条线上。然而，从右相机看直线$O_LX$，则是像面上的一条线直线$e_RX_R$，被称为epipolar line极线。从另一个角度看，极面$XO_LO_R$与相机像面相交形成极线。极线是3D空间中点X的位置函数，随$X$变化，两幅图像会生成一组极线。直线$O_LX$通过透镜中心$O_L$，右像面中对应的极线必然通过极点$e_R$。一幅图像中的所有极线包含了该图像的所有极点。实际上，任意一条包含极点的线都是由空间中某一点$X$推导出的一条极线。 如果两个相机位置已知，则：1.如果投影点$X_L$已知，则极线$e_RX_R$已知，点X必定投影到右像面极线上的$X_R$处。这就意味着，在一个图像中观察到的每个点，在已知的极线上观察到该点的其他图像。这就是Epipolar constraint极线约束：$X$在右像面上的投影$X_R$必然被约束在$e_RX_R$极线上。对于$O_LX_L$上的$X$，$X_1$，$X_2$，$X_3$都受该约束。极线约束可以用于测试两点是否对应同一3D点。极线约束也可以用两相机间的基本矩阵来描述。2.如果$X_L$和$X_R$已知，他们的投影线已知。如果两幅图像上的点对应同一点X，则投影线必然交于$X$。这就意味着$X$可以用两个像点的坐标计算得到。 由极线约束可知，我们可以将原来的匹配点搜索范围由2D转换成1D，这样做可以很大程度上减少计算量。我们将左右视图摆放成更容易理解的形式，可以发现对应点的匹配问题转换成了在同一条扫描线上（极线）的匹配问题。 可以发现相机的摆放姿势影响着扫描线的方向。在上图A中，相机与水平呈一定角度地摆放，其扫描线为右图所示，同样是与水平倾斜的扫描线。假如两个相机平行摆放的话，其拍出来匹配对是扫描线已经对齐了的。 深度与视差如上图所示为扫描线已经对齐了的匹配图像对（以下简称匹配对）。可以发现：$PO_RO_T$与$Ppp’$是相似三角形，由于相似三角形原理，我们可以很容易知道： \frac{b}{Z}=\frac{(b+x_T)-x_R}{Z-f}其中，$x_R-x_T$就是视差，Z表示深度，B为基线，f是焦距。 所谓视差就是匹配对中对应点之间x方向上的差异，我们可以将这种差异转换成为灰度图（越近越白），如上最后一个图所示。 上图展示了物体距离相机越近的话，视差就越大。其实很容易理解，将人的双眼比作成双目相机，对比将手指放在双眼前方近处与远处晃动的区别，可以发现在近处的话人眼感知到手指的晃动是比远处晃动的“程度”明显的，那么这种程度就是视差在人脑中的反映。 视界 图为双摄装置，基线为b，焦距为f，那么双摄的视界被视差范围所限定{$d_{min},d_{max}$}，如图中绿色包裹的区域。 深度是通过利用立体匹配系统将视差离散成一系列平行的平面来测量的；每一层平面对应着一个视差。 可以通过超像素的方法得到效果更好的深度图。 图为5个视差{$d_{min},d_{min}+4$}组成的视场。 图为5个视差{$\Delta+d_{min},\Delta+d_{min}+4$}组成的视场 $\Delta&gt;0$时，视场收缩并向相机靠近 深度估计图中为传统算法以及ICCV2011当时最好的结果。可以发现，能够达到较好的视差是具有挑战性的。下面将要展示视差估计的基本流程。 通过双摄设备采集图像，此时图像是存在镜头畸变的，在进行扫描线对齐之前要进行离线标定以消除镜头畸变。扫描线对齐的过程叫做镜头矫正（rectificaition），经过这步之后就可以进行1D的匹配点搜索（stereo correspondence）了。随后通过三角形相似原理得到相应的深度/视差图。 离线标定标定的目标是寻找： 相机内参：焦距、图像中心、镜头畸变参数 相机外参：排列相机使其对齐的参数 注意的是，相机标定的话一般需要10对以上的图像（通常拍摄棋盘格图像，利用张氏标定法进行标定）。 标定程序可以见Opencv39和Matlab40。 更为详细的介绍参见20 21 22。 匹配矫正利用标定步骤得到的相机的内参对相机镜头畸变进行校正，同时对其扫描线。 立体匹配目标：从匹配对中寻找对应的点，反映在图像中就是视差图像。 三角测量给定视差图像，基线长度以及焦距可以通过三角计算得到当前位置对应的3D位置。 立体匹配的挑战性光度失真以及噪声 高光表面 透视收缩 透视变形 无纹理区域 重复/混淆区域 透明物体 遮挡区以及不连续区域（1） 遮挡区以及不连续区域（2） Middlebury数据集Middlebury数据集提供了一套可供深度估计的数据集以及评价系统，深度估计算法可在该数据集上进行测试性能。2003年的数据集提供了Tsukuba, Venus, Teddy and Cones这几个场景的匹配对。 匹配问题立体匹配的算法可以分成以下几个步骤： 匹配量/损失计算 损失聚合 视差计算/优化 视差精化 局部算法包括：1-&gt;2-&gt;3（简单的WTA算法） 全局算法包括：1（-&gt;2）-&gt;3（全局或者半全局算法） 数据预处理数据预处理是为了消除图像的光度失真。常见的操作有： LoG滤波器41 消减附近像素中计算的平均值42 双边滤波 统计变换 最简单的立体匹配算法如下图所示，逐像素地计算SAD匹配损失；然后通过WTA得到初始视差，但是此时得到的视差质量是很差的。那么如何提高深度图像的质量呢？通常来说有两种不同类别的策略。 局部算法。同样是利用到了简单的WTA提取到初始视差，但是通过计算窗口内的损失量提高了信噪比。有时需会加入平滑项。Steps 1+2 (+ WTA) 全局/半全局算法。寻找能够使得能量函数的最小值的视差以得到逐点视差。Steps 1+ Step3。（有时，损失函数需要聚合） 两种算法都假设了匹配对是平滑的，但有时，该假设并不成立。这个假设在局部算法中隐晦地提及，却在全局算法中明确地建模，如下形式。 E(d)=E_{data}(d)+E_{smooth}(d)损失量的计算逐像素的匹配误差 绝对值误差 e(x,y,d)=|I_R(x,y)-I_T(x+d,y)| 平方误差 e(x,y,d)=(I_R(x,y)-I_T(x+d,y))^2 鲁棒匹配子（M-estimators）如截断绝对误差（truncated absolute differences (TAD)）可以减少离群点的干扰： e(x,y,d)=min\{|I_R(x,y)-I_T(x+d,y),T\} 相异性测量对于图像噪声不敏感（Birchfield and Tomasi27） 视差空间图像（DSI）是一个如下图所示张量$W\times H\times(d_{max}-d_{min})$，其中的每一个元素$C(x,y,d)$表示$I_R(x_R,y)$与$I_T(x_R+d,y)$之间的匹配度。 区域匹配损失 绝对误差和（Sum of Absolute differences (SAD)）C(x,y,d)=\sum_{x\in S}|I_R(x,y)-I_T(x+d,y)| 绝对平方和（Sum of Squared differences (SSD)）C(x,y,d)=\sum_{x\in S}\left(I_R(x,y)-I_T(x+d,y)\right)^2 截断绝对误差和（Sum of truncated absolute differences (STAD)）C(x,y,d)=\sum_{x\in S}\{|I_R(x,y)-I_T(x+d,y),T\} Normalized Cross Correlation 57 Zero mean Normalized Cross Correlation 58 Gradient based MF 59 Non parametric 60 61 Mutual Information 30 Combination of matching costs 损失聚合那么从最简单的固定窗口（FW）损失聚合开始，以下为利用FW聚合的TAD损失然后利用WTA得到的深度图。理想是完美的，但现实是骨感的，可以看到下图给出的结果并不佳，这是什么原因呢？a. 隐含地假设前额表面处于同一视差b. 忽略了深度的非连续性c. 平坦区域的处理不佳d. 重复的区域 对于a. 隐含地假设前额表面处于同一视差，很多即使是当前最好的损失聚合算法也会假设：在一个小的支持域里面的所有点所处的视差是相同的。但实际情况并非如此，可以观察以上两图，人体头像模型的面部是不规则的表面，展现出来的是视差的不断变化；下面的图是桌子平面，它表面是倾斜的，同样表现出来的是视差的变化。 对于b. 忽略了深度的非连续性，原本假设真实场景中的正面平行表面在支持域内深度不会变化，但是这个假设在深度不连续处的附近被打破。可以看到下图中在台灯灯罩的边界处出现了深度的间断，这样经过损失聚合之后得到的深度就会出现边界误匹配的现象，表现在图中为边界没有很好的对齐。不过利用TAD可以在一定程度上减少这种现象。 目前最好的损失聚合算法都在想方设法地改变支持域的形状以适应在仅在相同的已知视差上做匹配。对于FW而言，就是减小其窗口大小，来减少边界定位问题。但是与此同时，这个改变使得匹配问题变得含糊不清，特别是对于有重复区域以及平滑区域的情形。 对于c与d，FW并不能很好地处理。在这两种情况下，损失聚合算法应该不断地加大支持域的尺寸以获得更多的相同深度上的点。 以上为FW所面对的诸多问题。令人吃惊的是，虽然FW看起来如此不堪一击，但是其应用却是如此广泛。原因可能有以下几点： 容易实现； 快！(特别感谢增量计算框架)； 可以在传统的处理器上实时完成计算； 仅需要很小的内存； 可硬件（FPGA）实时实现，且功率小（&lt;1W） 在介绍更加复杂的算法之前，我们首先介绍积分图像（Integral Images (II)）以及箱滤波（Box-Filtering (BF)）。 积分图像 箱滤波器 可以总结出积分图与箱滤波器的关系： 每个点需要4个运算 积分图可以支持不同的支持域尺寸 积分图有溢出风险 积分图对内存消耗较大 在实际应用当中，积分图对于可变支持域的情况会有帮助。 立体匹配中损失聚合策略的分类及评估在文献1中，作者实现、分类以及评估了超过10种损失聚合算法。这些损失聚合的策略包含几种方式： 位置 方向 位置与方向 权重 接下来就对文中但不限于文中提到的诸多算法进行介绍 (i.e. Fast Aggregation 64, Fast Bilateral Stereo (FBS) 65 and the Locally Consistent (LC) methodology 66)。 固定窗口 可移动窗口11这种方法是为了应对场景边界定位问题，这种算法不限制当前位置位于支持域中心。 多窗口7支持域内元素个数为常数；支持域的形状不限于为矩形；支持域大小可为5、9、25（5W,9W,25W）。下图所示的为9W： 可变窗口12这种方式，支持域的形状是固定的但是尺寸是变化的。支持域的位置是可变的。 基于分割的窗口5这种方式根据图像的颜色相似性将其分割成一系列图像块，这对于损失聚合、深度图像优化以及离群点检测都有帮助。这种算法假设：每个分割块内深度平滑变化。由于涉及到图像分割此时要求分割的精度很高，并且分割后的每个支持域的形状也是不规则的。如下图所示，对于一个可允许的最大支持域范围内，包含支持域中心点所在的分割所覆盖支持域权重赋值为1，支持域的其余部分赋值为$\lambda$，其中$\lambda&lt;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Lytro的光场AR之路：从巅峰到死亡]]></title>
      <url>%2Fposts%2Flytro-light-field%2F</url>
      <content type="text"><![CDATA[这篇文章将会介绍Lytro公司在光场以及VR领域的进展。我们知道NG博士在06年创办了这家公司，曾经被誉为硅谷最优潜力的公司之一。它曾经推出了世界上第一款商用的手持式光场相机，进而推出了第二代产品。但是低调的Lytro已经许久不出现大众的视野中，难道是盛名之下其实难副？声明：一切理解都是本人观点，如有疑问，还望在评论中留言。如需转载请与本人联系，谢谢合作! 邮箱：点我 其实不然，原来人家在搞大事情，Lytro公司在用光场技术搞AR！ 6自由度言归正传，提到AR首先讲下自由度的概念。人类在地球上的运动可以由6自由度来描述。6自由度描述了一个人在空间中的自然运动，6自由度会比3自由度提供两倍的自由度感受空间！具体而言，前三个自由度是大多数VR都支持的围着轴的旋转运动，后三个是沿着轴向的平移运动，这需要配套的位置跟踪设备的支持，如Oculus Rift, HTC Vive 或者Playstation VR。 接下来，详细介绍一下这六个神奇的自由度到底包括哪些。你想或者不想，我们每天都在6个自由度（6DoF）里运动着。 Yaw（平摇） Pitch（俯仰） Roll（翻滚） Left/Right（左右） Up/Down（上下） Forward/Backward（前后） 光场与AR我们生活在一个充满光明的世界，无论是自然光还是人造光，光无时无刻不照普照大地。 光的定向传播会照射在我们周围的物体表面，从而物体才能够显示出其纹理特征。即使是在虚拟的环境中，光场也可以通过计算机图形学(CG)的模拟光线通过在虚拟场景中反射3D对象来进行创建。在光场的所有无限光线中，我们只能看到那些照在我们眼睛上光线，这些光线穿过瞳孔打在视网膜上从而让我们感知光线的存在。对于这些光线，我们的眼睛会根据光的刺激产生某种神经信号，这些信号中包括光线的颜色，强度以及方向等信息。我们的大脑正是通过这些信号来感知大千世界。 当我们在光场中移动时，不同的光线会通过我们的瞳孔，给我们的大脑提供额外的信息，使我们能够理解物体在空间中的位置，同时也会了解物体的其他信息例如：材质、反射、折射等等。 记录光线角度/方向为了捕获和再现光场，需要记录光线的颜色及其路径（方向/角度）。确定光线的颜色和亮度很简单，那么问题在于如何记录光线的方向/角度。在一个光场内捕获的2D图像中的任何像素（实际动作或被渲染），可以提供与该像素所有相交光线的颜色和亮度信息。这通常被称为“光束”（所有的光线被一个像素捕获），但为了简单起见，我们将使用术语“光线”来描述由光线捕获的光线单个像素。 那么如何记录光线的角度和方向呢？目前已经有很多种记录光线的角度和方向路径的技术，但这些技术都需要至少两个点来确定实际的方向/角度。一种常见的方法是使用双平面法（2PP）来计算光线路径。光线穿过两个平面并相交于两点，利用这两个交点，可以确定射线的角度和方向。 视差视差也是记录光场的比较常用的技术。通过计算两个或者多个相邻相机之间拍摄的2D图像的差异可以得到视差信息。这些2D图像是由场景中物体光的颜色亮度组成的彩色像素组成。通过对一系列图像的显著特征进行三角测量，同时比较图像之间像素间的视差，可以计算出各个物体在空间中的位置和距相机的距离（深度）。这种方式可以从2D数据恢复光场。对3D场景的计算机图形渲染，通常会提供对象与摄像机的距离（深度信息），并作为渲染过程的一部分。在以下场景中，我们使用三个相邻的相机对苹果和橘子进行记录。每个单独的相机从不同的位置记录该场景，这会产生图像之间的差异（视差）。 每个2D图像是一组彩色像素的集合，仅仅表示场景中苹果和橙色表面的颜色和亮度。视差必须通过分析这些2D图像之间的差异得到。 利用三个2D图像之间的视差，可以确定苹果和橘子在空间中的位置以及与三个视点之间的距离。随后经过处理的光线角度和颜色信息会记录在光场体（Light Field Volume）中。在VR中，光场体能够为我们提供沉浸式的高质量视觉体验。为了能够达到这个水平，光场体验需要囊括多种视觉效果。例如每个方向上的完美立体感，光场体的全视差和六个自由度，以及正确的场景流，以实现视觉相关效果（镜面反射和折射）。光场无论在实景动作还是计算机渲染，都可以产生最为优秀的VR电影体验。 光场VR设备 Lytro VT不安分的Lytro最近发布了名为“Lytro Volume Tracer”(Lytro VT)的产品，它作为一套强大的工具可以用于CG 3D场景的光场体的创建，同时能够为用户提供视觉高质量以及完全沉浸式的VR体验。 Lytro VT可以使用任何DCC和渲染引擎（例如Maya和VRay）来生成一组3D场景的2D采样。首先，Lytro VT将虚拟相机放置于CG场景中，虚拟相机包含场景中任何可能的视角，需要注意的是这些场景已经包含在定义好的光场体中，并且虚拟相机可以根据需要调整以最大限度地提高显示质量和性能。渲染引擎用于追踪场景中的虚拟光线，并从设备中每个摄像头捕获一定数量的2D图像样本。Lytro VT通过追踪从每个被渲染的像素到其相机的原点的光线(光积跟踪)来创建视觉体，通过以上神操作就可以感受到沉浸式的光场VR体验。 以上是由1000个视点组成的视觉体（图片加载慢，24.74M）。在该视觉体中，VR HMD中的观看者可以体验具有最高级别的光线追踪光学效果，每个方向上完美的视差以及六个自由度（6DOF）的重建虚拟场景。 光线跟踪的样本包括对颜色和深度信息（RGBZ等数据）的跟踪。摄像机的数量及其配置取决于场景的视觉复杂程度以及播放过程中所需视图的预定大小。 Lytro VT处理来自于该2D样本的颜色以及深度信息，并通过Lytro Player创建用于在VR中展示的光场体。 该3D场景中的视图体由白色立方体表示。单个相机由绿色球体表示，它具有自己单独的视点。虚拟的Lytro VT摄像机包含有成百上千个独立的摄像机。2D场景样本渲染使用虚拟装备中每个独立像机进行光线追踪。 以上是一个相机跟踪的来自于场景中5个不同位置的光线的局部放大图，通过对每个独立相机进行光线跟踪就可以重建光场。 光线追迹在将来，Lytro VT与渲染可以和并为一个无缝过程，允许光场直接进行光线跟踪，而不需要2D图像样本的中间步骤。然而这是需要代价的，这一过程需要很强的渲染器集成，并且要放弃这个如今如此灵活的Lytro VT。 作为从虚拟3D场景创建真实2D图像的渲染技术，光线追踪能够产生极高质量的图像。用最简单的术语来说，基于模拟光线与3D场景中的物体表面的相互作用，反映在2D图像平面就是被渲染的彩色像素。 光线追踪适用于精确渲染某些光学效果，例如如反射，折射和散射（光度），但这些需要大量的计算时间。具有全光学效果的光线追踪对于实时帧率而言简直太慢。但是不得不说，光线追踪非常适合需要最高级别图像质量并可以脱机的应用，如电影视觉效果。 上图为光线跟踪的过程：通过虚拟相机的视角可以看到，虚拟相机跟踪到了物体与物体之间的光线反复反射，并最终到达光源的位置。如果有些物体遮挡了光线，那么就会产生被遮挡的光线。这种技术的计算效率很高，因为它只需追踪相机通过虚拟镜头看到的光线路径。Lytro VT和光线追踪是相辅相成的，然而在光线追踪的概念方向上形成对比。如上所示，光线跟踪通过跟踪从固定摄像机向外看光线的路径，从而呈现图像中的彩色像素。相反，Lytro VT通过从一个视觉体内的每个视点向内朝着观察者，去追踪来自每个渲染像素的光线来重建光场体（这句话翻译的不佳，原因是我没太理解VT与光线跟踪的区别…有大神能够理解的话，请在评论区给出）。于是在Lytro Player中，观众在这些密集的光线的移动，沉浸在具有最高级视觉质量的重建CG场景中，并且在每个方向都具有完美的视差和六个自由度。 在这种体验中，光线不是实时呈现，而是从大量预先渲染的光线中实时获取，为视图体积内每个位置的每只眼睛组成一张图像。 PS: Lytro公司在2017年11月30号之后停止了对lytro live photo的线上支持，其相机业务至此告一段落。通过光场VR转型，不知Lytro能否再次创造辉煌？这里留下一个疑问，等待时间的检验吧！ 从巅峰到倒闭3月29日更新。 世界上第一个光场技术初创公司Lytro昨日发表声明，正式宣布倒闭！ 看来时间并不允许Lytro继续存活，光场进阶之路就此截止了吗？早些时间就有传闻称Lytro即将倒闭，Google或将接盘。起因是Google早前公布了一款能够显示沉浸式VR场景的App，这种VR场景据说是由多摄像机采集得到，貌似用到了第三方公司的技术。有人猜测这个第三方公司就是Lytro，这是一家以光场技术著称的公司，它利用其光场采集设备获取场景深度，并将其利用到了VR技术之中。 但这只是猜测，并没有得到印证。有来源显示Lytro早前进行的属于“资产抛售”，抛售额度不超过4000万美元。也有人说，这个额度更低，不超过2500万美元。 有可能接盘的公司包括Google，Facebook，Apple等。有知情人透露，Lytro内部员工已经陆续离职，与此抛售的还有Lytro公司的59项光场专利。这个抛售金额对于Lytro而言简直是低价销售，因为在其成立之初融资金额已经达到了2亿美元，并且到其2017年最后一次融资时已经达到了3.6亿美元！ 投资者多是科技投资巨头，例如Andreessen Horowitz，富士康，GSV，Greylock，NEA，Qualcomm Ventures等。创业艰辛，Lytro同样面对。从2006年成立之初，Lytro就面临着创业圈共同面对的问题。光场技术的硬件实现异常艰难，同时VR技术的发展并没有想像中那么快。同时，大型平台逐渐成为具有说服力的整合商，这是其发展的一大阻力。 与此同时，Lytro的推出的光场相机迷之昂贵，这是VR技术的重要技术支点，同时也成为了其发展中最大的短板。如今看来，Lytro完全有足够的时间和资金提供一个面向大众更具有说服力的报价，以等待在正确的市场条件下推出正确的产品。同时，Lytro公司应该考虑把光场应用到更加广阔的领域，例如无人汽车、智能导航、地图以及游戏等。 一年前Lytro CEO Jason Rosenthal 在其官方博客中写道：“我认为我们有能力重新定义下一代Lytro的产品线、技术以及品控”。仅仅时隔一年，此时或许该轮到Google来完成Lytro的雄心壮志了。虽然Google对Lytro具体有何企图不得而知，但是我们可以确定的是，借助Google这个世界上最大的移动手机系统提供商，如果光场技术能够成功整合，这将是科技界的一大奇迹！ 以下是Lytro官方通告原文（大意是：我虽已死，光场犹存）：At Lytro, we believe that Light Field will continue to shape the course of Virtual and Augmented Reality, and we’re incredibly proud of the role we’ve been able to play in pushing the boundaries of what’s possible. We’ve uncovered challenges we never dreamed of and made breakthroughs at a seemingly impossible pace. We’ve had some spectacular successes, and built entire systems that no one thought possible. More importantly, we built a team that was singularly unified in its focus and unrivaled in its dedication. It has been an honor and a pleasure to contribute to the cinema and Virtual Reality communities, but starting today we will not be taking on new productions or providing professional services as we prepare to wind down the company. We’re excited to see what new opportunities the future brings for the Lytro team as we go our separate ways. We would like to thank the various communities that have supported us and hope that our paths will cross in the future. Lytro was founded in 2006 by Executive Chairman Ren Ng, whose Ph.D. research on Light Field imaging won Stanford University’s prize for best thesis in computer science. In late 2015, Lytro announced the world’s first Light Field solution for Virtual Reality (VR), Lytro Immerge, that was quickly followed by the 2016 launch of Lytro Cinema, the world’s first Light Field capture system for cinematic content. With these products, Lytro pioneered the generational shift from legacy 2D imaging to 3D volumetric video. 后记光场民用领域的践行者离我们而去，不知光场的未来将何去何从？敢问Raytrix和Magic Leap你们可好？ 参考 6DoF Holiday Edition: What Are the Six Degrees of Freedom? What is a Light Field? Ray tracing, Lytro Volume Tracing and CG generated Light Fields in VR Primer on Types of 360° Video for VR Homepage: Techcrunch]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Stephen Hawking]]></title>
      <url>%2Fposts%2Fstephen-hawking%2F</url>
      <content type="text"><![CDATA[昨天晚上，拜访许久没去的羽毛球馆 在去之前有种不好的预感 结果真出事了 脚踝意外扭伤 今天上午去医院拍片 人很多，需要排队 谁也没料到 就在这等待的过程中 霍金去世了 这条消息惊得我一身冷汗 整个人半天没有反应过来 我确认了几遍 无法挽回，这就是事实 维基百科霍金主页瞬间把 1942年1月8日－至今 改成了 2018年3月14日 巧合的是 他的生日恰好是伽利略·伽利莱的忌日 而忌日亦恰好是阿尔伯特·爱因斯坦的生日 世界圆周率日 或许在世界的某个角落 一个婴儿降临在地球 若干年后 响彻寰宇 桌上还有一本他写的 《A Brief History of Time》 至今没有读完... 高中起迷恋你的宇宙大爆炸和虫洞理论 晚自习第三节课 经常会拿出日记本 书写着由你构建的虫洞世界 大学看了 《万物理论》 你作为一个立体的人 重新定义了我对你的认知 考研时英语作文里写到了你 一个永垂不朽的凡人 代表了人类好奇心的极值 你是最接近于外星人的人类 我的偶像 霍金老爷子，一路走好 时间简史，霍耀苍穹 冥想 也许在如此浩瀚的宇宙中 我们人类 仅仅作为宇宙大爆炸数百亿年后 出现的粒子随机组合 相比于宇宙的无穷 人类的寿命抑或人类的历史 只是这其中的 一朵不经意的涟漪 但 这何尝不是一种幸运呢 对吧？ 霍老爷子]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[常用的生产力工具]]></title>
      <url>%2Fposts%2Fsome-useful-tools%2F</url>
      <content type="text"><![CDATA[强大的工具能够极大地提高生产力，而我们的要求似乎会更多一些：我们总是在寻找功能与美观兼得的工具。我是一个喜欢折腾的人，正好我有几个不错的工具推荐给大家。需要注意的是，这些软件是为提高生产力服务的，一味地追求炫酷而不求效率可谓顾此失彼，总之，不可亵玩。 Writing 目前写作使用的是Markdown文本标记语言，比较好的编辑软件有作业部落和有道云笔记。除此之外，印象笔记也不错。这些工具都能够实现不同客户端之间的同步，将使我们不会受制于空间与时间。 桌面端Markdown编辑器使用Haroopad，但有个缺点就是速度比较慢。 Mathtype破解版 Windows/Office大礼包破解工具 Programing 推荐一款跨平台的终端Termius。它同时支持Andriod、IOS以及PC端（Ubuntu &amp; Windows），可以说很好用了。 Cmder完全可以取代Windows下的cmd，同时它还集成了git shell和power shell，可以说很好用很强大了。 另外推荐一个可以在windows系统操纵Linux的神器——MobaXterm，支持直接拖动粘贴复制。这里是v8.2版本。 Clion大礼包，C/C++多平台支持集成开发环境，学生可以申请免费使用。 Visual Studio Code，一个强大的轻量级文本编辑器，支持多插件下载。它将开发者的重心放在了coding上，并没有涉及编译。类似的还有Sublime，Notepad++和Atom。 Screenshots Snipaste，一款支持多种自定义的Windows截图工具，颜值与高效并存。 轻量级录屏软件GifCam Search Everything，在NTFS卷上快速地根据名称查找文件和目录，全盘秒搜！类似的还有Listary等。 APPs on Phone 我觉得GitHub能设计个手机客户端就好了，目前App store里有几个非官方版本的GitHub客户端，但是用户参差不齐。我目前发现的比较好用的有GitBucket (free)，PPHub for Github (￥12)。 Coding手机客户端，能够很方便地查看自己以及他人的coding仓库。界面小清新，还加入了社交功能，但是貌似不太活跃。 Gitter一款聊天社交平台，我的网站（右下角）就用了Gitter提供的API。 MeassureKit，能够利用手机摄像头测距，绘制运动轨迹等非常炫酷的功能。这是一款在内侧阶段一直在玩的AR应用，应该是最早的一批使用Apple提供的ARKIT的APP了吧。世上神器千万，如有推荐，欢迎在评论区中出现。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[深度学习在深度(视差)估计中的应用(2)]]></title>
      <url>%2Fposts%2Fdepth-estimation-using-deeplearning-2%2F</url>
      <content type="text"><![CDATA[最近一段时间一直痴迷于如何将深度学习用于深度估计，看了不少关于该方面的介绍，再次做一个简单的总结。虽说深度学习和深度估计都有深度二字，但是其意义确是完全不一样。一个是deep一个是depth，前者表示网络层纵向的延伸度，后者表示三维场景中物体距离摄像头的距离。这两个差异如此之大的名词是如何结合在一起的呢？且听我慢慢解释。 深度学习的历史在此不做介绍，我们只关心深度学习在深度估计的方面的成果。在开始之前要到一个著名的网络FlowNet，这是Dosovitskiy等人发表在ICCV2015上的作品。这篇文章其实就是做了两件事情：1. 建立了两种结构的FlowNet；2. 建立了一个虚拟场景训练集（Flying Chairs）。最后的测试效果还不错，虽说仅仅在这个数据集上进行了训练，但是泛化能力能够达到业界水平。 FlowNet首先大体看一下这个可以end-to-end训练的网络长得如何，如下图所示：对于输入的图像对，依次经过一个收缩（contractive）网络以及放大（expanding）网络，最后输出得到对应的光流。很难想象CNN可以用来做classification的同时，也可以做到寻找图像之间的相关信息。作者这么做的目的就是为了验证CNN这种强大的特性。原话如是说“The idea is to exploit the ability of convolutional networks to learn strong features at multiple levels of scale and abstraction and to help it with finding the actual correspondences based on these features”。 接下来就是网络的设计环节，首先作者回顾了之前的网络设计策略。一种是最简单的“sliding window”方式，但这种方式的缺点在于计算量很大，它使用了各种优化包括重用网络的临时输出；另外一种对各个层的临时输出做上采样到全分辨率，然后将这些图叠起来，这行对于每一点而言，都能够得到相应的多级特征向量，这个向量可用来学习想要的信息。 Contrasting Part作者受到“per-pixel prediction tasks”的相关工作的启发，设计了两种光流网络框架。一种是相对简单的实现：首先将输入的图像对叠加起来作为输入，然后输入一个网络，让网络自己学，最后提取运动信息。 另外一种方式就是将输入的图像pair（左图&amp;右图）分开训练，提取出高维丰富的信息之后再做相关性连接，即增加了correlation layer。这个correlation layer是为了衡量左右图相应位置的相似度而设置的。一个的很直观地理解就是，在左图选取一个patch，同时在右图中的可能的位置选择同样大小的patch进行匹配运算(点积运算或者说是卷积运算)。具体而言，分别在左右feature map（$f_1$和$f_2$）以$x_1$和$x_2$为中心的块之间进行卷积运算。correlation的定义如下： c(x_1,x_2)=\sum_{o \in [-k,k] \times [-k,k]}{}其中f_1和f_2的维度为w \times h \times c。可以看到计算一次$c(x_1,x_2)$需要$c \times K^2$次乘法，$K:=2k+1$。而对于所有的位置则需要$w^2 \times h^2 \times c \times K^2 $次乘法，可想而知，这个计算量是巨大的。于是作者为了减少运算量，对搜素窗口进行了限制，设置了最大的搜索半径为$d$，则$x_2$就能在窗口大小是$D=2d+1$里计算correlation了。另外值得一提的是，我们以上的过程是在计算光流信息，所以应该在某个某个窗口内进行匹配，而不是在某个方向，而后续即将提到的DispNet的 correlation 是在某一个方向上进行搜索。那么最后得到的correlation的维度是$w \times h \times D^2$。 Expanding Part如下是优化网络的结构，大部分都是结合缩放阶段信息的反卷积操作。这里不再赘述，最后得到的结果图像大小是输入图像的$1/4$。 最后作者利用variational approach对低分辨率的输出做了20次迭代以得到高分辨率的光流图，之后又对全分辨率的光流图做了进一步优化。 很多深度估计的工作受到这一篇论文启发，特别是correlation layer实现了寻找图像对之间的相关性这一点，对于后续DispNet的诞生起到重要作用。其实DispNet就是在FlowNet的基础上进行的改进，接下来就会详细的介绍DispNet。 DispNet受到FlowNet的启发，另外一篇论文将光流估计拓广到了视差以及场景流的估计。2015年12月放在arxiv上的大作A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation便是具体代表性的作品。这篇文章有两个主要贡献： 建立了三个仿真视差数据集（3 stereo video datasets），这是当时第一个超大规模的用于视差以及光流场景流训练以及评估的数据集； 设计了DispNet，SceneFlownet。 开篇文章一开始就说估计场景流这个问题，堪称“皇家赛事”级别的任务（场景流是估计空间三维物体的运动场，相比于光流多了一个深度信息）。然后又提到没有好的数据集无法做到完美的训练，那么这么好的数据集又不是天上掉下来的，那只能自食其力，自力更生，自己造吧，于是他们的超大规模（35000 stereo frames）的数据集便成功诞生！ 模型参考了前方开创性的FlowNet工作之后，他们就把FlowNet改造成DispNet，真是华丽丽的变身。然后对于场景流模型的构建，作者提到：虽然在以往的成千上万的论文中均有涉及到光流的估计，但是仅仅有极少数的工作敢去尝试估计场景流。然后作者就是厉害，他们足够相信CNN具有强大的学习与抽象能力，能够通过某种方式的组合使场景流的估计问题转化成为学习问题。文中提到“相机无关的场景流的学习可以转化成视差，光流以及光流变化学习问题”，于是在实际上文中提到的SceneFlownet就是FlowNet与DispNet的组合。以下重点介绍DispNet的实现。 DispNet &amp; DispNetC收缩部分从conv1到conv6b；在放大部分，upconvolutions (upconvN), convolutions (iconvN, prN)和loss layers是交替出现的。从低层提取的特征与高层的特征进行串联，增加特征的多样性。最后的网络输出是pr1。 二者与Dosovitskiy提出的FlowNet两种结构差不多，总结起来共有3个变化： 对原来的FLowNet进行了改造，在上卷积层之间增加了卷积层，这样可以使得最终的深度图像更加的平滑； 将原来的2D correlation 改造成了1D correlation；并且发现加入correlation 层之后会有普遍的效果提升；原因在于左右图均进行了rectify，基于极线约束，我们就可以在一个方向进行搜索。所以类似于FlowNet，我们得到的correlation map的大小是$D \times w \times h$。 放大部分比Flownet多做了一次deconv，使输出为原来的$1/2$。注意：DispNet对应于FlowNet的第一种实现，DispNetC对应于FlowNet的第二种实现； 数据增强虽然文中提出了一个超大的数据集，但是仍然需要一定的数据增强以获得更加多样的训练数据。具体方法：空间变化（旋转，变形，裁剪，缩放），色度变换（颜色，对比度，明暗）。 结果文中比较了Zbontar&amp;LeCun的MC-CNN以及opencv中SGBM方法。DispNet是在FlyingThings3D数据集上做得训练，然后在KITTI 2015数据集上做了优化，注意“K”表示优化之后的网络。 其他网络To be continued, more depth net pls refer to this link. 问题 此处对correlation的理解还不透彻; 待补充其他类型深度/视差估计网络； Change Log 添加了对correlation的解释。 参考 Homepage: Freiburg: Pattern Recognition and Image Processing Homepage: FlowNet: Learning Optical Flow with Convolutional Networks Paper: FlowNet Paper: A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation Github: Stereo matching by training a convolutional neural network to compare image patches Blog: Disparity Estimation by Deep Learning Paper: Unsupervised Adaptation for Deep Stereo Code: Unsupervised Adaptation for Deep Stereo Blog: 【论文学习】神经光流网络——用卷积网络实现光流预测（FlowNet: Learning Optical Flow with Convolutional Networks） Blog: 论文阅读笔记之Dispnet]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[深度学习在深度(视差)估计中的应用(1)]]></title>
      <url>%2Fposts%2Fdepth-estimation-using-deeplearning-1%2F</url>
      <content type="text"><![CDATA[本文对KITTI stereo 2015 datasets 冠军之作Cascade Residual Learning: A Two-stage Convolutional Neural Network for Stereo Matching进行简要解读。 前言目前深度学习发展的如火如荼，利用CNN可以将图像对的匹配问题看成一个学习问题。但是如何能够得到高质量的深度图像仍然是一个普世问题。本文作者提出了一种新型的层叠式（cascade）CNN结构（CRL:Cascade Residual Learning）去估计深度信息。深度估计的过程大致可以分成两个步骤： 在现有的DispNet的基础上添加几个反卷积模块，目的是为了得到full resolution的初始的深度信息，同时能够学习到更多的细节信息； 第二步是对第一步中学习到的深度信息进行校准（rectify）；这一步利用到了第一步得到的多尺度的深度信息，然后并非是直接学习到优化后的深度信息，而是学习了每个尺度下的深度残差，然后结合第一步中得到的多尺度深度信息合成最终的深度图（这里有点类似于何凯明的residual的思想： It is easier to learn the residual than to learn the disparity directly）。 网络结构下面详细的介绍下这个网络的结构: 可以很清楚地在上图中看到这两个不同的阶段。对于第一个阶段，类似于文献[1]中提到的DispNetC结构（C是correlation层的意思），本文作者同样采取了沙漏形的网络结构。但是DispNetC网络的输出图像的分辨率只有原始尺寸的一半！CRL中的DispFulNet在DispNetC的基础上，在最后的两个卷积层增加了添加了反卷积模块，然后再串联左图；通过再次添加一个额外的卷积层，可以使得网络输出为全分辨率（和左右图大小一致）。注意：每个尺度（共6个尺度）上的临时输出与其对应的ground truth之间计算$l_1$损失。总结一下就是，这个DispFulNet学习了这样一个网络：通过输入一对图片$I_L$和$I_R$，学习到了视差$d_1$，使得： \tilde{I}_L(x,y)=I_R(x+d_1(x,y),y)上式中的$\tilde{I}_L$就是把右图根据视差移动后的结果，我们的目标就是$\tilde{I}_L$越来越接近$I_L$。 接下来就是第二阶段，将$I_L$,$I_R$,$\tilde{I}_L$,$d_1$以及$e_L=|I_L-\tilde{I}_L|$串联起来[2]作为dispResNet的输入。此优化网络最后学到的是多尺度的残差$ \{r_2^{(s)} \} _s^S$，其中s=0时表示全尺度残差。最后与DispFulNet输出的多尺度深度图$\{d_1^{(s)}\}_s^S$做和运算得到最后优化后的深度$\{d_2^{(s)}\}_s^S$： d_2^{(s)}=d_1^{(s)}+r_2^{(s)},0 \leq s \leq S于是$d_2^{(0)}$就是最后的全尺度输出。 实验结果以下是对其结果展示： 参考文献[1]. N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4040–4048, 2016.[2]. E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. Flownet 2.0: Evolution of optical flow estimation with deep networks. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2462–2470, 2017.[3]. KITTI: Stereo Evaluation 2015[4]. code: Cascade Residual Learning (CRL)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Matlab Deep Learning学习笔记]]></title>
      <url>%2Fposts%2FMatlab-Deep-Learning%2F</url>
      <content type="text"><![CDATA[最近对深度学习尤其着迷，是时候用万能的Matlab去践行我的DL学习之路了。之所以用Matlab，是因为Matlab真的太强大了！自从大学开始我就一直用这个神奇的软件，算是最熟悉的编程工具。加上最近mathworks公司一大波大佬的不懈努力，在今年下半年发行的R2017b版本中又加入了诸多新颖的特性，尤其在DL方面，可以发现：仅仅几条简单的代码，就能够实现复杂的功能。基于以上，我在本文列举了几个在Matlab上学习Deep Learning的例子：1. 手写字符识别；2. 搭建网络对CIFAR10分类；3.搭建一个Resnet。务必保证主机已经安装Matlab 2017a及以上。 手写字符识别利用CNN做数字分类实验。 接下来的实验会阐明如何进行： 加载图像数据 设计网络结构 设置网络训练参数 训练网络 预测新数据的类别 加载图像数据12345678910111213digitDatasetPath = fullfile(matlabroot,'toolbox','nnet','nndemos',... 'nndatasets','DigitDataset');% imageDatastore函数 能够通过文件夹名自动地把数据存储成ImageDatastore 对象digitData = imageDatastore(digitDatasetPath,... 'IncludeSubfolders',true,'LabelSource','foldernames');% Display some of the images in the datastore.figure;perm = randperm(10000,25);for i = 1:25 subplot(5,5,i); imshow(digitData.Files&#123;perm(i)&#125;);end 以下是手写字符的部分数据： 创建训练集与验证集123trainNumFiles = 750;[trainDigitData,valDigitData] = splitEachLabel(digitData,750,'randomize'); % 每类有1000个，选择其中的750类作为训练集，剩下的作为验证集；此处750可以换成一个比例：75% 注意Matlab里面支持的层的类型，包括：CLICK THIS LINK。如下所示： Epoch Iteration Layer Type Function Image input layer imageInputLayer Sequence input layer sequenceInputLayer 2-D convolutional layer convolution2dLayer 2-D transposed convolutional layer transposedConv2dLayer Fully connected layer fullyConnectedLayer Long short-term memory (LSTM) layer LSTMLayer Rectified linear unit (ReLU) layer reluLayer Leaky rectified linear unit (ReLU) layer leakyReluLayer Clipped rectified linear unit (ReLU) layer clippedReluLayer Batch normalization layer batchNormalizationLayer Channel-wise local response normalization (LRN) layer crossChannelNormalizationLayer Dropout layer dropoutLayer Addition layer additionLayer Depth concatenation layer depthConcatenationLayer Average pooling layer averagePooling2dLayer Max pooling layer maxPooling2dLayer Max unpooling layer maxUnpooling2dLayer Softmax layer softmaxLayer Classification layer classificationLayer Regression layer regressionLayer 创建自己的网络结构123456789101112131415161718192021222324%% Define Network Architecture% Define the convolutional neural network architecture.layers = [ imageInputLayer([28 28 1]) convolution2dLayer(3,16,'Padding',1) batchNormalizationLayer() reluLayer() maxPooling2dLayer(2,'Stride',2) convolution2dLayer(3,32,'Padding',1) batchNormalizationLayer() reluLayer() maxPooling2dLayer(2,'Stride',2) convolution2dLayer(3,64,'Padding',1) batchNormalizationLayer() reluLayer() fullyConnectedLayer(10) softmaxLayer() classificationLayer() ]; 以下就是该网络结构及参数设置： 123456789101112131415 1 '' Image Input 28x28x1 images with 'zerocenter' normalization 2 '' Convolution 16 3x3 convolutions with stride [1 1] and padding [1 1 1 1] 3 '' Batch Normalization Batch normalization 4 '' ReLU ReLU 5 '' Max Pooling 2x2 max pooling with stride [2 2] and padding [0 0 0 0] 6 '' Convolution 32 3x3 convolutions with stride [1 1] and padding [1 1 1 1] 7 '' Batch Normalization Batch normalization 8 '' ReLU ReLU 9 '' Max Pooling 2x2 max pooling with stride [2 2] and padding [0 0 0 0]10 '' Convolution 64 3x3 convolutions with stride [1 1] and padding [1 1 1 1]11 '' Batch Normalization Batch normalization12 '' ReLU ReLU13 '' Fully Connected 10 fully connected layer14 '' Softmax softmax15 '' Classification Output crossentropyex 网络训练参数设计123456 options = trainingOptions('sgdm',...'MaxEpochs',3, ... % 训练最大轮回'ValidationData',valDigitData,... % 验证集'ValidationFrequency',30,...'Verbose',false,...'Plots','training-progress'); 开始训练1net = trainNetwork(trainDigitData,layers,options); 测试新的数据123predictedLabels = classify(net,valDigitData);valLabels = valDigitData.Labels;accuracy = sum(predictedLabels == valLabels)/numel(valLabels) 查看某层参数例如查看第2层的weight参数，输入以下命令：123456montage(imresize(mat2gray(net.Layers(2).Weights),[128 128]));set(gcf,'color',[1 1 1]); frame=getframe(gcf); % get the frameimage=frame.cdata;[image,map] = rgb2ind(image,256); imwrite(image,map,'weight-layer2.png'); 图像如下所示： 再看一下第10层的参数：12345678910111213141516171819[~,~,iter,~]=size(net.Layers(10).Weights);name='weight.gif';dt=0.4;for i=1:iter montage(imresize(mat2gray(net.Layers(10).Weights(:,:,i,:)),[128 128])); set(gcf,'color',[1 1 1]); %变白 title(['Layer(10), Channel: ',num2str(i)]); axis normal truesize %Creat GIF frame(i)=getframe(gcf); % get the frame image=frame(i).cdata; [image,map] = rgb2ind(image,256); if i==1 imwrite(image,map,name,'gif'); else imwrite(image,map,name,'gif','WriteMode','append','DelayTime',dt); endend 搭建网络对CIFAR10分类CIFAR10和CIFAR100是80 million tiny images的子集，是由Geoffrey Hinton的弟子们Alex Krizhevsky和Vinod Nair共同采集。 CIFAR10CIFAR10由60000张32*32的彩色图像组成，一种分成10类，平均每类图像6000张。共有50000张训练图像，10000张测试图像。这个数据集被分成了5个分支，其中每个分支10000张。测试集包含每类中随机选择的1000张图像。训练集就是剩下的那些图像。对于每个分支的数据的大小是：10000*3072；其中3072=32*32*3。数据以行优先的顺序存储，所以前1024个数据是r通道的数据，接下来的1024个数据是g通道的数据，最后1024个数据是b通道的。假如原始的数据是data，我们想要将其重新排列成我们需要的数据。首先对其进行转置，然后再用reshape函数对图像重组（可选：最后将图像前两维互换（转置），之所以这么做，可以更好的可视化）。 123XBatch = data';XBatch = reshape(XBatch, 32,32,3,[]);XBatch = permute(XBatch, [2 1 3 4]); 以下是cifar10的部分数据。共有10类，包括：airplane，automobile，bird，cat，deer，dog，frog，horse，ship，truck。 Just run it接下来我们就开始运行以下代码，来训练我们的网络。闲话少说，我把代码放在了Github，欢迎$star$。 1234567891011121314151 'imageinput' Image Input 28x28x1 images with 'zerocenter' normalization2 'conv_1' Convolution 16 3x3x1 convolutions with stride [1 1] and padding [1 1 1 1]3 'batchnorm_1' Batch Normalization Batch normalization with 16 channels4 'relu_1' ReLU ReLU5 'maxpool_1' Max Pooling 2x2 max pooling with stride [2 2] and padding [0 0 0 0]6 'conv_2' Convolution 32 3x3x16 convolutions with stride [1 1] and padding [1 1 1 1]7 'batchnorm_2' Batch Normalization Batch normalization with 32 channels8 'relu_2' ReLU ReLU9 'maxpool_2' Max Pooling 2x2 max pooling with stride [2 2] and padding [0 0 0 0]10 'conv_3' Convolution 64 3x3x32 convolutions with stride [1 1] and padding [1 1 1 1]11 'batchnorm_3' Batch Normalization Batch normalization with 64 channels12 'relu_3' ReLU ReLU13 'fc' Fully Connected 10 fully connected layer14 'softmax' Softmax softmax15 'classoutput' Classification Output crossentropyex with '0', '1', and 8 other classes 以下是训练过程输出： Epoch Iteration Time Elapsed (seconds) Mini-batch Loss Mini-batch Accuracy Base Learning Rate 1 1 0.06 2.3026 8.59% 0.0020 1 50 1.27 2.3026 14.06% 0.0020 1 100 2.52 2.3024 7.81% 0.0020 1 150 3.73 2.2999 20.31% 0.0020 1 200 5.01 2.2740 15.63% 0.0020 1 250 6.28 2.1194 21.09% 0.0020 1 300 7.58 1.9100 23.44% 0.0020 1 350 8.86 1.8892 28.13% 0.0020 2 400 10.08 1.7490 29.69% 0.0020 2 450 11.32 1.8377 31.25% 0.0020 2 500 12.57 1.6073 39.84% 0.0020 … … … … … … 20 7650 407.74 0.2858 93.75% 2.00e-05 20 7700 409.06 0.3127 89.84% 2.00e-05 20 7750 410.38 0.3254 87.50% 2.00e-05 20 7800 411.64 0.2456 92.19% 2.00e-05 最后测试我们的模型的性能，accuracy=76%左右。但是训练时，我们的batch-accuracy已经达到了90%以上，说明我们的模型过拟合了。显然这不是我们想要的结果，进一步的调参将会在此补充。 可视化某层的参数123456789101112131415% Extract the first convolutional layer weightsw = cifar10Net.Layers(2).Weights;% rescale and resize the weights for better visualizationw = mat2gray(w);w = imresize(w, [100 100]);figuremontage(w)name='cifar10-weight-layer2';set(gcf,'color',[1 1 1]);frame=getframe(gcf); % get the frameimage=frame.cdata;[image,map] = rgb2ind(image,256); imwrite(image,map,[name,'.png']); 搭建一个Resnet接下来，为了验证下这个DL工具包的强大之处，我打算纯手工建一个Resnet。为方便起见，我搭了一个Resnet34（更深的网络敬请期待吧）。这里是它的prototxt，我们可以用网络可视化工具进行查看resnet34的结构。以下是Resnet34的一部分（太长了没有截下全部视图）。 定义每一层与连接层以从pool1到res2a为例子建立网络。 12345678910111213141516layers_example=[ % pool1 - res2a maxPooling2dLayer(3, 'Stride', 2,'Name','pool1'); % branch2a convolution2dLayer(3,64,'Stride', 1,'Padding', 1,'Name','res2a_branch2a') batchNormalizationLayer('Name','bn2a_branch2a') reluLayer('Name','res2a_branch2a_relu') % branch2b convolution2dLayer(3,64,'Stride', 1,'Padding', 1,'Name','res2a_branch2b') batchNormalizationLayer('Name','bn2a_branch2b') % add together additionLayer(2,'Name','res2a') reluLayer('Name','res2a_relu')]; 上述过程仅仅完成了网络的一个小分支，记下来要完成res2a_branch1这部分的连接。这时候要用到DAG的一些方法。通过添加新层同时建立新的连接即可，方式如下。 1234567891011121314lgraph = layerGraph(layers_example);figureplot(lgraph)%% add some connections (shortcut)layers_2a=[ convolution2dLayer(1,64,'Stride', 1,'Padding', 0,'Name','res2a_branch1') batchNormalizationLayer('Name','bn2a_branch1')];lgraph = addLayers(lgraph,layers_2a);lgraph = connectLayers(lgraph,'pool1','res2a_branch1');lgraph = connectLayers(lgraph,'bn2a_branch1','res2a/in2');% show netplot(lgraph) 其他部分的构建同上，经过一系列重复的工作，我们可以构建出这个不太深的Resnet34，全部代码见我的Github。 一些基本问题 参数的基本格式 Height \times Width \times (\#Channels) \times (\#Filters) SGD是什么？可以参见好友写的一篇博文。 什么是epoch？模型训练的时候一般采用stochastic gradient descent（SGD），一次迭代选取一个batch进行update。一个epoch的意思就是迭代次数*batch的数目 和训练数据的个数一样，就是一个epoch。 为什么要是用BN？Batch normalization layers normalize the activations and gradients propagating through a network, making network training an easier optimization problem. Use batch normalization layers between convolutional layers and nonlinearities, such as ReLU layers, to speed up network training and reduce the sensitivity to network initialization. RELU的作用？Max-Pooling Layer Convolutional layers (with activation functions) are sometimes followed by a down-sampling operation that reduces the spatial size of the feature map and removes redundant spatial information. Down-sampling makes it possible to increase the number of filters in deeper convolutional layers without increasing the required amount of computation per layer. One way of down-sampling is using a max pooling. The max pooling layer returns the maximum values of rectangular regions of inputs. add more Resnet中scale层是如何定义的？有什么用途？ Resnet中为何残差$F(x)$比$H(x)$好学？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CNN框架(CNN Architectures)]]></title>
      <url>%2Fposts%2FCNN-Architectures%2F</url>
      <content type="text"><![CDATA[本文来自于CS231N（2017 Spring），将介绍几种较为常见的CNN结构。以下网络均是ImageNet比赛的冠军之作，我们将从网络结构，参数规模，运算量等来描述各个网络的特点。 AlexNet VGG GoogLeNet ResNet 后续将补充以下几种网络： NiN(Network in Network) wide ResNet ResNeXT stochastic Depth DenseNet FractalNet SqueezeNet 以下是正文。 AlexNet网络结构网络的输入大小为：227*227*3，每一层的结构以及参数设置如下： Layer Type #Filters Stride Pading OUTPUT SIZE Parameters CONV1 #96 @11*11 4 0 55*55*96 11*11*3*96 MAXPOOL1 3*3 2 0 27*27*96 0 NORM1 27*27*96 55*55*96 CONV1 #256 @5*5 1 2 27*27*256 55*55*96 MAXPOOL2 3*3 2 0 13*13*256 55*55*96 NORM2 13*13*256 55*55*96 CONV3 #384 @3*3 1 1 13*13*384 55*55*96 CONV4 #384 @3*3 1 1 13*13*384 55*55*96 CONV5 #256 @3*3 1 1 13*13*256 55*55*96 MAXPOOL3 3*3 2 0 6*6*256 55*55*96 FC6 4096 55*55*96 FC7 4096 55*55*96 FC8 1000 55*55*96 The size of output image is $\frac{N-Conv+2\times Pading}{stride}+1$ 后续将使用Matlab DL 工具包补充Alexnet实验… VGGNetThe winner of ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014. 网络结构small filters, deeper networks。将原来8层的AlexNet扩展到了16&amp;19层。卷积层的大小仅仅有3*3，stride=1，pad=1；池化层仅仅有stride=2的2*2的MAXPOOL。以下是其与AlexNet的结构对比图。 更加具体的，VGG16的网络的参数个数以及内存消耗如下： Q：为何采用更小的CONV？A：几个3*3的CONV叠加后的接受域和一个7*7大小的CONV的接受域一致，但是与此同时，网络层数变深，引入了更多的非线性，参数数量更少。（Stack of three 3x3 conv (stride 1) layers has same effective receptive field as one 7x7 conv layer，But deeper, more non-linearities. And fewer parameters: $3\times3^2C^2$ vs. $7^2C^2$ for C channels per layer） 更多细节 ILSVRC’14 2nd in classification, 1st in localization Similar training procedure as Krizhevsky 2012 No Local Response Normalisation (LRN) Use VGG16 or VGG19 (VGG19 only slightly better, more memory) Use ensembles for best results FC7 features generalize well to other tasks GoogLeNet论文地址：https://arxiv.org/pdf/1409.4842.pdf代码地址：NULLDeeper networks, with computational efficiency。GoogLeNet是ILSVRC’14的图像分类冠军网络，它加入了Inception模块，并且去除了全连接层，大大减少了参数的个数。 22 layers (with weights) Efficient “Inception” module No FC layers Only 5 million parameters! 12x less than AlexNet ILSVRC’14 classification winner (6.7% top 5 error) “Inception module”精心设计了一个局部网络模块，并且将这些模块叠加构成GoolgeNet。这种经过精心设计的模块就是Inception。（design a good local network topology (network within a network) and then stack these modules on top of each other）。Inception包含几个接受域不同的CONV核（1*1，3*3，5*5）以及池化操作（3*3）；最终将这些操作后的输出在depth方向串联。以下是两种两种不同的实现方式，左图时原始的inception模块，右图是改进版的inception模块。对于naive inception而言，它面临这运算量巨大的问题。由于池化层的输出会保留原始输入的depth，所以经过CONV&amp;MAXPOOL过后的输出的feature map势必比原始输入的depth更深。那么如何去解决以上问题呢，一个通常的方式就是降维。我们在每个CONV前加上1*1的CONV（“bottleneck” layers）来减少feature map的维度。所谓的1*1CONV就是在保持输入的空间分辨率不变的情况下来减小depth维度，即通过将不同depth上的feature map进行组合，从而将输入的feature map映射到更低的depth维度上。经过以上操作就可以将运算的操作次数大大降低。 于是GoogLeNet的全貌如下： ResNet利用残差连接成的超级深网络。这里有一个何凯明在ICML2016的Tutorial，内容比较详细。ICML 2016 Tutorial on Deep Residual Networks代码在这里Code: deep-residual-networks 概况 152-layer model for ImageNet ILSVRC’ 15 classification winner (3.57% top 5 error) Swept all classification and detection competitions in ILSVRC’ 15 and COCO’ 15! 深度增加带来的问题从上图可以发现，当网络层数增加时，训练误差和测试误差都有所下降。这并不符合以往的经验，我们会想，既然网络层数增加了，那么模型参数势必增多，此时会造成过拟合。然而过拟合的表现是：训练误差减小，测试误差增大。但是事实和分析并不吻合。何凯明认为：The problem is an optimization problem, deeper models are harder to optimize。这是一个优化问题，更深的网络更难优化。并且，更深的网络应该至少比浅层网络不差，这是因为我们可以通过拷贝浅层网络+identity mapping（恒等映射）来构造一个更深的网络，这个结构化的方案表明深层网络可以达到和浅层网络一致的性能。 解决方案Use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping.作者假设：相较于最优化最初的无参照映射（残差函数以输入x作为参照），最优化残差映射是更容易的。利用网络去拟合残差$F(x)$，并非直接拟合$H(x)$。 整个ResNet框架 Stack residual blocks Every residual block has two 3x3 conv layers Periodically, double # of filters and downsample spatially using stride 2 (/2 in each dimension) Additional conv layer at the beginning No FC layers at the end (only FC 1000 to output classes) 对于ImageNet比赛而言，ResNet设置的网络深度有34、50、101以及152层。对于层数较多的网络，利用“bottleneck”（类似于GoogLeNet的1*1卷积操作）来提高效率。 总结论文An Analysis of Deep Neural Network Models for Practical Applications 比较了2016年以来的一些神经网络的规模、运算量、能耗以及精度等项目。可以从上图总结出以下几点： GoogLeNet: most efficient VGG: Highest memory, most operations AlexNet: Smaller compute, still memory heavy, lower accuracy ResNet: Moderate efficiency depending on model, highest accuracy 其他网络变体后续补充。 疑问 ResNet为何能够使网络层数更深，应如何正确理解残差网络？He是受何启发从而发明了这种结构？ more questions will be added… 参考文献 DeepLearning.net Reading List ImageNet Classification with Deep Convolutional Neural Networks 为什么ResNet和DenseNet可以这么深？一文详解残差块为何有助于解决梯度弥散问题 An Analysis of Deep Neural Network Models for Practical Applications CS231n: Convolutional Neural Networks for Visual Recognition Densely Connected Convolutional Networks Deep Residual Networks (Deep Learning Gets Way Deeper)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[理解LSTM网络【译】]]></title>
      <url>%2Fposts%2Flstm%2F</url>
      <content type="text"><![CDATA[本文是我对大神Christopher Olah的Understanding LSTM Networks的译文。 循环神经网络（Recurrent Neural Networks）人们并非每时每刻都在大脑一片空白时开始思考。当我们读这篇文章的时候，我们会根据之前学习的知识来理解当前你在阅读的内容。我们不是把原来的知识丢的一干二净来重新学习，我们的记忆有一定的持久性。传统的神经网络做不到这些，这是它的一大缺陷。比如说，可以想象有这样一种情况，我们想知道一部电影的每一帧画面正在发生什么。使用传统的神经网络很难通过理解电影之前的画面来推断以后将要发生的事件。（传统的神经网络不能处理带有时序的样本）循环神经网络（Recurrent Neural Networks）尝试解决了以上问题。这种网络是一种带有循环结构的网络，可以使得信息得以持久保持。 上图是一个RNN模块，$A$读取输入$x_i$，同时输出$h_t$，$A$这个循环允许信息从网络的当前步骤传递到下一步骤。上述过程把RNN的过程讲的有些神秘感。但是，如果我们仔细想想，这也不比一个正常的神经网络难以理解。一个RNN可以看成是多个相同网络的拷贝，每一个拷贝都会向后续网络传递信息，下图我们把RNN展开。这种链式的特性揭示了RNN与序列和列表有关，RNN是对这些数据最自然的表达。RNN目前已经被广泛使用！在过去的几年间，RNN在很多领域都有着出色的表现：语音识别，语言建模，翻译，图像描述…推荐大家阅读大神Andrej Karpathy的博文 The Unreasonable Effectiveness of Recurrent Neural Networks，来领略下RNN的诸多应用，这简直不能太棒！以上的成功案例离不开使用长短期记忆（LSTM: Long Short Term Memory）网络，这是一种特殊的RNN网络并且能够应对更多任务，相比于标准的RNN网络，它具有更为出色的表现。几乎所有的RNN都是基于LSTM来实现的，接下来我们讨论下LSTM的奥义。 长期依赖（Long-Term Dependencies）问题RNNs的要求之一就是它能够连接之前的信息到当前的任务之上，例如利用之前的视频帧来理解当前帧的内容。如果RNNs能够做到这一点的话，它将会超级有用。但是它真的可以吗？看情况而定。有时，我们仅仅需要离当前任务最近的几个任务信息。例如，我们有一个语言模型，它的目标是根据当前已有的词语去预测接下来将要出现的词语。如果我们尝试去预测“the clouds are in the sky”中的最后一个单词，我们不需要任何更多的语料，很明显最后一个单词将会是“sky”。在这种情况下，相关信息和当前需要预测词的位置的间隔是非常小的，这时RNNs可以去利用过去的信息。但是也有一种情况是，我们需要更多的信息才能够做预测。例如我们的语言模型需要预测下面句子的最后一个单词“I grew up in France… I speak fluent French.”。从相邻近的几个单词可以推断最后一个单词可能是一种语言，但如果我们想要知道到底是哪种语言的话，我们需要句子最前面的一个单词“France”。这会使得相关信息以及需要预测词的位置的间隔变得很大。遗憾的是，随着间隔的逐渐增大，RNNs不能够去关联有用的信息。在理论上，RNNs能够解决长期依赖的问题。人们可以通过仔细选取参数来解决这类问题。但是实际上RNNs并不能去解决这个问题。 Hochreiter (1991) [German] and Bengio(1994)等人深入研究了该问题为何如此艰难。阿弥陀佛，LSTMs并没有上述问题。 LSTM网络长短期记忆网络——经常被叫做“LSTM”——是RNN的这一种特殊的形式，它能够解决长期依赖的问题。LSTM是由Hochreiter &amp; Schmidhuber (1997)提出，并由很多后来者完善以及推广。LSTM能够在很多问题上取得优秀的结果，现如今被广泛引用。LSTM被设计成防止长期依赖问题的发生。在实践中，LSTM的长期记忆是默认行为，而并非艰苦习得！所有的RNN都有链式重复的神经网络模块。在标准的RNN中，这些重复的模块仅仅有简单的结构，例如$tanh$层。当然，LSTM中也存在这样的链式结构，但是其中的重复模块就大为不同了。LSTM的重复的模块中包含4种不同的层，它们以一种特殊的结构交错着。看不懂，不用担心，细节即将展开。接下来，我们会一步步来讲解LSTM的网络结构。首先我们先明确几个会经常用到的表示方法。在以上的图示中，每条实线传输着整个向量，从一个节点的输出到其他节点的输入。粉红色的圆圈表示的是逐点操作，例如向量的加法，黄色的方形表示已经学习了的网络层。汇集的线表示串联，分叉的线表示复制操作，这些复制的内容流向不同的位置。 LSTM背后的核心技术LSTM的关键技术在于细胞（cell）状态，也就是图表中最上方水平穿行的直线。细胞状态可以理解成是一种传送带。它仅仅以少量的线性相交，贯穿整个链式结构上方。信息沿着这条传动带很容易保持不变。LSTM有一种能向细胞增加或者移除信息的能力，这种经过精心设计的结构称作门（gates）。所谓的门就是一种让信息选择性通过的方法。它是由一个$sigmoid$层和一个逐点乘法单元构成。如下图： $sigmoid$层的输出是$0$-$1$之间的数字，它描述了每个部分可以有多少量能够通过。$0$代表“啥都不能通过”，$1$代表“啥都能通过”！一个LSTM有三种这样的门结构，用来保证以及控制细胞状态。 逐步理解LSTMLSTM的第一步是来决定啥信息将要从细胞状态中丢弃。这个决定是由一个叫做“遗忘门”（“forget gate layer”）的$sigmoid$层来决定。它的输入是$h_{t-1}$和$x_t$，输出是一个介于$0$到$1$之间数值，给每个在状态$C_{t-1}$的数值。$1$表示“完全保留”，$0$表示“完全丢弃”。让我们回到之前的的语言模型的例子中，我们还是基于以前的词语来预测后续的单词。在这样一个问题中，细胞状态可能会包含当前主语的性别信息，所以正确的介词将会被使用。当我们看到一个新的主语时，我们要遗忘掉旧的的主语。 下一步就是决定啥新信息将要存储在细胞状态中。这包括两个方面。第一，一个叫做“输入门层”的$sigmoid$层来决定哪些值我们要更新；第二，一个$tanh$层创造了新的候选值$\tilde{C}_t$，这个值将会加入到新的状态中去。进一步，我们要把上述两个方面结合起来来更新细胞状态。在我们的语言模型中，我们想要在新的主语对应的细胞状态中加入性别信息，去代替我们遗忘掉的那个旧主语状态。 我们现在更新旧的细胞状态，从状态$C_{t-1}$到状态$C_{t}$。上述步骤已经详述了具体如何操作，我们现在就开始更新！我们将旧的细胞状态乘以$f_t$，目的是忘记我们要忘记的旧状态。然后我们加上$i_t*\tilde{C}_t$，这就是我们创造新的候选值，这个值根据我们想要更新每个状态值的程度进行伸缩变化（这就是$i_t$的意义）。在我们的语言模型中，这就是我们根据最开始确定的目标，丢弃旧主语性别以及增加新主语信息的地方。 最后一步我们要决定到底输出什么信息。这个输出信息会基于细胞状态，但将会是一个经过过滤后的结果。首先，我们用一个$sigmoid$层去决定细胞状态的哪一部分将会被输出。然后，我们将细胞状态通过$tanh$（将其值规范到$-1$到$1$之间）。最后我们将这个值与$sigmoid$输出相乘，这样我们仅仅输出我们想要输出的部分。还是对于之前提到的语言模型，因为它只看到了一个主语，它可能会输出一个与动词相关的信息。例如，可能输出这个主语是单数还是复数，所以我们会知道紧跟的动词是何种形式。至此，基本的LSTM介绍完毕。 LSTM的变体我们以上描述的均是最为普通的LSTM。但是并不是所有的LSTM都是以上那个样子。事实上，似乎每一篇涉及LSTMs的论文均对其做了细微的修改。其中的差别不大，以下列举几种LSTM的变体。 其中之一就是一种特别流行的LSTM变体，它由 Gers &amp; Schmidhuber (2000)提出，加入一种窥视孔连接（peephole connections）的结构。这使得细胞状态可以作为门层（译者：gete layers:$sigmoid$ layers &amp; $tanh$ layer）输入。以上的图示为每个门层加入了窥视孔连接，但是也有一些论文并不是所有的门层都加。 另外一种变体是加入了双遗忘门（coupled forget）以及输入门。我们同时考虑了何时遗忘以及应该加入何种新信息，而并非分别考虑。我们仅仅在我们需要就地输入信息的时候才会遗忘，同时我们仅仅在遗忘掉旧的信息的时候才会加入新的信息（译者：此时$f_t=0$，表示遗忘旧的细胞状态，同时加入新的输入$\tilde{C}_t$）。 另外一种改动较大的变体是门控循环单元（Gated Recurrent Unit）即GRU。这个算法由Cho,et al.(2014)提出，它把遗忘门和输入门结合起来构成一个“更新门”（update gate）。与此同时，它还将细胞状态和隐含状态合并起来，当然还有一些其他变化在此不一一赘述。最终的变体比标准的LSTM简单，这使得它很受欢迎。 以上均是最近比较劲爆的LSTM变体。当然也有很多其他形式的变体，如Yao,et al. (2015)提出的深度门RNN（Depth Gated RNNs）。还有一些变体用完全不同的方式来解决长期依赖问题，例如Koutnik,et al.(2014)提出的时钟频率驱动RNN（Clockwork RNNs ）。 列举了诸多LSTM变体，那么到底哪一种变体是最好的呢？其中的差异真的很重要吗？Greff, et al.(2015)做了一个非常棒了比较，发现这些变体几乎都是一样一样的。Jozefowicz,et al.(2015)测试了上万种RNN框架，发现了一些框架在特定任务上会比LSTM表现出色。（译者：没有一种算法一统江湖） 结论以上，我提到了人们利用RNNs得到了很多优秀的结果。在本质上说，几乎所有的RNNs都使用了LSTMs。LSTMs在诸多任务上表现优异。在介绍LSTMs的过程中写了很多公式，这让它看起来很吓人。幸运的是，我们在文中通过一步步地探索，让LSTM看起来平易近人。LSTMs是我们完成RNNs的重大成果。我们很自然地想：还有没有其他的重大成果？研究员们的共识是：当然有！下一个重大成果就是——注意力。这个观点是让RNNs的每一步都能够从更大的数据集中挑选信息。例如，如果你想利用RNNs去给一幅图像创造标题来描述它，这就可能会选择图像的一部分作为输入，然后根据这些输入来得到每个单词。事实上，Xu,et al.(2015)就是这么做的——这可能你探究注意力这个领域的起点。还有诸多使用注意力取得的令人激动的研究结果，看起来还有更多需要探索。注意力并非RNN唯一令人激动的研究方向。例如，Kalchbrenner, et al. (2015)提出的网格LSTM（Grid LSTMs）看似非常有前景。Gregor, et al.(2015)，Chung, et al.(2015)和 Bayer &amp; Osendorfer (2015)等人的研究工作是在生成模型中使用RNNs，这些工作都看起来非常有趣。过去几年是RNNs异常火爆的时期，未来也会有更多更加有意义的成果出现。 致谢我非常感谢那些帮助我去理解LSTMs的大佬们，同时感谢对可视化进行评论并在这篇博文提供反馈的网友们。非常感激谷歌同事们的反馈，尤其感谢Oriol Vinyals，Greg Corrado，Jon Shlens，Luke Vilnis以及Ilya Sutskever。我也由衷感谢很多同事朋友的帮助，包括Dario Amodei和Jacob Steinhardt。值得特别感谢还有Kyunghyun Cho，这哥们对图表的绘制给了我极大的帮助。在写这篇博文之前，我尝试在我讲授的神经网络课程中利用两系列研讨会的时间来解释LSTMs。感谢每一位参与者，感觉大家的反馈。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[降维之PCA主成分分析原理]]></title>
      <url>%2Fposts%2Fpca%2F</url>
      <content type="text"><![CDATA[背景在许多领域的研究与应用中，往往需要对反映事物的多个变量进行大量的观测，收集大量数据以便进行分析寻找规律。多变量大样本无疑会为研究和应用提供了丰富的信息，但也在一定程度上增加了数据采集的工作量，更重要的是在多数情况下，许多变量之间可能存在相关性，从而增加了问题分析的复杂性，同时对分析带来不便。如果分别对每个指标进行分析，分析往往是孤立的，而不是综合的。盲目减少指标会损失很多信息，容易产生错误的结论。因此需要找到一个合理的方法，在减少需要分析的指标同时，尽量减少原指标包含信息的损失，以达到对所收集数据进行全面分析的目的。由于各变量间存在一定的相关关系，因此有可能用较少的综合指标分别综合存在于各变量中的各类信息。主成分分析与因子分析就属于这类降维的方法。 目的PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。能够对高维数据降维的算法包括： LASSO 主成分分析法 聚类分析 小波分析法 线性判别法 拉普拉斯特征映射 降维有什么作用降维有什么作用呢？ 数据在低维下更容易处理、更容易使用 相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示 去除数据噪声 降低算法开销 常见的降维算法有主成分分析（principal component analysis,PCA）、因子分析（Factor Analysis）和独立成分分析（Independent Component Analysis，ICA）。 优化目标将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。注意：PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。 PCA原理 最大化样本点在基上的投影，使得数据点尽量的分离。令第一主成分的方向是u_1，我们的目标就是将样本点在该方向上的投影最大化，即： \max \frac{1}{n}\sum_{i=1}^n^2 \frac{1}{n}\sum_{i=1}^n \rightarrow \frac{1}{n}\sum_{i=1}^n(x_1^Tu_1)^2=\frac{1}{n}\sum_{i=1}^n(x_1^Tu_1)^T(x_1^Tu_1) =\frac{1}{n}\sum_{i=1}^n(u_1^Tx_1x_1^Tu_1)=\frac{1}{n}u_1^T\left(\sum_{i=1}^nx_1x_1^T\right)u_1=\frac{1}{n}u_1^T\left(XX^T\right)u_1其中的X=[x_1,x_2,...,x_n]^T,x_i\in R^{m}。那么优化函数就变成了： \max u_1^T\left(XX^T\right)u_1以上式子是个二次型，可以证明XX^T是半正定矩阵，所以上式必然有最大值。 \max u_1^T\left(XX^T\right)u_1=\max ||X^Tu_1||_2^2优化函数 max||Wx||_2 s.t. W^TW=I解释：最大化方差同时最小化协方差（PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”）。最大化方差意味着，使得每个样本点在每个维度上与均值有很大差异，就是说非常有个性，有个性才能分辨出来；同时协方差越小的话表明样本之间的互相影响就非常小，如果协方差是0的话，表示两个字段完全独立。 寻找协方差矩阵的特征向量和特征值就等价于拟合一条能保留最大方差的直线或主成分。因为特征向量追踪到了主成分的方向，而最大方差和协方差的轴线表明了数据最容易改变的方向。根据上述推导，我们发现达到优化目标就等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将特征值按大小从上到下排列。协方差矩阵作为实对称矩阵，其主要性质之一就是可以正交对角化，因此就一定可以分解为特征向量和特征值。 具体实施步骤总结一下PCA的算法步骤，设有m条n维(字段数)数据，我们采用以下步骤对数据降维。 将原始数据按列组成n行m列矩阵X. (行数代表字段数目，一个字段就是取每个样本的该维度的数值；列数代表样本数目) 将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值 求出协方差矩阵C=\frac{1}{m}XX^T 求出协方差矩阵的特征值及对应的特征向量 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P Y=PX即为降维到k维后的数据 去均值化的目的下面两幅图是数据做中心化（centering）前后的对比，可以看到其实就是一个平移的过程，平移后所有数据的中心是(0,0). 在做PCA的时候，我们需要找出矩阵的特征向量，也就是主成分（PC）。比如说找到的第一个特征向量是a = [1, 2]，a在坐标平面上就是从原点出发到点（1，2）的一个向量。如果没有对数据做中心化，那算出来的第一主成分的方向可能就不是一个可以“描述”（或者说“概括”）数据的方向了。还是看图比较清楚。 黑色线就是第一主成分的方向。只有中心化数据之后，计算得到的方向才能比较好的“概括”原来的数据。 限制PCA虽可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关。 参考 PCA的数学原理 K-L变换和PCA的区别 从PCA和SVD的关系拾遗 数据什么时候需要做中心化和标准化处理 主成分分析（PCA）原理详解 附上最近比较火的一首歌Time]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SIFT和SURF特性提取总结]]></title>
      <url>%2Fposts%2FSIFT-and-SURF%2F</url>
      <content type="text"><![CDATA[SIFT（Scale-invariant feature transform）是一种检测局部特征的算法，该算法通过求一幅图中的特征点（interest points,or corner points）及其有关scale 和 orientation 的描述子得到特征并进行图像特征点匹配 什么是SIFT先看看上图利用sift进行匹配的结果： 这个结果应该可以很好的解释sift的尺度、旋转以及光照不变性。接下来就介绍一下这个神奇的算法的奥义。我把代码放在了Github，感兴趣的同学自己下载下来试试。 算法描述SIFT特征具有尺度不变性，旋转不变性，光照不变性。 实现流程构建尺度空间尺度空间的目的是模拟图像的多尺度特性。高斯卷积核是实现尺度变换的唯一线性核，于是 一副二维图像的尺度空间定义为： L(x,y,\sigma)=G(x,y,\sigma)*I(x,y)其中的G(x,y,\sigma)是尺度可以发生变化的高斯函数G(x,y,\sigma)=\frac{1}{2\pi{\sigma}^2}e^{-\frac{x^2+y^2}{2{\sigma}^2}}。(x,y)表示空间坐标，\sigma是尺度系数，描述了图像的模糊程度。为了能够更为有效的提取出特征点，提出了DOG（高斯差分尺度空间）的概念。通过不同尺度下的高斯差分核与图像卷积形成： D(x,y,\sigma)=(G(x,y,k\sigma)-G(x,y,\sigma))*I(x,y) =L(x,y,k\sigma)-L(x,y,\sigma) 图像金字塔的建立：为了实现尺度不变特性，对于每一幅图像I(x,y)，分成子八度（octave），第一个子八度的scale为原图大小，后面每个octave为上一个octave降采样的结果，即原图size的1/4（长宽分别减半），构成下一个子八度（高一层金字塔）。此时要强烈注意size和尺度空间的概念。size是图像大小，而尺度空间表示不同\sigma的图像的集合。那么尺度空间的集合是： 2^{i-1}(\sigma, k*\sigma,k^2*\sigma,k^3*\sigma,...,k^{n-1}*\sigma)其中的 k=2^{1/S}，S表示尺度金字塔每个octave的层数，n表示尺度金字塔的总层数，i表示的是在某个octave的第i层，i\in[1,2,3,...n]。 由图片size决定建几个塔，每塔几层图像(S一般为3-5层)。0塔的第0层是原始图像(或你double后的图像)，往上每一层是对其下一层进行Laplacian变换（高斯卷积，其中σ值渐大，例如可以是σ, k*σ, k*k*σ…），直观上看来越往上图片越模糊。塔间的图片是降采样关系，例如1塔的第0层可以由0塔的第3层down sample得到，然后进行与0塔类似的高斯卷积操作。 在DoG空间找到关键点为了寻找尺度空间的极值点，每一个采样点要和它所有的相邻点比较，看其是否比它的图像域和尺度域的相邻点大或者小。如图所示，中间的检测点和它同尺度的8个相邻点和上下相邻尺度对应的9×2个点共26个点比较，以确保在尺度空间和二维图像空间都检测到极值点。 一个点如果在DOG尺度空间本层以及上下两层的26个领域中是最大或最小值时，就认为该点是图像在该尺度下的一个特征点,如图所示。使用Laplacian of Gaussian能够很好地找到找到图像中的兴趣点，但是需要大量的计算量，所以使用Difference of Gaussian图像的极大极小值近似寻找特征点.DOG算子计算简单，是尺度归一化的LoG算子的近似。 去除不好的点 这一步本质上要去掉DoG局部曲率非常不对称的像素。通过拟和三维二次函数以精确确定关键点的位置和尺度（达到亚像素精度），同时去除低对比度的关键点和不稳定的边缘响应点(因为DoG算子会产生较强的边缘响应)，以增强匹配稳定性、提高抗噪声能力，在这里使用近似Harris Corner检测器。 给特征点赋值一个128维方向参数并描述前面的几个步骤确定了特征点到底在哪里，此步骤是为了描述特征点。(x,y)处梯度的模值和方向公式为： m(x,y)=\sqrt{(L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2} \theta(x,y)=tan^{-1}\left(\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}\right) 利用关键点邻域像素的梯度方向分布特性为每个关键点指定方向参数，使算子具备旋转不变性。 其中L所用的尺度为每个关键点各自所在的尺度。至此，图像的关键点已经检测完毕，每个关键点有三个信息：位置，所处尺度、方向，由此可以确定一个SIFT特征区域。 在实际计算时，我们在以关键点为中心的邻域窗口内采样，并用直方图统计邻域像素的梯度方向。梯度直方图的范围是0～360度，其中每45度一个柱，总共8个柱, 或者每10度一个柱，总共36个柱。Lowe论文中还提到要使用高斯函数对直方图进行平滑，减少突变的影响。直方图的峰值则代表了该关键点处邻域梯度的主方向，即作为该关键点的方向。直方图中的峰值就是主方向，其他的达到最大值80%的方向可作为辅助方向。 计算keypoint周围的16*16的window中每一个像素的梯度，而且使用高斯下降函数降低远离中心的权重。图左部分的中央为当前关键点的位置，每个小格代表关键点邻域所在尺度空间的一个像素，利用公式求得每个像素的梯度幅值与梯度方向，箭头方向代表该像素的梯度方向，箭头长度代表梯度模值，然后用高斯窗口对其进行加权运算。 该图是8*8的区域计算得到2*2描述子向量的过程。但是在实际中使用的是在16*16的区域计算得到4*4的特征描述子，如下图： 这样就可以对每个feature形成一个4*4*8=128维的描述子，每一维都可以表示4*4个格子中一个的scale/orientation。将这个向量归一化之后，就进一步去除了光照的影响。 sift的缺点SIFT在图像的不变特征提取方面拥有无与伦比的优势，但并不完美，仍然存在： 实时性不高。 有时特征点较少。 对边缘光滑的目标无法准确提取特征点。 PS: 论文见这里：Distinctive Image Features from Scale-Invariant Keypoints，这里是David Lowe大神做的一个Demo Software: SIFT Keypoint Detector. SURF 简介参考了好友整理的一篇文章特征与匹配 整体的思路就是将计算DOG的一整套东西来检测关键点的理论替换成了利用hessian矩阵来检测关键点，因为当Hessian矩阵的判别式取得局部极大值时，判定当前点是比周围邻域内其他点更亮或更暗的点，由此来定位关键点的位置。上述过程要进行Hessian判别式的计算，可以采用box filter的方式进行加速。 构建尺度金字塔的方式不同，具体见下图： Sift特征点方向分配是采用在特征点邻域内统计其梯度直方图，取直方图bin值最大的以及超过最大bin值80%的那些方向作为特征点的主方向。而在Surf中，采用的是统计特征点圆形邻域内的harr小波特征。即在特征点的圆形邻域内，统计60度扇形内所有点的水平、垂直harr小波特征总和，然后扇形以0.2弧度大小的间隔进行旋转并再次统计该区域内harr小波特征值之后，最后将值最大的那个扇形的方向作为该特征点的主方向。该过程示意图如下： 生成特征点描述子: 在Sift中，是取特征点周围4*4个区域块，统计每小块内8个梯度方向，用着4*4*8=128维向量作为Sift特征的描述子。surf算法中，也是在特征点周围取一个4*4的矩形区域块，但是所取得矩形区域方向是沿着特征点的主方向。每个子区域统计25个像素的水平方向和垂直方向的haar小波特征，这里的水平和垂直方向都是相对主方向而言的。该haar小波特征为水平方向值之后、垂直方向值之后、水平方向绝对值之后以及垂直方向绝对值之和4个方向。把这4个值作为每个子块区域的特征向量，所以一共有4*4*4=64维向量作为Surf特征的描述子，比Sift特征的描述子减少了2倍。 参考 SIFT特征提取分析 特征匹配-SURF原理与源码解析（一） 特征与匹配]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[统计学习方法总结]]></title>
      <url>%2Fposts%2Fsummary-statistical-learning%2F</url>
      <content type="text"><![CDATA[本文主要研究监督学习，所谓的监督学习就是在给定的，有限的，用于学习的训练数据集合（training data）出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个集合，即假设空间；我们根据一定的评价准则，从假设空间中选取一个最优的模型，使它对已知的训练数据以及未知的测试数据在给定评价准则下有最优的预测，最优模型的选取由算法实现。所以统计学习方法有三个要素：模型、策略、算法。 统计学习 监督学习 半监督学习 无监督学习 强化学习 输入空间、特征空间与输出空间 输入变量&amp;输出变量均连续-&gt; 回归问题 输出空间为有限个离散变量的预测问题-&gt; 分类问题 输入与输出均为变量序列的预测问题-&gt; 标注问题 风险函数 期望风险：模型关于联合分布的期望损失 经验风险：模型关于训练样本的平均损失按照大数定律，当样本数据量区域无穷时，经验风险趋近于期望风险；但是当样本容量很小时，经验风险的效果就不会太好，此时容易出现过拟合现象。此时，结构风险就被提出。结构风险是在经验风险的基础上添加上表示模型复杂度的正则化项/罚项。极大似然估计是经验风险最小化的一个特例。最大后验概率估计是结构风险最小化的一个特例； 模型监督学习里要学习的模型就是决策函数或者条件概率分布。 此时不得不提到生成方法以及判别方法。 生成方法，由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布模型，即生成模型： P(Y|X)=\frac{P(X,Y)}{P(X)} 判别方法是由数据直接学习决策函数或者条件概率分布作为预测模型，即判别模型。 生成模型与判别模型 生成模型常见的主要有： 高斯混合模型 朴素贝叶斯 混合高斯模型GMM 隐马尔可夫模型HMM 马尔可夫的随机场 KNN 常见的判别模型有： 支持向量机 传统的神经网络 线性判别分析 线性回归 条件随机场 最大熵模型 逻辑斯特回归 策略指定策略的目的就是为了挑选出假设空间中到底哪个模型才是我们真正需要的。此时会用到损失函数以及风险函数的概念。 0-1 损失函数 L(Y,f(X))=\left\{\begin{aligned} 1, && Y \neq f(X)\\ 0, &&Y = f(X) \end{aligned}\right. 平法损失函数 L(Y,f(X))=(Y-f(X)^2 绝对值损失函数 L(Y,f(X))=|Y-f(X)| 对数损失函数 L(Y,P(Y|X))=-logP(Y|X) 损失函数越小的话代表模型越好。$(X,Y)$是随机变量符合联合分布概率$P(X,Y)$，所以损失函数的期望被定义为： R_{ref}(f)=E_p[L(Y,f(X))]=\int_{\mathcal{X} \times \mathcal{Y}}L(y,f(x))P(x,y)dxdy以上是模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为风险函数或者期望损失（expected loss）。但是呢，期望损失不易求解，我们一般用模型关于训练数据集的平均损失来逼近期望损失，即： R_{emp}(f)=E_p[L(Y,f(X))]=\frac{1}{N} \sum_{i=1}^NL(y_i,f(x_i))经验风险最小化的策略认为，经验风险最小的模型就是最优的模型，于是按照这种定义，我们有： f^*={argmin}_{f \in \mathcal{F} } R_{emp}其中的$\mathcal{F}$是假设空间。最大似然估计就是经验风险最小化的一个例子：当模型为条件概率，损失函数是对数损失时，经验风险最小化就等价于极大似然估计。根据大数定理可知，当样本容量N趋近于无限时，经验风险趋近于期望风险。但是如果样本数量是有限时，此时会出现过拟合现象，那么这时候需要结构风险的帮助。结构风险是为了防止过拟合而提出的策略，结构风险最小化等价于正则化（regularization）。其定义是 R_{srm}(f)=\frac{1}{N} \sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)其中的$J(f)$是衡量模型复杂度的项，也叫罚项。当模型越复杂时，$J(f)$越大；模型越简单时，$J(f)$越小。最大后验概率估计（MAP）就是结构风险最小化的一个例子：当模型时条件概率，损失函数是对数损失函数，模型复杂度由先验概率表示时，结构风险最小化就等价于MAP。 监督学习的模型可以分为概率模型与非概率模型，由条件概率分布$P(Y|X)$或者决策函数$Y=f(X)$表示。 算法算法是指学习模型的具体计算方法： find global optimization solution efficiently. 模型选择模型选择的主要方式有：正则化与交叉验证。 在经验风险最小化时已经了解到正则化的由来，正则化就是针对过拟合现象提出的解决策略。 过拟合是指学习模型时选择的模型包含的参数过多，以致于这一模型对已知数据的预测很好，而对未知数据的预测能力变得很差。 以下介绍两种正则化的范数：$L_1$ and $L_2$ 正则化L2范数正则 C=C_0+ \frac{\alpha}{2n}\sum_ww^2对$w$以及$b$求导如下： \frac{\partial C}{\partial w}= \frac{\partial C_0}{\partial w}+\frac{\lambda}{n}w \frac{\partial C}{\partial b}= \frac{\partial C_0}{\partial b}由梯度下降法可知： w \rightarrow w-\eta\frac{\partial C}{\partial w}=\left(1-\frac{\eta\lambda}{n} \right)w-\eta\frac{\partial C_0}{\partial w}系数$\left(1-\frac{\eta\lambda}{n} \right)$是小于1的，相比于原来的系数等于一，此时的效果相当于就是权值衰减（weight decay）。 注意到过拟合的时候，我们的假设函数要顾及到每一个数据点，模型就会尝试对所有数据点进行拟合，包括一些异常点；此时形成的拟合函数的波动性会非常大，可以看到此时的拟合参数会异常大。通过L2正则化处理可以使这些参数变小，解释如下： 更小的权重，表示网络的复杂组更低，对数据拟合的刚刚好（奥卡姆剃须刀原理） L1范数正则L1正则假设参数的先验分布是Laplace分布，可以保证模型的稀疏性，也就是某些参数等于0；L2正则假设参数的先验分布是Gaussian分布，可以保证模型的稳定性，也就是参数的值不会太大或太小 参考链接lasso and ridge regularization 交叉验证就是将整个数据集切分成三个部分，分别是训练集、验证集和测试集。训练集用来训练模型，验证集用于模型 的选择，而测试集用于对学习方法的评估。 但是一般情况下，训练数据是不足的，那么此时可以重复的利用数据，进行反复训练以得到最优模型。 常见的方法有： S折交叉验证（S-fold cross validation） 留一交叉验证（S=N，当S为数据集的容量时，S折交叉验证就变成了留一交叉验证） 极大似然估计最大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知” 举个别人博客中的例子，假如有一个罐子，里面有黑白两种颜色的球，数目多少不知，两种颜色的比例也不知。我 们想知道罐中白球和黑球的比例，但我们不能把罐中的球全部拿出来数。现在我们可以每次任意从已经摇匀的罐中拿一个球出来，记录球的颜色，然后把拿出来的球 再放回罐中。这个过程可以重复，我们可以用记录的球的颜色来估计罐中黑白球的比例。假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例最有可能是多少？很多人马上就有答案了：70%。而其后的理论支撑是什么呢？ 我们假设罐中白球的比例是$p$，那么黑球的比例就是$1-p$。因为每抽一个球出来，在记录颜色之后，我们把抽出的球放回了罐中并摇匀，所以每次抽出来的球的颜 色服从同一独立分布。这里我们把一次抽出来球的颜色称为一次抽样。题目中在一百次抽样中，七十次是白球的概率$P(Data|M)$，这里Data是所有的数据，M是所给出的模型，表示每次抽出来的球是白色的概率为$p$。如果第一抽样的结果记为x_1，第二抽样的结果记为x_2… 那么Data = (x_1,x_2,...,x_{100})。这样， P(Data|M)= P(x_1,x_2,...,x_{100}|M)= P(x_1|M)P(x_2|M)...P(x_{100}|M)= p^{70}(1-p)^{30}那么p在取什么值的时候，$P(Data|M)$的值最大呢？将$p^{70}(1-p)^{30}$对$p$求导，并其等于零。 70p^{69}(1-p)^{30}-p^{70}*30(1-p)^{29}=0解方程可以得到p=0.7。 最大熵原理最大熵原理使概率学习中的一个准则。学习概率模型时，在所有的可能概率模型（分布）中，熵最大的模型是最好的模型。最大熵原理也可以理解成在满足约束条件的模型中选择熵最大的模型！ H(p)=-\sum_x{log(P(x))P(x)}其中$X$服从的概率分布为$P(X)$，$X$服从均匀分布时，熵最大。当没有其他已知的约束时，$\Sigma{P(x)}=1$，此时按照最大熵原理，当$P(x_1)=P(x_2)=…=P(x_n)$时，熵最大；此时等概论，等概论意味着对事实的无知，因为没有更多可能的信息，所以此时的判断也是合情合理的。 线性分类器线性分类器有三大类：感知器准则函数、$SVM$、$Fisher$准则，而贝叶斯分类器不是线性分类器。 感知器准则函数：代价函数$J=-(W*X+w_0)$，分类的准则是最小化代价函数。感知器是神经网络（$NN$）的基础，网上有很多介绍。 $SVM$：支持向量机也是很经典的算法，优化目标是最大化间隔（margin），又称最大间隔分类器，是一种典型的线性分类器。（使用核函数可解决非线性问题） $Fisher$准则：更广泛的称呼是线性判别分析（$LDA$），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。 评价指标 召回率就是有多少正确的被你找回来了；准确率就是你找到的有多少是正确的；(一般情况下召回率和准确率呈负相关，所以可以用F值衡量整体效果) TP(True Positive)是你判断为正确的，实际就是正确的； FP(False Positive)是你判断是错误的，实际也是错误的； TN(True Negative)是你判断为正确的，但实际是错误的； FN(False Negative)是你判断是错误的，但实际是正确的； 朴素贝叶斯 Naive Bayes基本方法朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类方法。NB的核心在于它假设向量的所有分量之间是独立的。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分。对于由$P(X,Y)$独立产生的训练集$T={(x_1,y_1),(x_2,y_2),…, (x_N,y_N),}$而言通过朴素贝叶斯的方法学习这个联合概率分布。大致分两步： 计算先验分布： P(Y=c_k),k=1,2...,K 条件概率分布： P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_k),k=1,2...,K其中的$x \in R^{n}$，$n$表示这个样本的维度。 但是在计算条件概率时因为朴素贝叶斯做了条件独立性假设，那么该条件概率分布可以写成： P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)},...,X^{(n)}=x^{(n)}|Y=c_k) \\ =\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)在实际分类时，对于给定的输入x，通过学习到的模型估计后验概率$P(Y=c_k|X=x)$将后验概率最大的类作为x的类的输出。 P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_{k}P(X=x|Y=c_k)P(Y=c_k)}\ =\frac{P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_{k}P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}因为分母就是$P(X=x)$的概率，这是不变的。因此我们仅需要知道分子哪个大就可以，也就是： y={argmax}_{c_k} P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)上式的意思是求解到底是哪些类别c_k能够让最大后验概率最大化。 参数估计极大似然估计 计算先验概率 P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N} 计算条件概率 P(X=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}其中 j=1,2...n;l=1,2...S_j;k=1,2...K 对于给定的实例x，计算 P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k) 确定实例的类别]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu上使用Git以及GitHub]]></title>
      <url>%2Fposts%2Fgit-notebook%2F</url>
      <content type="text"><![CDATA[安装Git在Ubuntu上安装Git的命令为: 1sudo apt-get install git 配置GitHub安装git结束之后就是配置github用户资料，如下：将其中的 “user_name”替换成自己 GitHub的用户名并且将”email_id” 换成你创建GitHub账号所用的邮箱.123git config --global user.name &quot;user_name&quot;git config --global user.email &quot;email_id&quot; 建立本地仓库（repository）在自己的电脑上建立本地仓库，这个仓库将会在后续推送到GitHub上，语句如下：1git init Mytest 如果初始化成功，你将会得到以下提示： 1Initialized empty Git repository in /home/user_name/Mytest/.git/ 其中的user_name为本地计算机用户名，因人而异。Mytest是”init”创建的文件夹，然后进入该文件所在目录：1cd Mytest 创建README来描述这个仓库该步骤可有可无，但是作为一个优秀的工程师还是写点东西比较好。 1gedit README 然后输入：1This is a git repo 将仓库文件加入index（缓存）这一步尤其重要，我们将需要上载到GitHub的文件们添加到index。这些文件可以是文本文档，m/c/c++/PDF/jpg…几乎任何类型文件，一般而言我们可以把需要上载的文件拷贝到这个文件夹内，然后再用一个语句来把需要上传到文件添加到index，如下：1234git add file1.txtgit add file2.cgit and file3.m... 提交到本地仓库当我们已经把文件添加／修改到index后，就可以进行提交了；利用如下语句： 1git commit -m &quot;some_message&quot; 其中some_message可以是任何字符，例如：”my first commit” 或者”edit in readme”等。上面代码的-m参数，就是用来指定这个mesage 的。 注意：git是分为三部分，一部分是文件，另外一个是缓存区，最后一个是本地库。当你修改了自己的文件后，你会git add xx将修改保存到缓存区，然后再用commit推送修改到本地库中。git push 将本地仓库修改推送到服务器上的仓库中commit是将本地修改保存到本地仓库中。 在GitHub创建仓库这个仓库的名字要和本地的一致，该部分不做展开。然后连接到远程仓库 1git remote add origin https://github.com/user_name/Mytest.git 其中user_name就是自己的GitHub的用户名。 推送到远程仓库最后的一步就是提交到远程仓库，用以下命令：1git push origin master 原文地址]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习修炼手册]]></title>
      <url>%2Fposts%2Fmachine-learning%2F</url>
      <content type="text"><![CDATA[对机器学习的学习我开始于二年级的《数据挖掘》课，当时袁老师对数据挖掘中的常用的算法做了一些介绍，但是这仅仅是个入门教学，我并没有深入了解的其中的原理。到现在我才深刻的意识到ML的重要性，我就抽空看了一些这方面的资料，整理了这一份文档。 机器学习算法包括，监督学习(回归、分类)以及非监督学习(聚类)。 梯度下降\bbox[5px,border:2px solid red] { \theta_j:=\theta_j-\alpha\frac{\partial}{\partial \theta}J(\theta) }其中$\alpha$为学习率一般为很小的数字(0.001-0.1)，$\theta$为我们需要求解的参数，$J(\theta)$为能量函数或者为损失函数，通过上述公式可知，梯度下降是沿着损失函数梯度的反方向寻找迭代寻找最优值的过程。梯度下降容易陷入局部最极小点，所以我们要采取一定的措施来阻止这种现象的发生。 过拟合（Overfitting）如果训练样本的特征过多，我们学习的假设可能在训练集上表现地很好，但是在验证集上表现地就不尽人意 避免过拟合 减少特征的大小 正则化 在保证所有特征都保留的情况下，限制$\theta$的大小，即Small values for parameters $ \theta_0,\theta_1,\theta_2…\theta_n$ 当特征量很多时，该方式仍然表现的很好 交叉验证(Cross Validation) 正则化线性回归对于线性回归而言，其损失函数形式如下： J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^2引入正则化之后的损失函数的形式为： J(\theta)=\frac{1}{2m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_{j}^2\right)GD迭代求解参数Repeat{ \theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)x_0^{(i)} \theta_j:=\theta_j-\alpha\frac{1}{m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\theta_j\right)}梯度下降法的学习率$\alpha$需要提前指定，并且还要制定收敛标准。 Normal Equation \theta=\left(x^Tx+\lambda\begin{bmatrix} {0}&{0}&{\cdots}&{0}\\ {0}&{1}&{\cdots}&{0}\\ {\vdots}&{\vdots}&{\ddots}&{\vdots}\\ {0}&{0}&{\cdots}&{1}\\ \end{bmatrix}_{(n+1)(n+1)}\right)^{-1}x^Ty上式是对线性回归正则化后的矩阵解。可以证明的是当$\lambda&gt;0$时，求逆符号内部的式子总是可逆的。 逻辑回归在没有加入正则化之前，逻辑回归的损失函数的形式是这样的： J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})\log\left(1-h_{\theta}(x^{(i)})\right)\right)加入正则项之后的形式为： J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\left(y^{(i)}\log\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})\log\left(1-h_{\theta}(x^{(i)})\right)\right)+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2GD迭代求解参数Repeat{ \theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)x_0^{(i)}\theta_j:=\theta_j-\alpha\frac{1}{m}\left(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\lambda\theta_j\right)} SVM支持向量机支持向量机又被称作最大间距（Large Margin）分类器，损失函数的形式是： J(\theta)=C\sum_{i=1}^{m}\left(y^{(i)}cost_1\left(h_{\theta}(x^{(i)})\right)+(1-y^{(i)})cost_0\left(h_{\theta}(x^{(i)})\right)\right)+\frac{1}{2}\sum_{j=1}^{n}\theta_j^2其中：$h_{\theta}(x^{(i)})=\theta^Tx^{i}$，$cost_1$以及$cost_0$的形式如下图所示： \begin{cases} \text{we want } \theta^Tx\ge1, & \text{if $y$ =1} \\[2ex] \text{we want } \theta^Tx\le-1, & \text{if $y$ =0} \end{cases}在考虑到soft margin时的损失函数是hinge损失，SVM就等价于Hinge损失函数+L2正则。此时损失函数为0时候就对应着非支持向量样本的作用，“从而所有的普通样本都不参与最终超平面的决定，这才是支持向量机最大的优势所在，对训练样本数目的依赖大大减少，而且提高了训练效率”。以下是七月在线大神July写的一篇关于SVM的介绍，个人觉得不错。分享下：支持向量机通俗导论（理解SVM的三层境界）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux指令学习笔记]]></title>
      <url>%2Fposts%2FLinux-commands%2F</url>
      <content type="text"><![CDATA[今天看了基本的Linux指令以及makefile的写法，大体总结了一下。 常用指令及意义 root 表示根目录 cd path 切换到path目录， cd / 切换到根目录 cat file.txt 查看file.txt中的内容 pwd查看当前所在目录 rmdir删除目录 rm 删除文件 ls 列出文件名字， ls -l 列出文件列表 cp 复制文件 cp file1.txt file2.txt (复制file1并重命名为file2) head file.txt -n 7 查看file.txt的头7行 tail file.txt -n 7 查看file.txt的末尾7行 wc file.txt 统计file.txt文件中字符数，返回3个数字：row_size, word_number, character_number; -l 统计行数。即 wc -l file.txt-m 统计字符数。这个标志不能与 -c 标志一起使用。-w 统计字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。-L 打印最长行的长度。 mv 命令有2中功能 移动文件夹：mv file.txt file 修改文件名: mv file1.txt file2.txt chmod 修改用户权限，有3种用户，分别是： 解释 u g o 用户 user 作者 group小组 other其他 读写运行 r w x r w x r w x 二进制 1 0 0 1 0 0 1 0 0 十进制 4 4 4 假如取消作者的写的权限则：chmod u-w file.txt，其中减号表示“去除”假如添加作者的写的权限则：chmod u+w file.txt，其中减号表示“添加”假如除了作者以外的人都没有读写权限：chmod go-r file.txt同样可以用二进制设置权限 chmod 444 file.txt，表示：u,g,o都只有读的权限。 远程连接服务器1ssh username@server_IP_address 之后会提示输入密码。 远程拷贝文件和cp一样，我们改用scp。在本机键入如下命令，其中file是本地的文件，后面的一长串是目标主机的以及其相应目录。 1scp -f file username@serverIP:/yourfolder 监视GPU状态1watch -n 0.2 nvidia-smi 无法连接远程服务器首先明确两点： make sure you have connected the internet. make sure you have installed openssh-server, if not, you can install it by using the following comands: 12sudo apt-get updatesudo apt-get install openssh-server 脚本语言就是将Linux命令集中在一起，组成一个文件，例如有一个test.sh脚本文件123lsdatecal 后缀名 .sh 运行 sh test.sh 变量赋值不用加空格, b=1, a=$b 输出 echo 字符串写不写双引号一样效果，但是还是推荐写双引号 大于号 -gt 小于号 -lt 等于号 -eq 大于等于 -ge 小于等于 -le 不等于 -ne 判断语句123456if [ expr ]then...else...fi 循环语句1234for x in 1 2 3 4 5 6do echo $xdone 数组例如：arr=(1 2 3 5 6 3 4)注意在运行时候不能继续用sh (1979脚本)；应该改用bash (后期脚本，有数组的时候就用bash)。 12345678910arr=(1 2 5 9 8 6 5 4 3 2)max=$&#123;arr[0]&#125;for i = $&#123;arr[@]&#125;do if [ $i -gt $max ] then max=$i fidoneecho max= $max 全局变量 $USER $HOME 可以用 ~ 代替 环境变量 $PATH,将一个路径加入这个全局变量:PATH=$PATH:/home/vincent/tutorial (注意所有的路径都是用冒号间隔开的，) 常用指令压缩 把几个文件打包成file.zip, zip file.zip * (星号的意思是打包所有的文档) 把全部的子文件夹都递归打包 zip file.zip -r files/* 利用tar命令： tar -zcvf file.tar.gz files/ 解压 利用tar命令： tar -zxvf file.tar.gz 下载命令 wget 下载后重命名 wget web_address -O file.tar.gz 注意用-O makefile的写法当编译代码的时候，如果有很多子文件，gcc 后面的语句非常长，我们可以选择使用makefile来对其进行处理以加速编译速度并加强可读性。基本的语句是如下所示的格式：12Target: dependencies(tab键) command 其中Target表示目标，例如最后想把所有的.c .o 文件们打包成main,那么Target就是main dependencies表示依赖项们，即所有的.c .h .o command为命令即如：gcc test.c -o test 例如我们想把tool.c 和main.c 打包成main.o的目标文件, 则makefile的写法为：12main: main.c tool.o gcc main.c tool.o -o main 但是我们发现并没有tool.o文件所以，还要把tool.o怎么来的说明一下：12tool.o: tool.c gcc -c tool.c 注意：gcc -c表示直接把tool.c编译成目标文件tool.o最后呢，如果代码开源的话一般不需要保留.o文件以及main，所以最后还需要把所有的.o以及main文件删除，我们需要在makefile文件的最后添加如下代码：12clean: rm *.o main 最后这个makefile可以写成：123456main: main.c tool.o gcc main.c tool.o -o maintool.o: tool.c gcc -c tool.cclean: rm *.o main 如果编译器不是gcc，而是其他的编译器，这时候我们有必要做一下代换来提高代码的可移植性。令：CC=gcc， 在引用的时候 $(CC)1234567CC=gccmain: main.c tool.o $(CC) main.c tool.o -o maintool.o: tool.c $(CC) -c tool.cclean: rm *.o main 例如有test1.c，test2.c，它们分别实现了查找最大值和最小值的功能；然后我们把这两个函数的声明分别放在Max.h和Min.h里面，最后我们在主函数main.c里面包含这两个头文件，然后调用这个两个求最大最小值的函数。 123456789CC=gccmain: main.c Max.o Min.o $(CC) main.c Max.o Min.o -o mainMax.o: test1.c $(CC) test1.c -o MaxMin.o: test2.c $(CC) test2.c -o Minclean: rm *.o main 如果还有第三方的库文件，我们如何链接呢？12345678910CC=gccCFLAGS=-lm -Wall -gmain: main.c Max.o Min.o $(CC) $(CFLAGS) main.c Max.o Min.o -o mainMax.o: test1.c $(CC) $(CFLAGS) test1.c -o MaxMin.o: test2.c $(CC) $(CFLAGS) test2.c -o Minclean: rm *.o main 未完待续，需要学习的知识太多了，深深地感觉到了自己的无知:(]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[初试HCI光场数据集]]></title>
      <url>%2Fposts%2Fnew-hci-lightfield-datasets%2F</url>
      <content type="text"><![CDATA[好的数据集是做出漂亮实验的必要条件.声明：一切理解都是本人观点，如有疑问，还望在评论中留言。如需转载请与本人联系，谢谢合作! 邮箱：点我 Wanner光场数据集目前光场数据集有如下几种主流的数据集， 斯坦福大学光场数据集； Wanner(HCI)数据集(Old 4D Light Field Benchmark)； 4D Light Field Dataset(Konstanz大学与Heidelberg大学的HCI合作,New 4D Light Field Benchmark)。 下面对Wanner数据集进行讨论。学习光场的同学应该很熟悉Wanner提供的数据集共有10个场景，分别是： Buddha Buddha2 Couple Cube Mona Medieval Papillon StillLife Horses rx_watch 其中，1-8为仿真场景，9-10是由Raytrix拍摄的场景。他们的文件后缀为 .h5, 格式是HDF5，这是一种文件组织格式，可以很好的将数据组织在一起，具体不做展开。MATLAB 提供了一系列相应的读取该文件的函数，如：h5disp，hdf5info(新版本用h5info)，hdf5read等函数，如利用h5disp就可以得到HDF5文件的内容信息，如下图： 以下给出解码HDF5文件得到子孔径图像以及重排图像的代码： 123456789101112131415161718192021222324252627input_file = 'Buddha2.h5'; % file nameinput_folder = ''; % your datasets folder[pathstr,name,ext] = fileparts([input_folder '/' input_file]);file_path=[pathstr,name,ext];hinfo_data = hdf5info(file_path);if strcmp(file_path,'Cube') || strcmp(file_path,'Couple') data = hdf5read(hinfo_data.GroupHierarchy.Datasets(3));else data = hdf5read(hinfo_data.GroupHierarchy.Datasets(2));enddata = permute(data, [3 2 1 5 4]);data = im2double(data(:, :, :, :, end:-1:1));% parameters from inputUV_diameter = size(data, 4); % angular resolutionUV_radius = floor(UV_diameter/2); % half angular resolutionh = size(data, 1); % spatial image heightw = size(data, 2); % spatial image widthy_size=h;x_size=w;UV_size=UV_diameter^2;LF_y_size = h * UV_diameter; % total image heightLF_x_size = w * UV_diameter; % total image widthLF_Remap = reshape(permute(data, ... [4 1 5 2 3]), [LF_y_size LF_x_size 3]); % the remap imageIM_Pinhole = data(:,:,:,UV_radius+1,UV_radius+1); % the pinhole image 经过以上步骤可以得到相应的中心视角图像以及Remap（重排）之后的图像，从而进一步方便接下来的工作，例如基于该数据集的深度图像估计算法估计。 HCI 4D光场数据集(4D Light Field Benchmark)The 4D Light Field Benchmark was jointly created by the University of Konstanz and the HCI at Heidelberg University. 上周整理上一篇博客的时候，想再次查看HCI数据集是否更新，结果惊喜地看到它竟然更新了！激动之余，就连夜把数据以及代码下载了下来，看看这个数据集的庐山真面目。 数据集概况这个数据集共有4大类： Stratified（4） training（4） test（4） additional（16） ​ 总结而言这个4D光场数据集提供了如下信息： 9x9x512x512x3 light fields as individual PNGs（角度分辨率：9×9，空间分辨率：512×512） Config files with camera settings and disparity ranges（相机配置文件以及视差范围） Per center view (except for the 4 test scenes):（除了测试类外每类的中心视角图像） 512×512 and 5120×5120 depth and disparity maps as PFMs（深度图像以及视差图：512×512低分辨率，以及5120×5120高分辨率） 512×512 and 5120×5120 evaluation masks as PNGs（png格式的评价掩膜：512×512低分辨率，以及5120×5120高分辨率） 16组additional的每个视角的Ground Truth Depth图像（pfm格式） 数据集下载开始下载吧！在该页面的get the data处填写自己的邮箱，然后点击request download links。接下来你的邮箱里就会出现这个数据集的下载链接，链接有点多，你可以选择性的下载或者全部下载。方便起见，我把邮件中提供的链接贴在了下面： Benchmark package with the 12 benchmark scenes Full package with all 28 scenes(这是全部的场景，共28类；注意不包含深度图像) Packages per category: stratified test training additional Stratified scenes: backgammon dots pyramids stripes Test scenes: bedroom bicycle herbs origami Training scenes: boxes cotton dino sideboard Additional scenes: antinous boardgames dishes greek kitchen medieval2 museum pens pillows platonic rosemary table tomb tower town vinyl Depth and disparity maps for all views of the additional scenes 数据集初体验测试代码下载在其官方给出的代码页面下载测试程序，下载完毕后将convert*.m以及lib文件夹其放置在与上述数据集同级目录。例如：TEST目录下同时包括：convert.m 以及 lib/， 同样也包含 additional/, stratified/, test/, 以及 training/。 生成LF.mat convertAll. 对于每一个场景都声称一个LF.mat文件 如果我们仅仅下载了几个场景我们可以利用如下函数得到相应的LF.mat convertBlenderTo5D(‘FOLDER’) 这个LF.mat中包含该场景的光场信息诸如： 光场数据 (LF.LF) 真实值 (LF.depth/disp_high/lowres) 评价掩膜（mask） 中心视角图像 注意：生成LF.mat的过程用到的参数通过加载相应文件夹下parameters.cfg得到，并将其存储在了LF.parameters中；H变换矩阵存储在了LF.H中（可以参考论文“Decoding, Calibration and Rectification for Lenselet-Based Plenoptic Cameras” ）；两个平面的距离存储在LF.f, 单位 [mm]； 相机焦距：LF.parameters.intrinsics.focal_length_mm. 生成点云（Point Cloud）接下来我以additional文件下的antinous为例子展示如何利用深度图像（官方利用视差）与纹理图像生成点云。 12345678910111213141516171819202122232425262728filename='antinous';addpath('lib');% 得到antinous的LF.matconvertBlenderTo5D(['additional/',filename])load(['additional/',filename,'/LF.mat']);img=LF.LF(5,5,:,:,:); %中心视角，用于着色r=img(:,:,1);g=img(:,:,2);b=img(:,:,3);% 深度图读取d=pfmread(['additional_depth_disp_all_views\',filename,'\gt_disp_lowres_Cam025.pfm']);d=mat2gray(d);mkdir(['PointClouds-color/',filename]);建立一个文件夹存储图片[ X,Y,Z ] = getPointcloud(LF,'disp',d);ptCloud1 = pointCloud([X(:),Y(:),Z(:)],'color',[r(:) g(:) b(:)]);h1=figure(1);pcshow(ptCloud1);axis offset(gcf,'color',[1 1 1])set(gcf,'Position',[800,300,600,600], 'color','w')view(90.6338, 88.5605);zoom(1.2) 结果如下所示： 注意：生成点云这一步，低版本的MATLAB（如R2014a）由于没有加入相应的函数所以不能够生成点云，高版本（R2016b）可以正常生成。另外，在此提供另外一个函数visualizeZ_3D，该函数将depth map当做彩色图像的z向延伸，然后构图。 123456789101112131415161718192021&lt;!--The function visualizeZ_3D--&gt;function visualizeZ_3D(Z,im)if (im == 0) surf(Z, visualizeZ(Z), 'EdgeColor', 'none'); imtight; axis image ij; %view(-180, 91);else surf(Z, im, 'EdgeColor', 'none'); imtight; axis image ij; %view(-180, 91);endendfunction imtightaxis image off;xMult = 1;yMult = 1;borderSize = 0;PLOTBASESIZE = 500;set(gca, 'PlotBoxAspectRatio', [xMult yMult 1])set(gcf, 'Position', get(gcf, 'Position') .* [1 1 0 0] + [0 0 PLOTBASESIZE*xMult PLOTBASESIZE*yMult]);set(gca, 'Position', [borderSize borderSize 1-2*borderSize 1-2*borderSize]);end]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[实习季到了，大家又浮躁了起来]]></title>
      <url>%2Fposts%2Finternship%2F</url>
      <content type="text"><![CDATA[实习季堪比就业季，今年的形势尤其严峻。伴随着忐忑的心情，我迎来了这个不得不面对的时期。 阿里巴巴视觉算法工程师算法与视觉部分 BF，NN的区别 激活函数的种类 怎么防止过拟合 CUDA的内存模型 HMM是什么 SVM的优缺点 SVD分解的过程 PCA过程 光流法 模版匹配SSD与NCC的优缺点 有哪些形态学操作 相机畸变的参数到底有哪些 交叉熵的概念 Sift与Surf的区别 由前序遍历与中序遍历求后序遍历 深度优先遍历可能的顺序 腾讯基础研究实习生 上机考试包括很多数学方面的知识，比考研数学简单多了，但是范围很广，我想过了这么久大家都忘记了吧。概率论部分占了不少题目，尤其要注意后验概率以及假设检验的题目。基础研究没有编程题目！ 数学部分 特征值与特征向量：Ax=λx 假设检验，第一类错误与第二类错误 求解偏导数 切比雪夫不等式 F分布的性质 简答部分 假设检验来确定零件是否符合标准（可以查看概率论的部分例题） 神经网络以及SVM的对比，优缺点介绍 根据某项调查研究，来确定某结论的正确性； 现场面试部分一面主要包括以下部分： 自我介绍（1分钟内） 项目经历（占了60%时间） 编程题目（反转链表，可参考《剑指offer》） 意向，做工程还是做算法(ps: 被腾讯挂掉了，惨啊) 一面（数据挖掘）猝不及防地又来了一波电话面试，我一脸懵逼的节奏，完全没有准备。我是小白有没有，面试官主要问了以下几个问题： 解释方差，协方差以及样本方差的概念 解释过拟合以及过拟合的概念以及预防措施 解释TCP滑动窗口的概念（这是啥？） 求超级长数组的中位数 析构函数是否可以为虚函数（我是C++小白） 项目介绍 商汤算法实习生 在线笔试 本人申请的岗位是见习算法研究员，笔试1个小时，20道选择填空题，3道编程题。时间略紧。涉及面也非常广，数学，智力，概率统计，线代矩阵，图形学，机器学习，神经网络，C++，均有涉及。 一、选择填空题(部分)： S市A，B共有两个区，人口比例为3：5，据历史统计A的犯罪率为0.01%，B区为0.015%，现有一起新案件发生在S市，那么案件发生在A区的可能性有多大? (概率题，考查贝叶斯公式，牛客网有) A = [1, 2 ; 2, 1]，求A的k次方。(线代，对A进行对角化，求特征值以及特征方程) git常用命令，克隆到本地是（），提交到仓库区（），取回远程仓库的变化，并与本地分支合并（）, 推送所有分支到远程仓库（），显示有变更的文件（）(答：clone, commit, pull, push, status) 表示矩阵需要多少个数字，表示矩阵的投影需要多少个数字? 给出先序序列，中序序列，求后序序列。 一个关于继承和虚函数问题。 掷两个骰子，得到两个数字A,B，设 C = A+B，那么设 C 除以4 的余数为0，1，2，3的概率分别为p0, p1, p2, p3，求它们的大小关系。 图片分辨率为512x512，pad = 2, stride = 3, kernel_size = 9, group = 4, 求卷积后输出分辨率大小。 一个关于图形自由度的问题。（本人完全没概念，所以题目具体记不清了） 以下哪个不能使用迭代器？a) map, b) set, c) queue, d) vector. (c) 有两个样本点，第一个点为正样本,它的特征向量是(0,-1);第二个点为负样本,它的特征向量是(2,3),从这两个样本点组成的训练集构建一个线性SVM分类器的分类面方程是() （答案猛击这里） 在一个无序数组中，求前k个最小数字，复杂度最小为？ 根据以下程序：求func(500)的值。(经典问题，相当于求500的二进制中1的个数，《剑指offer》) 12345678910int func(x)&#123; int countx = 0; while(x) &#123; countx ++; x = x&amp;(x-1); &#125; return countx;&#125; 在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（）a) 增加训练集量b) 减少神经网络隐藏层节点数c) 删除稀疏的特征d) SVM算法中使用高斯核/RBF核代替线性核 关于vector初始化的一个问题。 有4个车站连通情况如下，每辆车每天都会等概率随机从一个车站出发，然后在某个车站呆一夜，第二天再出发。求稳定之后，每个车站的车辆比例。 ( 根据马尔科夫链 平稳分布，π=πP( P为转移概率矩阵)，和π1+π2+π3+π4=1，同时π1=π4, π2 = π3。可以求得2:3:3:2 ） 二、编程题: 连续子数组的最大和。（leetcode或剑指offer原题） Minimum Window Substring .(leetcode 原题) 特别鸣谢smallpum123的商汤回忆版！ 现场面试 4月25日，从学校匆匆到了商汤科技进行面试，幸亏提前到了1个小时，要不然就被淋成落汤鸡了。 一共有两个面试官依次面了我，这两名面试官的侧重点不同，第一位是大体了解面试对象，第二位面试官更加具体深入了解面试者。 第一个面试官简单地聊了一段时间，首先是自我介绍，然后是项目经历，最常用的编程语言（我说的是Matlab），然后又问了我有没有用过Matlab的高级函数（bsxfun、ismember等），其他的没有很深入地讨论项目细节。（20 min） 接下来就是第二个面试官，还是重复前面的问题，自我介绍，项目经历，不过这次更加具体了。因为我的方向是做一些基于光场相机的深度图像估计研究的，面试官就问了我关于光场相机原理以及深度估计算法细节方面的东西；然后又问了我第二个基于GPU加速的项目，具体是如何加速代码的（该项目偏工程，没有具体展开）。项目的最后又问了我这些工程的代码量有多大，多少行的样子（我说最少得两、三千行吧）。 最后就是编程题目，面试官问我关于商汤在线评测代码书写的问题，我的回答是：对于连续子数组的最大和问题仅仅写了思路，没有写全代码。然后就是让我现场手写代码了，大概修改了4遍的样子，终于“调试”（所谓调试就是，现场测试代码一步一步写出结果）成功。（60 min） 当然，面试的最后通常面试官都会问面试者想要了解公司的事情，我就如实将我想要知道的事情想他请假了一下下，然后就没有然后了… 经过大概一个半小时，面试结束。无论结果如何，我的心情瞬间轻松许多。还是静候佳音吧~ 电话面试由香港那边的负责人对我进行了远程电话面试，主要包括自我介绍以及项目介绍，重点在项目介绍上面。Ps：当时电话那头是两位面试官听着我的陈述，我竟然浑然不知。就这样过了40分钟，结束。等待的时间最为忐忑，我觉得自己表现平平，不知道给面试官留下了什么印象。 顺利通过经过一个漫长的劳动节并时逢校庆的假期，5月6号的下午收到了HR打来的电话，成功通过面试，现在心情还是特别激动。 搜狐图像处理实习生 初选简历过关之后进行面试。 笔试（60min）根据应聘的实习岗做不同的题目，因为我面的是千帆直播下的图像处理岗位，所以我的题目中有很多关于这方面的相关知识。以下是我的会议版本： do while 和while do 的区别 char const *p 与char * const p 的区别(答：p都是指向const char类型的指针, 不可以赋值给*p, 就是不可通过这个指针改变它指向的值; 第二个: char * const p是指向char的常指针, 指针需在声明时就初始化, 之后不可以改变它的指向) 创建并且初始化一个双向链表 代码实现二分查找 对一个WAV格式的文件头用适当的数据结构进行表示 队列与栈的区别，分别以什么数据结构表示 常见的视频压缩方法，视频格式，音频格式 汇编语言和C/C++混合编程有哪些方法 如何引用一个已经定义好的全局变量，并比较异同 gdb如何调试线程，多线程呢（ps:我根本不会这个题目） 解释“熵”的概念（答：熵，热力学中表征物质状态的参量之一，用符号S表示，其物理意义是体系混乱程度的度量。信息熵表示信息的丰富程度，定义为E=-plog(p)） 请解释1080p的含义（答：1080指的是分辨率1920*1080，p为扫描方式：逐行扫描） 请解释FPS的全称以及含义（答：Frames Per Second，帧率的意思） 解释“码率”的概念（答：即比特率，一秒钟处理的数据量大小，影响到视频的质量以及帧率） MPEG标准中有哪些帧类型 有以下数据结构，请问最后输出结果的是？（注意共用体的大小）12345678910111213typedef union&#123; long i; int j[3]; char k;&#125;DATA;struct data&#123; int m; DATA n; double q;&#125;;data max;printf(&quot;%d/n&quot;,&amp;(sizeof(DATA)+sizeof(max)); 还有其他的题目，记不太清了，但是主要就是以上的。难度适中，即有涉及到程序也有图像以及视频处理的相关知识。因为当时没有好好准备，猝不及防的给我了这些题目，感觉一脸懵逼。 现场面试面试小哥很nice，人很好。首先是自我介绍，然后就是项目经历。基本上简历上的内容问了一遍，感觉还不错。问了我如果调试正在进行中的程序，如何用markdown语言引用代码，对Latex的了解等等。小哥面试结束后，以为女面试官姐姐再次对我的一些基本情况进行了询问，最后还送了一个可爱的小狐狸。无论结果如何等结果吧，祝我好运！ps：很幸运地被录为实习生，但是还是选择了商汤。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[日本与美国之行]]></title>
      <url>%2Fposts%2Fjournal2JP-USA%2F</url>
      <content type="text"><![CDATA[早上起得很早，坐车从深圳到香港乘坐飞机。一切就绪，出发！ 日本国之行 小时候因为极其喜欢动漫的缘故，我喜欢上了日本，从那时起已经向往能够进行一次日本之行。2017年1月8日早上的闹钟比以往想得更加早些。迎着黎明前第一缕阳光的到来，我们来到了香港机场。即将开始人生第一次出国高校访问之旅，我心情相当激动。 飞机经历两个小时到达日本，偌大的东京比深圳多了一份凉意。拖着行李坐着日本地铁匆匆来到了池袋的旅馆，安顿下来，第一天就这样结束。接下来的几天我们团队依次到了东京、京都、奈良等地，参观日本几所具有代表性的大学，体验日本的文化以及感受日本的风土人情。 初来乍到，凛冬已至，虽寒意袭来，但心中激动之情让我变得火热。日本之行我们团队参观了5所大学（早稻田大学，东京大学，法政大学，京都大学，秋田大学）以及1个科研院所（NHK科技研究实验室）。 日本同学介绍的科研内容并非高大上的东西，有些只是利用现有的算法来解决现实问题；但是他们做出的东西都是系统化的，最终的结果是以实物或者可视化的形式作为展示。具体而言，对于早稻田大学3个机器人实验室，他们的研究方向更加贴近于实际生活，而并非空谈理论，关注于“人”本身，而并非突发奇想。一位早稻田大学的师兄提到，他们的研究成果必须最终以Demo的形式展示，相比之下，国内科研更加注重仿真，如若计算机仿真能够成功的实现预期目标算是大功告成，省去了硬件实现或者可视化展示这一步。这对于该领域内的科研人员而言是及其容易理解的，但是对于大众而言，大都不能够理解算法的原理以及实现细节，他们关注的是研究的目的以及能够解决的问题。因此如何以一种通俗易懂的方式向大众介绍自己的研究内容成为了一个问题，日本高校相比国内高校在科研成果展示方面具有一定的领先。 值得注意的是，最后一站的秋田大学与以前的几个大学有着明显差异，它位于日本的北端，抵达时正值秋田几十年来最大的一场雪，这场大雪增加了参观秋田大学的乐趣。起初，我们对于这个农业科学见长的高校知之甚少。通过对其参观，我了解到了这个新兴高校的发展历史以及现有科研水平，秋田大学如何在这十几年时间内迅速成长必然值得我国高校学习。 总结而言，东京大学与京都大学历史悠久，科研气氛浓厚且资源丰厚；早稻田大学注重实践，校企合作成为常态；政法大学兼容并包，面向世界；秋田大学后起之秀，努力吸收优秀资源。尽管参观时间有限，但是我们能够明显的感觉到日方高校对于我们来访的热情，同时也可以了解到日本最有代表性大学的科研水平。最大的收获在于，亲身感受到了跨文化交流。我们用英文与日本同学交流，语言不再是最大的障碍。保持好奇心一直是我的座右铭，尽管研究领域不尽一致，但我会努力的提出自己的疑问并聆听日本同学的讲解。起初时候会担心自己不能够很好的提出疑问或者不能够听懂他们的回答，但是主动尝试之后会发现，自己所担心的问题并不存在，同时我也逐渐喜欢上了与他们之间的交流的过程。 京都游玩的地方很多，交流之余，游遍几个好玩的地方 京都街头随拍： 京都伏见稻荷大社： 京都清水寺求姻缘之地： 此外，很早就听说日本民众素质很高，这次真切的感受到了。值得注意的是，他们在日常生活中表现出的礼貌也值得人敬佩，这很符合日本人不愿给别人添麻烦的特点。他们时常把“すみませ”挂在嘴边，比如进电梯时，进入餐厅时或者超市付款时；无论多赶时间也会排队，另外在公共场合他们不会大声喧哗，具体而言在地铁上无论人再多再挤，车厢内也是安静的让人感觉不可思议；他们的房屋建设的很低，密密麻麻的挤在一起，电线在房屋之间穿梭，但是让人感觉并不杂乱，“规矩”一词用在他们身上并不为过。同时日本对于古老传统的保留非常重视，以前就听说日本是保留中国古文化最好的非华人国家。果然，“相扑，茶道，和服，空手道”等诸多优良文化在日本保留的相当好，在日剧以及日本民族产业动画中都有很好体现。改革开放以来，我国的经济实力突飞猛进，如今已经跃居世界第二大经济体，但是国际上的文化软实力远远不如日本。很多欧美人士所了解的很多“日本传统”实际是源于中国，日本在国际上的形象是优于中国的。“择其善者而从之”，我认为国内的道德建设以及文化建设必须同经济发展相协调，提高全民素质，打造真正属于中国的国际形象。 大阪上空： 东京远望富士山： 美利坚合众国之行 出国开会，顺便拍了几张图~ 新奥尔良法国区一隅： 教堂一隅： 杰克逊广场一隅: 杜兰大学军事学院： 杜兰大学一隅： 二战博物馆：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Light Field 光场以及MATLAB光场工具包(LightField ToolBox)的使用说明]]></title>
      <url>%2Fposts%2FLightField-Toolbox%2F</url>
      <content type="text"><![CDATA[这里我汇总了有关光场（Light Field）一些有用的链接以及光场数据的处理过程。目前还在整理中，随时更新。声明：一切理解都是本人观点，如有疑问，还望在评论中留言。如需转载请与本人联系，谢谢合作! 邮箱：点我 光场相机大家在刚刚入手光场领域的时候可能会用到目前消费级的手持光场相机，如Lytro或者ILLUM，如图（实验室的设备，我可买不起ILLUM）： 获取.LFP(or .LFR)原文件由Lytro拍摄的图像的原格式是.lfp格式，我们要将其解码成我们需要的格式。工具：Lytro Desktop，MATLAB光场工具包（很强大，推荐，本文介绍该方法）。 首先用数据线把设备连接到电脑，打开Lytro Desktop，点击想要导入的图片，选中点击右上角立即处理，然后打开我的电脑图片-&gt;Lytro Desktop\Libraries\Lytro 图库.lytrolibrary*，就可以发现有很多文件夹名字是一串很长数字字母云云。点击进去可以发现里面有几个文件，以lytro为例，这几个文件如下形式： raw.lfp就是我们需要的原文件，之后我们就要利用Matlab光场工具包对其进行解码操作。 Matlab Light Field ToolBox(光场工具箱)的使用下载光场工具包（LFToolBox）首先下载光场工具箱并仔细阅读说明文档，根据文档把相应的数据拷贝到工具箱的文件夹下(这一步很关键，要仔细配置)。如果不想在官网下载的话我上传到了度娘的云盘链接：链接 密码：yykc。这是我修改后的一个版本，可以直接运行。另外我在Github上传了一个版本，大家可以git clone链接。下载下来的工具包是这样的： LFToolbox0.4就是我们需要的工具包，该工具包里包含很多函数，如下图： 在此，我把一些比较常用的函数及文档用红色的框标注出来，其中PDF文档是该工具包的说明书。这个说明书中详细地介绍了该工具包的使用方法，我们完全可以根据该文档的介绍来实现自己想要的功能。如下是该说明书的截图： 该说明文档提供了各种函数用于从LFP文件中提取出自己需要的各种信息：白图像(white image)，Raw Image，对齐后的图像，以及颜色校正，频域滤波后的图像等。因为时间不足没有整理的，感觉大家都对这个过程比较感兴趣，我觉得有必要写一下到底如何读取lfp、lfr、raw文件了。好了言归正传，开写。 前期准备step 1: 创建自己的工作目录如果是直接clone我在github上的工程的话直接跳转step 2。如果没有，那就要建立自己的工作目录，便于文件的管理。这一步是必要的，如果建立的目录不一致，可能导致程序无法运行，这也是我当时初次用这个工具包时常常出错的地方。好了，建立这样的目录结构： step 2: 根据相机序列号修改文件名Sample_test表示我们的测试目录，里面包含了相机信息以及自己拍摄照片的图像（lfp/lfr）。Cameras 这个目录中又包含了几个文件夹，它们分别是以“A”或者“B”开头，在其后面有一长串数字。这其实就是光场相机的serial number，我们可以从默认目录 c:\Users\\AppData\Local\Lytro\cameras\sn- serial_number中找到，这个数字每个相机不一样，大家根据自己的相机序列号修改这个目录哈。”A”表示的是LYTRO系列相机，“B”表示ILLUM系列相机；以上图为例，”A303134427”就是我相机的序列号。 step 3: 把白图像文件拷贝到相应的文件夹下在每个序列号文件夹下又有一个文件夹WhiteImages，这里面放着由该相机拍摄的白图像。所谓白图像就是一张由光场相机拍摄的白色的图像，当然自己也可以拿着光场相机对着白色的墙面拍几张，但是效果并不一定很好。庆幸的是LYTRO官方提供了白图像，以Lytro为例，我们可以从以下目录找到: c:\Users\ username\AppData\Local\Lytro\cameras\sn- serial_number。如下图所示： 我们发现这里有以下4个文件：data.C.0/1/2/3，这是官方把白图像压缩成了这种格式，我们需要用工具箱进行解码。我们需要的正是这四个文件，拷贝出这4个文件，放在WhiteImages文件夹里。这一步相当关键，一定要确保拷贝对了目录。注意，Illum相机的白图像与Lytro的白图像的存放位置不一样，在相机的SD卡里。 step 4: 将测试文件放到Images目录Images文件夹下包含我们需要处理的文件们，F01下存放LYTRO系列拍摄的文件，B01下存放ILLUM系列拍摄的文件。以Lytro为例，由于前面已经有了测试文件raw.lfp，我们就把这个文件放在F01下。经过我们上述的过程之后，最后我们的目录会变成这样（注意：Sample_test与LFToolBox0.4为同级目录，各个文件夹的名字务必正确）： 测试开始我用的是实验室的电脑，配置是：Intel(R) Core(TM) i7-4790 CPU @3.60GHz 3.60GHz RAM 16GB。其中的Demo文件是本人编写的一小段测试代码，已经贴在了文末。接下来的过程就是RUN CODE了。程序大致可以分为以下几个测试： 处理白图像 解码LFP文件 频域滤波 颜色校正 处理白图像处理白图像的目的是得到相机的某些参数，我当时是为了获得每幅光场的中心点坐标才进行的这一步。白图像拍摄的场景没有纹理，此时可以清楚的观察到微透镜成像的边界信息。如下图所示： 可以看到的是，微透镜下成像是这种正六边形的网格，类似于蜂窝的结构，感觉很酷有木有。需要注意的是，该过程不是简单地提取出一张白图像来，而是提取几十张白图像对（image pairs），这个过程运行起来有点久，以下是运行的截图： 解码LFP文件如果只是单纯地读出LFP/LFR、RAW文件的数据的话可以分别用工具包提供的如下函数：LFReadLFP、LFReadRaw。注意两个函数的返回值不一样。LFReadLFP返回一个结构体类型的变量，它包含相机的各个信息，我们可以根据自己的需要保留数据。LFReadRaw返回的是一张uint16的灰度图，还没有经过demosaicing操作。去马赛克操作在malatb中有相应的函数，这点不用担心。我们在这里不是直接调用的LFReadLFP而是调用了工具箱提供的LFLytroDecodeImage函数。如果运行有问题（若是直接clone我github上项目的话，不需要修改），将LFLytroDecodeImage中的WhiteImageDatabase路径由以下： 12DecodeOptions = LFDefaultField( 'DecodeOptions', 'WhiteImageDatabasePath'...,fullfile('Cameras','WhiteImageDatabase.mat'));% line 71 改为： 123DecodeOptions = LFDefaultField( 'DecodeOptions', 'WhiteImageDatabasePath'... ,fullfile('Cameras',LFMetadata.SerialData.camera.serialNumber,'WhiteImageDatabase.mat'));%== 注意，这条插在 ---Select appropriate white image---上行，而不是在原来的71行修改==。 经过这样的修改之后，这下应该可以跑了。我们可以得到以下图像： 局部放大效果图： 所有视角的图像： 这时候可以看到在边界视角上的图像比较黑，所以我们接下来要进行频域滤波，以及颜色校正。 频域滤波以及颜色校正这部分分别用到了LFFilt4DFFT以及LFColourCorrect函数。以LYTRO 1.0 为例子，我们得到的光场图像一种有11*11个视角，但是这个121个视角子孔径图像的质量真的不敢恭维，尤其是边角处的视角(u=1,v=1)时，这个图像时完全变成黑色的。所以嘛，LFFilt4DFFT这个函数是将这些变成黑色的图像或者质量不好的图像进行校正的，具体原理不作展开。LFColourCorrect这个函数是利用gamma变化对原始图像进行颜色校正的，这一点比较简单。总之利用这两个函数能够让我们得到的光场图像的质量更好，当然你也可以选择不用。 以下是经过滤波之后的所有子孔径图像，可以发现边界的视角相比于频域滤波之前有了很好的可视性。 以下是经过颜色校正之后的所有所有子孔径图像。 经过以上的步骤我们可以学习到白图像的处理，以及光场图像的处理等操作。当然我没有列出这个工具包所有的功能介绍，大家可以根据需要建立自己工程，对自己的数据进行测试，以上！ 注意事项及测试代码参数设置好了再Run不少同学是因为设置不当，导致运行错误，以下我列举了可能出现错误的地方。 务必在WhiteImagesPath处写明相机型号，确定好到底是Lytro还是Illum 注意Illum相机的配对数据在相机的SD卡中，解压caldata-Bxxxxxxxxx.tar，将里面的文件拷贝出来放在路径Sample_test\Cameras\Bxxxxxxxxx\下即可 白图像的处理过程比较久，耐心等待就行即可 Lytro与Illum的频域滤波调用函数是不同的，我已经把代码添加在了相应位置；这个函数用时较久，耐心等待 结果存放在Results_saving文件夹下 再次提醒，由于Illum图像的分辨率比较大，所以当程序运行到LFLytroDecodeImage以及频域滤波时会造成内存以及磁盘的大量使用，慎重考虑。 如有Bug请及时联系我，请在评论区留言。 没有图像文件怎么搞 下载整个工程； 下载数据集：可以在这里下载Lytro数据集，该数据集包括白图像以及图像原文件； 然后将工程Sample_test/Cameras/下的文件Axxxxxxxxxxx修改为A303134643，然后将数据集LytroDataset\sn-A303134643文件夹下的data.C.0.1/2/3放在Sample_test/Cameras/A303134643文件夹下； 将LytroDataset\raws文件夹下的图像原文件放在工程Sample_test/Images/A01/文件夹下； 修改Demo.m中WhiteImagesPath=&#39;Sample_test\Cameras\Axxxxxxxxxx&#39;，以及lfpname=&#39;test&#39;改成步骤4中的任何一个图像原文件即可。 run起来吧~ 解码效果为何不佳另外，很多童鞋问过我一些问题，例如为何光场工具包解码出来的图像质量如此之差，始终达不到Lytro Desktop导出图像的质量。其实该问题是个普遍现象，目前没有一个好的解决方法。光场工具包的发明者Donald Dansereau在Google Plus也是这么认为的，我把原话附在下面： Q from email: there are differences between the toolbox decoded output and the Lytro Desktop’s image. The differences involve color, intensity and noise. How can I fix this? A: Thanks for the email. There will always be important differences between the Lytro software output and the toolbox output. The toolbox tries to generate a 4D light field that is as close as possible to the raw image measured by the camera, while still being a standard two-plane parameterized 4D light field. The Lytro software has a very different goal. They do not produce a 4D light field, they produce 2D renders. These are optimized to look nice, and evidently look much nicer than any 2D slice taken from the toolbox output. They use sophisticated decoding and denoising techniques to do this.The philosophy of the toolbox is to provide a 4D light field close to the raw image captured by the camera, to allow researchers to explore the characteristics of this kind of signal. It should, in theory, be possible to go from the 4D light field output by the toolbox to nice 2D renderings like those produced by the Lytro software. Making nice 2D renderings can also be accomplished by working directly from the lenslet image, as has been demonstrated in a few papers by researchers at Lytro and elsewhere. 从他的回复可以看到，他提供的光场工具包的目的是尽量提供光场相机采集的原汁原味的数据（raw data）用来逼近4D光场信息；同时让研究者去研究这些数据，为他们所用。但是Lytro公司提供Lytro desktop的目的是渲染出漂亮的2D图像，以供用户使用。以上也反映了研究和商业重要区别，Lytro并未提供他们渲染的方式，属内部机密。 此外我也单独问了Donald Dansereau同样的问题，我把他的回复建议原话附在下面：If you want to make nice 2D images, I suggest Filter before colour correction. Try a simple planar focus filter (LFFiltShiftSum, or one of the linear filters in the LFDemoBasicFilt* examples). Don’t use the toolbox colour correction, the code is extremely simplistic and is mostly meant to show you where the metadata is.Look into some 4D - to - 2D rendering techniques for light fields. Look into some 2D denoising techniques and apply them to your 2D render, or to the 4D light field slices. If you do find something that works well for you please share, as this is a common question. 我习惯的参数设置：颜色校正的参数设为Gamma=0.8或者小于0.8，这个参数你可以不停地试。另外，我一般不用我公布代码里的频域滤波，因为我认为这步会很大程度地破坏原始数据。我会选用中心视角邻域的几个视角，例如原来ILLUM提供的是15x15个视角，但是我只用其中的11x11或者更少，这样就可以不用考虑边界视角黑暗的问题了。当然大家可以尝试下按照Donald Dansereau的说法进行尝试，如果大家有好的方法，也可以告诉我。（PS：以上为回复@lixiaohao同学邮件部分内容） 测试代码以下是Demo文件的代码，仅供学习使用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111clc;clear all;clc;addpath(genpath('Sample_test'));addpath(genpath('LFToolbox0.4'));LFMatlabPathSetup;%% Step1: 解压data.C.0/1/2/3---&gt;white,结果存储在Cameras中fprintf('===============Step1: Unpack Lytro Files===============\n\n ');LFUtilUnpackLytroArchive('Sample_test')%% Step2: 包含刚刚解压出来的文件的目录fprintf('===============Step2: Process WhiteImages===============\n\n');WhiteImagesPath='Sample_test\Cameras\B5151500510'; % 务必要设置这个值 B5151500510 A303134427LFUtilProcessWhiteImages( WhiteImagesPath);%% Step3: 解码光场图像.lfpfprintf('=====================Step3: Decode LFP===================\n\n');cd('Sample_test'); % 进入Sample_test目录lfpname='baby'; %测试图像名称，改成你自己的if WhiteImagesPath(21)=='A' %找到型号 exist('LYTRO','var') version='F01';elseif WhiteImagesPath(21)=='B'%找到型号 exist('ILLUM','var') version='B01';endInputFname=['Images\',version,'\',lfpname,'.lfp'];[LF, LFMetadata,WhiteImage,CorrectedLensletImage, ...WhiteImageMetadata, LensletGridModel, DecodeOptions]... = LFLytroDecodeImage(InputFname);cd('..');imshow(CorrectedLensletImage) %Raw Imagemkdir(['Results_saving\',lfpname]);imwrite(CorrectedLensletImage,['Results_saving\',lfpname,'\',lfpname,'.bmp']);save(['Results_saving\',lfpname,'\',lfpname,'.mat'],'CorrectedLensletImage');%% =======================频域滤波================================%---Setup for linear filters---tic% lytroif strcmp(version,'F01')==1 LFPaddedSize = [16, 16, 400, 400]; BW = 0.03; FiltOptions = []; FiltOptions.Rolloff = 'Butter'; Slope1 = -3/9; % Lorikeet Slope2 = 4/9; % Building fprintf('Building 4D frequency hyperfan... '); [H, FiltOptionsOut] = LFBuild4DFreqHyperfan( LFPaddedSize, Slope1, Slope2, BW, FiltOptions ); fprintf('Applying filter'); [LFFilt, FiltOptionsOut] = LFFilt4DFFT( LF, H, FiltOptionsOut );% illumelseif strcmp(version,'B01')==1 LFSize = size(LF); LFPaddedSize = LFSize; BW = 0.04; FiltOptions = []; %---Demonstrate 4D Hyperfan filter--- Slope1 = -4/15; % Lorikeet Slope2 = 15/15; % Far background fprintf('Building 4D frequency hyperfan... '); [H, FiltOptionsOut] = LFBuild4DFreqHyperfan( LFPaddedSize, Slope1, Slope2, BW, FiltOptions ); fprintf('Applying filter'); [LFFilt, FiltOptionsOut] = LFFilt4DFFT( LF, H, FiltOptionsOut ); title(sprintf('Frequency hyperfan filter, slopes %.3g, %.3g, BW %.3g', Slope1, Slope2, BW)); drawnow save(['Results_saving\',lfpname,'\',lfpname,'5D.mat'],'LFFilt');end%% =======================颜色校正参数设置========================== ColMatrix = DecodeOptions.ColourMatrix;Gamma=DecodeOptions.Gamma;ColBalance=DecodeOptions.ColourBalance;% 对3280*3280的原始彩色图像进行颜色校正ColorCorrectedImage=LFColourCorrect(CorrectedLensletImage, ColMatrix, ColBalance, Gamma);imwrite(ColorCorrectedImage,['Results_saving\',lfpname,'\',lfpname,'ColorCorrectedImage.bmp']);save(['Results_saving\',lfpname,'\',lfpname,'ColorCorrectedImage.mat'],'ColorCorrectedImage')imshow(ColorCorrectedImage);title('Corrected Lenslet Image');%% 同样是颜色矫正， 为了得到光场数据。得到5-D LFColorCorrectedImage数据LFColorCorrectedImage=zeros(size(LF,1),size(LF,2),size(LF,3),size(LF,4),size(LF,5));for i=1:size(LF,1) for j=1:size(LF,2) temp =squeeze(LFFilt(i,j,:,:,1:3)); temp = LFColourCorrect(temp, ColMatrix, ColBalance, Gamma); LFColorCorrectedImage(i,j,:,:,1:3)=temp; imshow(temp); pause(0.1) endendLFColorCorrectedImage(:,:,:,:,4)=LF(:,:,:,:,4);save(['Results_saving\',lfpname,'\',lfpname,'RawLFColorCorrectedImage.mat'],'LFColorCorrectedImage');% very importanttoc%-------------------------------------------------------------------------------- ## Lytro官网可视化工具 谁让人家Lytro不开源呢，人家自己做的Demo还不错。通过鼠标就可以对以下图像进行重聚焦，变化视角，以及缩放等操作。话不多说，上图！ 呵，人家公司在2017年11月30号之后停止了对lytro live photo的线上支持，所以以下啥都没有了。 数字重聚焦利用如下的重聚焦公式可以实现光场图像的重聚焦。 L_{\alpha}(x,y,u,v)=L_0(x+(1-\frac{1}{\alpha})u,y+(1-\frac{1}{\alpha})v,u,v)左图为其中心视角图像，右图为重聚焦（参数 $\alpha$ =0.5）之后的图像。 给出部分测试代码如下，全部代码见Github; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283%% This is a demo of light field refocusing% input: LF % ouput: refocused pinhole image% Writen by: Vincent Qin% Data: 2018 May 17th 21:35:14%% Note: the input is 5D LF data decoded from Matlab Light field Toolboxclc;addpath(genpath(pwd));%% mex functioncd('src'); mex REMAP2REFOCUS_mex.c mex BLOCKMEAN_mex.c cd ..use_vincent_data=1;if use_vincent_data disp('Downloading LF data, please wait...'); URL='http://p8vl2tjcq.bkt.clouddn.com/LF.mat'; [f, status] = urlwrite(URL,'input/LF_web.mat'); if status == 1; fprintf(1,'Success！\n'); else fprintf(1,'Failed！\n'); end load('input/LF_web.mat');else load('input/your_LF_data.mat');enddisp('Processing LF to Remap image...');LF=LF(:,:,:,:,1:3);[UV_diameter,~,y_size,x_size,c]=size(LF);% get LF remap and pinhole image before refocusingLF_Remap = LF2Remap(LF);% IM_Refoc_1 = zeros(y_size, x_size,3);temp = zeros(y_size, x_size);% BLOCKMEAN_mex(x_size, y_size, UV_diameter, LF_Remap(:,:,1), temp);IM_Refoc_1(:,:,1)=temp;% BLOCKMEAN_mex(x_size, y_size, UV_diameter, LF_Remap(:,:,2), temp);IM_Refoc_1(:,:,2)=temp;% BLOCKMEAN_mex(x_size, y_size, UV_diameter, LF_Remap(:,:,3), temp);IM_Refoc_1(:,:,3)=temp;% get paramsLF_x_size = x_size * UV_diameter;LF_y_size = y_size * UV_diameter;UV_radius = (UV_diameter-1)/2;UV_size = UV_diameter*UV_diameter;% collect dataLF_parameters = struct(... 'LF_x_size',LF_x_size,... 'LF_y_size',LF_y_size,... 'x_size',x_size,... 'y_size',y_size,... 'UV_radius',(UV_diameter-1)/2,... 'UV_diameter',UV_diameter,... 'UV_size',UV_diameter*UV_diameter) ;% predefine outputLF_Remap_alpha = zeros(LF_y_size,LF_x_size,3) ;IM_Refoc_alpha = zeros(y_size,x_size,3) ;% here begins refocusingdisp('Processing refocusing...');alpha=0.5; % shearing numberREMAP2REFOCUS_mex(x_size,y_size,UV_diameter,UV_radius,LF_Remap,LF_Remap_alpha,IM_Refoc_alpha,alpha); % show figurecentral_view=squeeze(LF(UV_radius+1,UV_radius+1,:,:,:));figure; imshow(central_view);title('central view');set(gcf,'color',[1 1 1]); figure; imshow(IM_Refoc_alpha);title(['refocused image pinhole at alpha = ' num2str(alpha)]);set(gcf,'color',[1 1 1]); % concat themconcatImg=[central_view,IM_Refoc_alpha];figure;imshow(concatImg);set(gcf,'color',[1 1 1]); imwrite(concatImg,'concatImg.png'); Lytro DesktopLytro Desktop是曾经的Lytro官方提供的软件，可以处理由Lytro系列相机拍摄到的图像。利用该软件能够导出相机配对数据、重聚焦图像、全聚焦图、深度图、原始文件等。下面给出一些常用的导出文件。 导入需要处理的图像 导出配对数据 导出待处理图像的各种格式 全聚焦彩色图与内置深度图 重聚焦 H.264电影 (function(){var player = new DPlayer({"container":document.getElementById("dplayer1"),"loop":true,"video":{"url":"http://oofx6tpf6.bkt.clouddn.com/molly.mp4"},"danmaku":{"id":"4d5c01842f37d90651f9693783c6564279fed6f4","api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})() 以上，如有问题欢迎评论]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Matlab相关问题汇总]]></title>
      <url>%2Fposts%2Fissues-Matlab%2F</url>
      <content type="text"><![CDATA[以下是我在使用Matlab编程时遇到的问题以及解决方法，最后彩蛋随时补充。 Matlab绘制GIF动图12345678910111213141516171819202122232425262728293031input_file_path='your image folder';out_name = 'out'; %output nameimg_path_list=dir(strcat(input_file_path,'*.jpg')); %image format[~, ind] = sort([img_path_list(:).datenum], 'ascend');img_path_list = img_path_list(ind);img_num=length(img_path_list);if img_num == 0 disp('No images in the folder!');else for j=1:img_num image_name = img_path_list(j).name; read_image = imread(strcat(input_file_path,image_name)); imshow(read_image,'border','tight','initialmagnification','fit'); axis normal; truesize; n=j; frame(n)=getframe(gcf); % get the frame image=frame(n).cdata; [image,map] = rgb2ind(image,256); if n==1% imwrite(image,map,outname,'gif'); imwrite(image,map,[out_name '.gif'],'gif','Loopcount',inf); else imwrite(image,map,[out_name '.gif'],'WriteMode','append','DelayTime',0.2); end endend Matlab 局部放大图像论文里的局部放大图，再也不用每次手动截了。 12345678910111213141516171819202122232425262728293031323334353637function small_im=ZoomIm_(im,pos)% im= imread('lena.jpg');% pos=[226,221,340,384];% [左上X，左上Y，右下X，右下Y]clcclose all;figure;up_leftX=pos(1);up_leftY=pos(2);down_rightX=pos(3);down_rightY=pos(4);%% mark rectangle in source imageimshow(im);hold on;set(gcf,'color',[1 1 1]);line([up_leftX, down_rightX] ,[up_leftY ,up_leftY],'linestyle','-','linewidth',3,'color','r');line([up_leftX, up_leftX] ,[up_leftY ,down_rightY],'linestyle','-','linewidth',3,'color','r');line([down_rightX, down_rightX],[up_leftY ,down_rightY],'linestyle','-','linewidth',3,'color','r');line([up_leftX, down_rightX] ,[down_rightY,down_rightY],'linestyle','-','linewidth',3,'color','r');hold off;%% get rect imagefigure;small_im=im(up_leftY:down_rightY,up_leftX:down_rightX,:);imagesc(small_im);axis equalaxis offset(gcf,'color',[1 1 1]);set(gca,'xtick',[],'ytick',[]);% set(gca,'position',[0.1 0.1,0.8 0.8])set(0,'DefaultFigureMenu','figure');% figure('menubar','on');% set(0,'Default');set(gcf,'Position',[500,200,800,500])end Matlab 保存超高质量图像除了直接保存成eps或者emf格式之外，也可直接在绘制完图像之后，保持当前绘图窗口不要关闭，在命令行窗口键入如下命令，然后在word文档/PPT里ctrl+v粘贴即可。1uimenufcn(gcf,'EditCopyFigure') Matlab 写入Excel错误问题描述 Matlab 在创建EXCEL文件的时候总是出错，即使使用MATLAB自带的程序。 问题描述：在Matlab中使用xlswrite函数时，如果excel文件存在时，则程序能够正常运行；当excel文件不存在时，则会出现错误： 1Error using xlswrite (line 220) Error: 服务器出现意外情况。 解决之道xlswrite函数在调用时会占用Excel的com端口，所以要保证在调用时这个端口是开放的，也就是没有被其他程序占用。打开任意一个Excel（我的是16版）文档，点击文件—选项，弹出Excel选项卡，在加载项中可以看到，活动应用程序加载项，以及非活动应用程序加载项；由于我的系统中装了一个福昕阅读器，该程序占用了Excel的com端口，所以当Matlab再去调用这个端口时就会出现异常。具体解决方法：点击管理旁边的下拉菜单，选择COM加载项，点击转到，把福昕阅读器的前面的勾去掉，然后确定。 Matlab设置绘图坐标轴信息问题描述 Matlab 作图时更改纵轴刻度为科学计数法，指数放在框左上方 12345678plot([0 1],[0 .02]) % 作图，换成自己的图像就可以~oldLabels = str2num(get(gca,'YTickLabel'));scale = 10^2;newLabels = num2str(oldLabels*scale);set(gca,'YTickLabel',newLabels,'units','normalized');posAxes = get(gca,'position');textBox = annotation('textbox','linestyle','none','string',['x 10\it^&#123;' sprintf('%d',log10(1./scale)) '&#125;']);posAn = get(textBox,'position');set(textBox,'position',[posAxes(1) posAxes(2)+posAxes(4) posAn(3) posAn(4)],'VerticalAlignment','cap'); Matlab显示图片错误问题描述 MATLAB图像显示总是白色 imshow是一个很强大的”武器”，显示图像简直无所不能，但这其中往往会出现问题。在处理图像时，我们的图像经常是经过了某种运算，为了保证其精度，系统会自动的将uint8型数据类型转化成double型。 如果直接运行imshow(I)，我们会发现显示的是一个白色的图像。这是因为imshow()显示图像时对double型是认为在0~1范围内，即大于1时都是显示为白色，而imshow显示uint8型时是0~255范围。而经过运算的范围在0-255之间的double型数据就被不正常得显示为白色图像了。 解决之道 可以利用mat2gray()函数，这个函数是归一化函数，可以把数据归一化到0-1之间，再用imshow()就可以了； 或者对于一个处理后的黑白图像Img，若为double型可以这样写：imshow(Img/max(Img(:)))； 还有一种就是：imshow(Img,[]);就是加一个[]，即可以自动调整显示； 常用命令汇总 （2016.05.19）今天学到一个特别简单的语句，删除元胞数组中空元素： 1a(cellfun(@isempty,a))=[]; （2018年6月1日）在命令行敲入如下命令，如果运行出现错误，matlab会自动停在出错的那行，并且保存所有相关变量。 1dbstop if error 彩蛋 大神教我们怎么画图，#MATLAB无所不能#，戳戳这里~ Matlab 字体困扰了我很长时间，终于在网上找到了一个比较好的组合，猛戳这里！原文地址 Matlab与C混合编程 Matlab并行 Matlab代码优化：教你写出漂亮的Matlab代码]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HEXO建站备忘录]]></title>
      <url>%2Fposts%2Fbuild-a-website-using-hexo%2F</url>
      <content type="text"><![CDATA[Hexo作为建立Blog利器，为我们没有JS基础的小白们提供了建立专属自己博客的机会！经常使用的语法很简单，我们完全可以在10min分钟之内建立自己的Blog，后期的优化才是最耗费时间的。好了，直接进入正文。 123456hexo clean # 清除缓存，简写 hexo chexo generate # 作用：建立静态页面， 简写 hexo g hexo deploy # 部署自己的blog，本人部署在了Git上，简写 hexo dhexo server # 以启动本地服务， 可预览，简写 hexo shexo new blog_name # 新建以blog_name为名的blog在.md文档中加入 &lt;!-- more --&gt; 可以显示“阅读全文” Gitment 小小改动文件：~\themes\next\layout\_third-party\comments\gitment.swig，Gitment与Gitmint的css以及js文件上传到网站相应的目录下，分别是：~\themes\next\source\js\src以及~\themes\next\source\css。 1234567891011&lt;!-- LOCAL: You can save these files to your site and update links --&gt; &#123;% if theme.gitment.mint %&#125; &#123;% set CommentsClass = "Gitmint" %&#125; &lt;link rel="stylesheet" href="https://www.vincentqin.tech/css/gitmint-default.css"&gt; &lt;script src="https://www.vincentqin.tech/js/src/gitmint.browser.js"&gt;&lt;/script&gt; &#123;% else %&#125; &#123;% set CommentsClass = "Gitment" %&#125; &lt;link rel="stylesheet" href="https://www.vincentqin.tech/css/gitment-default.css"&gt; &lt;script src="https://www.vincentqin.tech/js/src/gitment.browser.js"&gt;&lt;/script&gt; &#123;% endif %&#125;&lt;!-- END LOCAL --&gt; Valine 刷新后评论消失参考这个Issue，删除文件.\themes\next\layout\_third-party\analytics\lean-analytics.swig中第四行： 1&lt;script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"&gt; 但是这样之后无法显示阅读量了。 删除 Powered by Valine文件路径： ~/themes/next/layout/_third-party/comments/valine.swig1234567891011new Valine(&#123;...pageSize:'&#123;&#123; theme.valine.pageSize &#125;&#125;' || 10,&#125;);&#123;# 新增以下代码即可，可以移除.info下所有子节点。#&#125;var infoEle = document.querySelector('#comments .info');if (infoEle &amp;&amp; infoEle.childNodes &amp;&amp; infoEle.childNodes.length &gt; 0)&#123; infoEle.childNodes.forEach(function(item) &#123; item.parentNode.removeChild(item); &#125;);&#125; 网页压缩参考@muyunyun给出的教程，可以进行如下设置。首先安装hexo-all-minifier，这个模块集成了对 html、css、js、image 的优化。1$ npm install hexo-all-minifier --save 然后在根目录下修改站点配置文件_config.yml，添加如下命令重新部署即可。 12345678910111213141516171819202122232425html_minifier: enable: true ignore_error: false exclude:css_minifier: enable: true exclude: - '*.min.css'js_minifier: enable: true mangle: true output: compress: exclude: - '*.min.js'image_minifier: enable: true interlaced: false multipass: false optimizationLevel: 2 pngquant: false progressive: false 代码区高级设置可以参考这里：HEXO下的语法高亮拓展修改，具体而言，Markdown的代码段的语法是这样的。格式： 123```[language] [:title] [lang:language] [line_number:(true|false)] [first_line:number] [mark:#,#-#] [diff:true|false] [url:http...] code snippet``` 支持的语言包括：c, abnf, accesslog, actionscript, ada, apache, applescript, arduino, armasm, asciidoc, aspectj, autohotkey, autoit, avrasm, awk, axapta, bash, basic, bnf, brainfuck, cal, capnproto, ceylon, clean, clojure, clojure-repl, cmake, coffeescript, coq, cos, cpp, crmsh, crystal, cs, csp, css, d, dart, delphi, diff, django, dns, dockerfile, dos, dsconfig, dts, dust, ebnf, elixir, elm, erb, erlang, erlang-repl, excel, fix, flix, fortran, fsharp, gams, gauss, gcode, gherkin, glsl, go, golo, gradle, groovy, haml, handlebars, haskell, haxe, hsp, htmlbars, http, hy, inform7, ini, irpf90, java, javascript, json, julia, kotlin, lasso, ldif, leaf, less, lisp, livecodeserver, livescript, llvm, lsl, lua, makefile, markdown, mathematica, matlab, maxima, mel, mercury, mipsasm, mizar, mojolicious, monkey, moonscript, n1ql, nginx, nimrod, nix, nsis, objectivec, ocaml, openscad, oxygene, parser3, perl, pf, php, pony, powershell, processing, profile, prolog, protobuf, puppet, purebasic, python, q, qml, r, rib, roboconf, rsl, ruby, ruleslanguage, rust, scala, scheme, scilab, scss, smali, smalltalk, sml, sqf, sql, stan, stata, step21, stylus, subunit, swift, taggerscript, tap, tcl, tex, thrift, tp, twig, typescript, vala, vbnet, vbscript, vbscript-html, verilog, vhdl, vim, x86asm, xl, xml, xquery, yaml, zephir。 以具体的例子进行讲解，以下是一段matlab程序，我们对其位置进行描述同时标记第1,3-4行，修改部分代码。 4 first_line=221234567891011121314151617r = 7;eps = 0.0001;-tic;+ticreverseStr = '' ;for d=1:nD p = weight_cost(:,:,d); q = guidedfilter_color(double(img_view), double(p), r, eps); weight_cost(:,:,d) = q; msg = sprintf('Processing: %d/%d done!\n',d, nD) ; fprintf([reverseStr, msg]); reverseStr = repmat(sprintf('\b'), 1, length(msg));endfprintf('Final depth estimation completed in %.2f sec\n', toc);[~,weightD] = max(weight_cost,[],3);save_img = uint8((256/(nD))*(weightD-1));imwrite(save_img,strcat(output_path,'SPO_depth.bmp')); 注意，我的网站此处显示有误（先占坑）!另外修改代码块颜色样式,12345678910// 文章```代码块顶部样式.highlight figcaption &#123; margin: 0em; padding: 0.5em; background: #eee; border-bottom: 1px solid #e9e9e9;&#125;.highlight figcaption a &#123; color: rgb(80, 115, 184);&#125; 修复行内公式显示乱码以下解决方案来自这里。 更换Hexo的markdown渲染引擎，hexo-renderer-kramed引擎是在默认的渲染引擎hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。12npm uninstall hexo-renderer-marked --savenpm install hexo-renderer-pandoc --save 执行上面的命令即可，先卸载原来的渲染引擎，再安装新的。 然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为hexo-renderer-kramed引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的escape变量的值做相应的修改： 12// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/, escape: /^\\([`*\[\]()#$+\-.!_&gt;])/, 这一步是在原基础上取消了对\,{,}的转义(escape)。同时把第20行的em变量也要做相应的修改。 12// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/ 重新启动hexo（先clean再generate）,问题完美解决。哦，如果不幸还没解决的话，看看是不是还需要在使用的主题中配置mathjax开关。如何使用了主题，要在主题（Theme）中开启mathjax开关，下面以next主题为例，介绍下如何打开mathjax开关。进入到主题目录，找到_config.yml配置问题，把mathjax默认的false修改为true，具体如下： 1234# MathJax Supportmathjax: enable: true per_page: true 别着急，这样还不够，还需要在文章的Front-matter里打开mathjax开关，如下： --- title: index.html date: 2016-12-28 21:01:30 tags: mathjax: true -- 不要嫌麻烦，之所以要在文章头里设置开关，是因为考虑只有在用到公式的页面才加载 Mathjax，这样不需要渲染数学公式的页面的访问速度就不会受到影响了。 显示文章阅读数量另外：显示文章阅读量， 服务主要用了LeanCloud服务提供商 官方主题设置我使用的是Next主题， Make the theme more beautiful, recommended 关于评论系统多说已死，评论系统转到了Disqus，但是被墙的事实让人感觉不爽。几经周折，从多说转到Disqus，然后在gitment和gitalk之间徘徊，最后还是选择了valine，不过它只能在中国区进行评论，于是我还是保留了gitalk。然而，最后的最后我还是选择了Hypercomment。 2018年6月21日 更新 最近评论系统HyperComments竟然开始收费了，于是我不得不改用新的评论系统LiveRe Gitalk/valine。这样一来，原来的评论都看不到了，由此给大家带来的不便，特此道歉！ Hello everyone, the comment system HyperComments is charging recently, so I had to switch to the new comment system LiveRe. As a result, the original comments are invisible. I deeply apologize for this inconveniences! 关于旋转头像把侧边栏头像变成圆形&amp;鼠标停留在上面出现旋转效果，具体修改文件的位置是next\source\css\_common\components\sidebar\sidebar-author.styl。更为具体的修改过程见Ehlxr写的这篇博客。12345678910111213141516171819202122232425262728293031.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; /* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/ -webkit-animation: play 2s ease-out 1s 1; -moz-animation: play 2s ease-out 1s 1; animation: play 2s ease-out 1s 1; /* 鼠标经过头像旋转360度 */ -webkit-transition: -webkit-transform 1.5s ease-out; -moz-transition: -moz-transform 1.5s ease-out; transition: transform 1.5s ease-out;&#125;img:hover &#123; /* 鼠标经过头像旋转360度 */ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);&#125; 背景颜色设置其实NEXT主题已经自带了几种动画了，我用的是three_waves；但是呢，存在一个问题就是因为Blog背景是透明的，这样文字和背景动画就有重叠效果了，很不方便阅读，这时把背景色设置为白色即可。添加background: white到如下路径next\source\css\_schemes\Muse\_layout.styl1234.header-inner, .container .main-inner, .footer-inner &#123; background: white; +mobile() &#123; width: auto; &#125;&#125; 页面宽度设置有时候我们可能会嫌弃博客的页面太小，留白过大。这时候可以对页面宽度进行设置，可以参考Hexo Next主题 Issue #759。对于 Pisces Scheme，需要同时修改 header 的宽度、.main-inner 的宽度以及 .content-wrap 的宽度。例如，使用百分比（Pisces 的布局定义在 source/css/_schemes/Picses/_layout.styl 中）： 123.header&#123; width: 80%; &#125; /* 80% */.container .main-inner &#123; width: 80%; &#125; /* 80% */.content-wrap &#123; width: calc(100% - 260px); &#125; 优化友情链接新建一个Friends页面：1hexo new page Friends 新建样式，进入themes\next\source\css\_custom\custom.styl，在最后新加上几行代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061$shadowColor = #333;$themeColor = #222;$link-image-size = 180px;.link-body&#123; ul&#123; display: flex; justify-content: space-between; align-items: center; flex-wrap: wrap; margin: 0; padding: 0; .link&#123; max-width: $link-image-size; min-width: $link-image-size; max-height: $link-image-size; min-height: $link-image-size; position: relative; box-shadow: 0 0 1px $shadowColor; magin: 6px; width: 20%; list-style: none!important; overflow: hidden; border-radius: 6px; img&#123; object-fit: cover; transition: transform .6s ease-out; vertical-align: middle; border-bottom: 4px solid #eee;//#e5642b; transition: 0.4s ; width: 100%; border-radius: 90px 90px 90px 90px; display: inline-block; float: none; vertical-align: middle; &#125; .link-name&#123; position: absolute; bottom: 0; width: 100%; color: #666; text-align: center; text-shadow: 0 0 1px rgba(0,0,0,.4); background: rgba(255,255,255,.7); &#125; &amp;:hover&#123; img&#123; overflow: hidden; //transition: 0.4s; border-radius: 0 0 0 0; &#125; .link-name&#123; color: $themeColor; text-shadow: 0 0 1px $themeColor; &#125; &#125; &#125; &#125;&#125; 然后编辑站点的source\Friends下的index.md文件，内容如下： 12345678910111213141516&lt;div class="link-body"&gt; &lt;ul&gt; &lt;!--your friend begin--&gt; &lt;li class="link"&gt;&lt;a href="your_friends_link" title="balabala" target="_blank" &gt; &lt;img src= "image_path" alt="balabala"/&gt; &lt;span class="link-name"&gt; balabala&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;!--your friend end--&gt; &lt;!--your another friend begin--&gt; &lt;li class="link"&gt;&lt;a href="your_friends_link" title="balabala" target="_blank" &gt; &lt;img src= "image_path" alt="balabala"/&gt; &lt;span class="link-name"&gt; balabala&lt;/span&gt;&lt;/a&gt;&lt;/li&gt; &lt;!--your another friend end--&gt; ... &lt;/ul&gt;&lt;/div&gt; 此时，点击友情链接可能不会跳转到相应的页面，参考这个issue，作以下修改。 12345wrapImageWithFancyBox: function () &#123; $('.content img') .not('[hidden]') .not('.group-picture img, .post-gallery img') .not('a img') // 这里添加 增加Gitter参考sidecar的示例，在themes\next\layout\_layout.swig的&lt;/body&gt;前增加如下代码：1234567&lt;!-- add gitter on sidebar --&gt;&lt;script&gt; ((window.gitter = &#123;&#125;).chat = &#123;&#125;).options = &#123; room: 'your_chat_room_name' &#125;;&lt;/script&gt;&lt;script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async defer&gt;&lt;/script&gt; 其中的room换成你自己在gitter创建的聊天室名字，例如我的是vincentqin-blog-chat/Lobby，所以我的设置room: &#39;vincentqin-blog-chat/Lobby&#39;。之后可以在themes\next\source\css\_custom\custom.styl里增加如下设置：12345678910111213// adjust the position of gitter.gitter-open-chat-button &#123; right: 20px; padding: 10px; background-color: #777;&#125;@media (max-width: 600px) &#123; .gitter-open-chat-button, .gitter-chat-embed &#123; display: none; &#125;&#125; 添加页面背景将背景图片放在themes\next\source\images下，例如bg.jpg，然后themes\next\source\css\_custom\custom.styl里增加如下设置：123456789101112@media screen and (min-width:720px) &#123; body &#123; background:url(/images/bg.jpg); background-repeat: no-repeat; background-attachment:fixed; // 鼠标滚动背景不动 background-position:50% 50%; &#125; #footer &#123; color:#c8c8c8; &#125;&#125; ~这里是我custom.styl所有配置，仅供参考。~ 添加fork me on github文件位置：\themes\next\layout\_layout.swig，在&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;下一行添加如下代码。 1&lt;a href="https://www.github.com/Vincentqyw" class="github-corner" aria-label="View source on Github"&gt;&lt;svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"&gt;&lt;path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"&gt;&lt;/path&gt;&lt;path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"&gt;&lt;/path&gt;&lt;path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/a&gt;&lt;style&gt;.github-corner:hover .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125;@keyframes octocat-wave&#123;0%,100%&#123;transform:rotate(0)&#125;20%,60%&#123;transform:rotate(-25deg)&#125;40%,80%&#123;transform:rotate(10deg)&#125;&#125;@media (max-width:500px)&#123;.github-corner:hover .octo-arm&#123;animation:none&#125;.github-corner .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125;&#125;&lt;/style&gt; 更多样式，参考这里。 MarkDown编辑器推荐Haroopad 插入PDF文档以及图片 插入PDF文档：将相应的PDF文档放在与博客标题同名的文件夹内，然后再按照如下方式进行插入。 1[点我，这里是PDF文档](latex入门教程.pdf) 点我，这里是PDF文档 利用htmlimg标签嵌入图片 12&lt;img src= image_path alt="Lytro相机" width="100%"&gt;&lt;center&gt;Lytro&lt;/center&gt; 注意以上的image_path既可以是图床中的路径，亦可以把图片放在source/images/文件下，然后image_path=/images/logo.png，当然也可以如下插入图片，更加方便。 1![](/images/logo.png) 利用插件，以下我在Github上找到的别人已经做好的一个小工具。安装插件hexo-tag-asset-res，打开Git Shell, 在Hexo根目录下, 输入如下代码： 1$ npm install hexo-tag-asset-res --save 修改Hexo根目录下_config.yml文件：打开Hexo根目录, 找到站点配置文件_config.yml文件, 用任何一个文本编辑器打开, 找到如下代码： 1post_asset_folder: false 将false改成true即可。测试插入代码： 1&lt;center&gt;&#123;% asset_img Naruto.jpg Naruto%&#125;&lt;/center&gt; 效果如下： 配置个性化的字体在themes\next\source\css\_variables\custom.styl文件中添加如下内容。 12345678910// 修改成你期望的字体族$font-family-base = "Monda","Microsoft YaHei", Verdana, sans-serif// 标题，修改成你期望的字体族$font-family-headings = "Roboto Slab", Georgia, sans// 代码字体$code-font-family = "PT Mono", "Input Mono", Consolas, Monaco, Menlo, monospace// 博客字体$font-family-posts = "Monda"// logo字体$font-family-logo = "Lobster Two" 在博客中插入网易云音乐我们可以利用网易云提供的代码直接在markdown文档里面插入。 在网页上找到你想要播放的音乐，如下图： 点击生成外链播放器 注意自动播放，以及音乐播放器的大小可调。 在Markdown文档里插入如下代码 1&lt;center&gt;&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=500 height=86 src="http://music.163.com/outchain/player?type=2&amp;id=29722263&amp;auto=0&amp;height=66"&gt;&lt;/iframe&gt;&lt;/center&gt; 播放视频推荐使用Dplayer。首先在站点文件夹根目录安装Dplayer插件：1npm install hexo-tag-dplayer --save 然后文章中的写法： 1&#123;% dplayer url="https://******.mp4" "http://******.jpg" "api=https://api.prprpr.me/dplayer/" "id=" "loop=false" %&#125; 要使用弹幕，必须有api和id两项。id 的值自己随便取，唯一要求就是前面这点。可以通过这里获取id，保证每次都不一样。 献上Maddi Jane 翻唱的Jessie J的Price Tag。(function(){var player = new DPlayer({"container":document.getElementById("dplayer0"),"video":{"url":"http://oofx6tpf6.bkt.clouddn.com/Maddi-Jane-Price-Tag.mp4"},"danmaku":{"id":"bbe4286bf164ef6a1497f18a7b42ff944e684b82","api":"https://api.prprpr.me/dplayer/"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})() 同时部署接下来主要涉及到以hexo框架搭建博客的版本管理。同时部署其实很简单，仅仅修改站点配置文件的_config.yml即可。在最后的deploy底下新增一项： 123repo: github: https://github.com/Your_Github_ID/Github_ID.github.io.git coding: https://git.coding.net/Your_Coding_ID/Your_Repo_Name.coding.me.git 以后hexo d时，就会同时部署到github和coding。 版本管理方案 1（推荐）下载第三方插件，more information refers to this link hexo-git-backup. When you are well configured, you can just run the following command.1hexo backup #或 hexo b 方案 2这里涉及到git的部分知识。 目的：实现整个blog源码级别的代码管理，包括站点配置&amp;主题配置。 首先明确一点，在每次hexo d时，都会自动产生一个名为.deploy_git的文件夹，这个文件夹下包含有hexo g渲染出的各种文件，这些文件就是构成github page或者coding page的重要源码；同时会自动的将这个.deploy_git设置成本地仓库，即产生一个.git的隐藏文件。我们做的事情和以上过程不尽一致，总结起来主要是以下几个命令。首先建立一个名为.gitignore的文件，表示我们并不上传这些文件，原因后续介绍。其内容为： 12345678.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/themes/ 接下来就是把blog的源文件夹搞成一个本地仓库，如下命令。 12345678910111213141516171819# 创建仓库git init# 为本地仓库添加文件git add -A# 提交到本地仓库git commit -m "your message"# 添加一个名为 origin 的远程，这个名字随便起git remote add origin https://github.com/Your_Github_ID/Your_Repo_Name.git# 为其添加 push 到 Github 的地址git remote set-url --add --push origin https://github.com/Your_Github_ID/Your_Repo_Name.git# 为其添加 push 到 Coding 的地址git remote set-url --add --push origin https://git.coding.net/Your_Coding_ID/Your_Repo_Name.git# push到远端的master分支git push --set-upstream origin master# 新建并切换分支git checkout -b "another-branch"# 各种更改文件......推送到远程git push --set-upstream origin another-branch# 以后可以直接 git push，不用set了。 通过以上命令，我们就可以同时部署在github仓库https://github.com/Your_Github_ID/Your_Repo_Name.git和coding仓库https://git.coding.net/Your_Coding_ID/Your_Repo_Name.git了。 设置主题远程仓库这时你会发现themes这个文件夹并没有同时被上传到远程仓库，同上操作，将theme/next设置成本地仓库并部署。之所以将这个仓库单独上传，是为了方便切换主题，以及主题升级。 设置node_modules远程仓库之所以将这个模块单独拎出来处理，是因为这个文件夹虽然容量不大，但是其中文件个数特别多。当和blog源文件一同被git add到暂存区之后，git shell的运行速度就会超慢。我的解决思路就是将其创建成一个仓库，这样git shell的速度就会快一些。具体步骤不再赘述，同上。 结语经过建立以上的3个仓库，实现了blog源码级别的版本管理。当然，如果你不想暴露自己的源码，那么你只需要在coding申请一个私有仓库并部署就ok了。虽然看起来有些麻烦，但是一旦配置完毕之后，我们就只需要以下几个步骤就可以实现管理。12345hexo clean # 不是必要步骤hexo d -g # 渲染+部署到github page以及coding pagegit add . # 添加到暂存区git commit -m "your message" # push到本地仓库git push # 上传到远程仓库（站点目录、next主题目录、node_modules目录） Good luck:) 所有配置集锦最后附上我的全部配置。文件位置：themes\next\source\css\_custom\custom.styl。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485// Custom styles.//修改文章内code样式code &#123;color:#c7254e;background:#f9f2f4;border:1px solid #d6d6d6;&#125;//修改文章中图片样式，改为居中.posts-expand .post-body img &#123; margin: 0 auto;&#125;// 更改文中链接的颜色.post-body p a &#123; color: $orange; text-decoration: none; border-bottom: 1; &amp;:hover &#123; color: $blue; //text-decoration: underline; &#125;&#125;// 增大post之间的margin.post &#123; margin-bottom: 30px; //padding: 45px 36px 36px 36px; //box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgba(255, 255, 255,0.8);&#125;// delete the border of image.posts-expand .post-body img &#123; border: none; padding: 0px;&#125;// [Read More]按钮样式: 黑底绿字.post-button .btn:hover &#123; color: rgb(136, 255, 13) !important; background-color: rgba(0, 0, 0, 0.75); //black&#125;// 页面底部页码.pagination .page-number.current &#123; border-radius: 100%; background-color: rgba(100, 100, 100, 0.75);&#125;// 页面底部页码, 去除鼠标置于上方时，数字上方的线.pagination .prev, .pagination .next, .pagination .page-number &#123; margin-bottom: 10px; border: none; color: rgb(1, 1, 1);&#125;// 页面底部页码，鼠标置于上方，黑底绿字.page-number:hover,.page-number:active&#123; color: rgb(136, 255, 13); border-radius: 100%; //background-color: rgba(255, 0, 100, 0.75); //品红 background-color: rgba(0, 0, 0, 0.75); //black&#125;.pagination .space &#123; color: rgb(0, 0, 0);&#125;.pagination &#123; border: none; margin: 0px;&#125;// 已运行时间#days &#123; display: block; color: rgba(0, 0, 0,0.75); font-size: 13px; margin-top: 15px;&#125;// 自定义页脚跳动的心样式@keyframes heartAnimate &#123; 0%,100%&#123;transform:scale(1);&#125; 10%,30%&#123;transform:scale(0.9);&#125; 20%,40%,60%,80%&#123;transform:scale(1.1);&#125; 50%,70%&#123;transform:scale(1.1);&#125;&#125;#heart &#123; animation: heartAnimate 1.0s ease-in-out infinite;&#125;.with-love &#123; color: rgb(236, 25, 27);&#125;// 自定义的文章置顶样式.post-sticky-flag &#123; font-size: inherit; float: right; color: rgb(0, 0, 0); cursor: help; transition-duration: 0.2s; transition-timing-function: ease-in-out; transition-delay: 0s;&#125;.post-sticky-flag:hover &#123; color: #07b39b;&#125;// 右下角返回顶部按钮样式.back-to-top:hover &#123; color: rgb(136, 255, 13); background-color: rgba(0, 0, 0, 0.75); //black&#125;// 下载样式a#download &#123; display: inline-block; padding: 0 10px; color: #000; background: transparent; border: 2px solid #000; border-radius: 2px; transition: all .5s ease; font-weight: bold; &amp;:hover &#123; background: #000; color: #fff; &#125;&#125;// 颜色块-黄span#inline-yellow &#123; display:inline;// padding:.2em .6em .3em; padding:.1em .4em .1em; font-size:90%; font-weight:bold; line-height:1; color:#fff; text-align:center; white-space:nowrap; vertical-align:baseline; border-radius:0; background-color: #f0ad4e;&#125;// 颜色块-绿span#inline-green &#123; display:inline;// padding:.2em .6em .3em; padding:.1em .4em .1em; font-size:90%; font-weight:bold; line-height:1; color:#fff; text-align:center; white-space:nowrap; vertical-align:baseline; border-radius:0; background-color: #5cb85c;&#125;// 颜色块-蓝span#inline-blue &#123; display:inline;// padding:.2em .6em .3em; padding:.1em .4em .1em; font-size:90%; font-weight:bold; line-height:1; color:#fff; text-align:center; white-space:nowrap; vertical-align:baseline; border-radius:0; background-color: #2780e3;&#125;// 颜色块-紫span#inline-purple &#123; display:inline;// padding:.1em .2em .1em; padding:.1em .4em .1em; font-size:90%; font-weight:bold; line-height:1; color:#fff; text-align:center; white-space:nowrap; vertical-align:baseline; border-radius:0; background-color: #9954bb;&#125;// 颜色块-红span#inline-red &#123; display:inline;// padding:.2em .6em .3em; padding:.1em .4em .1em; font-size:90%; font-weight:bold; line-height:1; color:#fff; text-align:center; white-space:nowrap; vertical-align:baseline; border-radius:0; background-color: #df3e3e;&#125;// 左侧边框红色块级p#div-border-left-red &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-left-width: 5px; border-radius: 3px; border-left-color: #df3e3e;&#125;// 左侧边框黄色块级p#div-border-left-yellow &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-left-width: 5px; border-radius: 3px; border-left-color: #f0ad4e;&#125;// 左侧边框绿色块级p#div-border-left-green &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-left-width: 5px; border-radius: 3px; border-left-color: #5cb85c;&#125;// 左侧边框蓝色块级p#div-border-left-blue &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-left-width: 5px; border-radius: 3px; border-left-color: #2780e3;&#125;// 左侧边框紫色块级p#div-border-left-purple &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-left-width: 5px; border-radius: 3px; border-left-color: #9954bb;&#125;// 右侧边框红色块级p#div-border-right-red &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-right-width: 5px; border-radius: 3px; border-right-color: #df3e3e;&#125;// 右侧边框黄色块级p#div-border-right-yellow &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-right-width: 5px; border-radius: 3px; border-right-color: #f0ad4e;&#125;// 右侧边框绿色块级p#div-border-right-green &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-right-width: 5px; border-radius: 3px; border-right-color: #5cb85c;&#125;// 右侧边框蓝色块级p#div-border-right-blue &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-right-width: 5px; border-radius: 3px; border-right-color: #2780e3;&#125;// 右侧边框紫色块级p#div-border-right-purple &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-right-width: 5px; border-radius: 3px; border-right-color: #9954bb;&#125;// 上侧边框红色p#div-border-top-red &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-top-width: 5px; border-radius: 3px; border-top-color: #df3e3e;&#125;// 上侧边框黄色p#div-border-top-yellow &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-top-width: 5px; border-radius: 3px; border-top-color: #f0ad4e;&#125;// 上侧边框绿色p#div-border-top-green &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-top-width: 5px; border-radius: 3px; border-top-color: #5cb85c;&#125;// 上侧边框蓝色p#div-border-top-blue &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-top-width: 5px; border-radius: 3px; border-top-color: #2780e3;&#125;// 上侧边框紫色p#div-border-top-purple &#123; display: block; padding: 10px; margin: 10px 0; border: 1px solid #ccc; border-top-width: 5px; border-radius: 3px; border-top-color: #9954bb;&#125;// gitalk config..gitalk_title &#123; display: inline-block; padding: 0 15px; color: #0a9caf; border: 1px solid #0a9caf; border-radius: 4px; cursor: pointer; font-size: 14px; // float: left;&#125;.gitalk_title:hover &#123; color: #fff; background: #0a9caf;&#125;.gitalk_container &#123; margin-bottom: 50px; border-bottom: 1px solid #e9e9e9;&#125;$shadowColor = #333$themeColor = #222$link-image-size-width = 180px;$link-image-size-height = 230px;.link-body&#123; ul&#123; display: flex; justify-content: space-between; align-items: center; flex-wrap: wrap; margin: 0; padding: 0; .link&#123; max-width: $link-image-size-width; min-width: $link-image-size-width; max-height: $link-image-size-height; min-height: $link-image-size-height; position: relative; box-shadow: 0 0 1px $shadowColor; magin: 6px; width: 20%; list-style: none!important; overflow: hidden; border-radius: 6px; img&#123; object-fit: cover; transition: transform .6s ease-out; vertical-align: middle; border-bottom: 4px solid #eee;//#e5642b; transition: 0.4s ; width: 100%; border-radius: 90px 90px 90px 90px; display: inline-block; float: none; vertical-align: middle; &#125; .link-name&#123; position: absolute; bottom: 53px; width: 100%; color: #666; text-align: center; //text-shadow: 0 0 1px rgba(0,0,0,.4); background: rgba(255,255,255,.8); &#125; .link-name-below&#123; position: absolute; bottom: 0; font-size: 13px; font-weight: 300; margin: 0 0 15px; line-height: 13px; width: 100%; color: #666; text-align: center; //text-shadow: 0 0 1px rgba(0,0,0,.4); //background: rgba(255,255,255,.7); &#125; &amp;:hover&#123; img&#123; overflow: hidden; //transition: 0.4s; border-radius: 0 0 0 0; &#125; .link-name&#123; color: $themeColor; font-weight: bold; text-align: center; //text-shadow: 0 0 1px $themeColor; &#125; &#125; &#125; &#125;&#125;// adjust the position of gitter.gitter-open-chat-button &#123; right: 20px; padding: 10px; background-color: rgba(45,45,45,0.80); color: rgba(255,255,255,0.75)&#125;@media (max-width: 600px) &#123; .gitter-open-chat-button, .gitter-chat-embed &#123; display: none; &#125;&#125;@media screen and (min-width:1200px) &#123; body &#123; background:url(/images/50.jpg); background-repeat: no-repeat; background-attachment:fixed; background-position:50% 50%; &#125; #footer &#123; color:#c8c8c8; &#125;&#125;#coding:link,#coding:visited&#123; color: rgb(153,153,153); //font-weight: normal; text-decoration: none;&#125;#coding:hover,#coding:active&#123; color: rgb(153,153,153); text-decoration: none;&#125; 参考主题优化]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[AR形势与应用]]></title>
      <url>%2Fposts%2FAR-applications%2F</url>
      <content type="text"><![CDATA[前言当前，微软、谷歌、苹果、Facebook 等 IT 巨头都在布局虚拟现实Virtual Reality简称 VR）虚拟现实也许是下一个颠覆人类生活的新技术之一。增强现实Augmented Reality简称 AR）是虚拟现实技术的延伸它可以用来模拟对象让学习者在现实环境背景中看到虚拟生成的模型对象 而且这一模型可以快速生成、操纵和旋转。 1．增强现实公司分类Augmented reality增强现实创业的公司大致可以分成如下几种公司类型[1]。（以下是翻译的国外网站的内容） AR平台公司（AR platform companies）。这些公司为开发者提供底层的开发工具，以便于开发者能够创造更多更加高级的AR解决方案。这样的公司有：Qualcomm Vuforia, METAIO’s SDK, TotalImmersion。 AR产品和游戏公司。这些公司主营自己独家AR零售产品例如：书籍、游戏。包括如下公司：Sphero, POPAR, Sony, Microsoft 以及 Nintendo。 自助DIY AR公司以及通用AR查看器。这些公司专为快速简单的AR体验或活动而设计，提供内容管理工具和基本AR效果菜单。借助自助AR工具，精通技术的个人可以创建简单的体验，例如发布单个视频或简单的动画。AR自助服务公司非常适合发布商，教育工作者，学生和其他想要测试或创建简单的增强现实体验的用户，而无需投资于完全定制的品牌应用体验。一些DIY公司还提供AR查看器，定制服务和白标签选项。这个领域的公司包括Layar，Aurasma，DAQRI和Zappar。 定制品牌应用开发公司。这些公司直接与品牌营销人员和机构合作，主要为的广告活动、贸易展览和现场活动构建定制的增强现实解决方案。自定义品牌应用程序允许营销人员结合独一无二的定制增强现实体验与个性化服务和项目管理。自定义功能通常包括品牌规格，导航，用户界面，动画，复杂或大规模的AR效果等。服务可以包括3D建模，与其他软件服务或电子商务平台的集成，游戏开发，基于位置的安装，通知，复杂动画，微位置或其他高级AR效果。这个领域的公司包括Appshaker，GravityJack和Marxent。 行业特定的垂直AR解决方案（Industry-specific vertical AR solutions）。最新出现的AR公司类别是那些提供AR解决方案的专业服务公司，专为服务于专业领域。如奢侈品零售，医疗服务，工业应用，制药公司和化妆品公司。这个领域的公司包括用于广告的Blippar，用于豪华珠宝零售的Holition，用于家具布置的Adornably以及用于消费零售，工业和企业销售工具的Marxent的VisualCommerce®。 做个不恰当的比喻，我觉得AR是VR的一个延伸，只是把VR的场景换成了现实场景，眼镜换成了透明的。 2．可行性分析一个良好的AR体验，大致可以分成一下几个方面： 1． 3D眼镜；这应该是VR或者AR最为重要的一部分； 2． 手柄；可以代替人手去操纵，在一定程度上增加了使用的灵活性。（但我觉得未来手柄一定会被淘汰，因为这只是人手不能被充分利用的代替手段）。 3． 3D显示屏幕，它可以跟踪用户的头的转动和手的动作，实时调整所看到的3D图像，并允许用户操控一些虚拟物体，就好比他们真正存在。 4． 待补充 技术难点VR得益于三维游戏的发展，而AR收益于影视领域的跟踪技术（video tracking）的发展。从技术门槛的角度来说，VR、AR和移动端重合的技术有：显示器、运动传感器、处理器、储存、记忆、无线连接等。在硬件上，这些都不是技术难点。VR、AR的难点都在感知和显示，感知是一种映射，VR 映射的是一个lighthouse的空间或者PS camera mapping的一个交叉；在显示上，VR如何精准地匹配用户的头部产生相应的画面，AR则在这基础上算出光照、遮挡等情况并让图像通透不干扰现实中的视线。而VR硬件的难点在于光学的镜片技术和位置追踪技术（SLAM），因为以前的移动端不涉及这些技术。AR的软件难点在于：1、定位相机；2、恢复场景的三维结构。通常情况下，这一技术被称作SLAM（Simultaneous Localization And Mapping）。当然还有一些其他的技术诸如：图像追踪、云端视觉搜索、人脸和表情追踪等。 目前国内外已经有多家技术公司提供了软件开发方面的AR解决方案和工具，使得全球众多开发者参与到AR应用开发中来。开发者不需要自己搭建系统架构，也不用理解底层SDK复杂的实现方式，只需要将AR模块嵌入到已有的业务逻辑中，就可以通过现成的开源代码或者平台工具，设计并开发属于自己的AR软件产品。就这个层面来讲技术是可以实现的，但就某一个特殊领域的实现方式可能有所差别。 技术支持Metaio 是由德国大众的一个项目衍生出来的一家虚拟现实初创公司，现已被苹果公司收购。 专门从事增强现实和机器视觉解决方案，产品主要包括Metaio SDK 和 Metaio Creator。 Metaio SDK 支持移动设备的AR应用开发，它在内部提供增强现实显示组件ARView，该组件将摄像机层、3D 空间计算以及POI信息的叠加等功能全部封装在一起，用户在使用增强现实功能时，只需要关注用户操作的监听器即可，摄像机层、3D 空间计算、图形识别以及空间信息叠加等逻辑，完全由ARView组件自己处理 。Metaio Creator相对Metaio SDK 来说，使用门槛更低，用户无需掌握移动开发技术，就可以通过 Metaio Creator 用户图形接口中简单的点击、拖拽、拉伸等方式，控制软件中组件的功能，以构建出自己的增强现实结果。（但是被苹果收购了，目前不提供服务） Wikitude 是由美国 Mobilizy 公司于 2008 年秋推出的一款移动增强现实开发平台， 支持 Android、 iOS、Black Berry 以及 Windows Phone 多个手机智能操作系统Wikitude SDK 是一款优秀的增强现实开发工具包， 它能够帮助开发人员减小增强现实应用程序开发的复杂性。 目前，Wikitude SDK 支持载入真实的物理环境向 AR 环境中添加虚拟物体、支持用户与虚拟物体的交互、响应用户的位置变化、AR 环境中信息提示、从本地或网络加载资源等功能。 ENTiTi Creator是由以色列一家创业公司 Waking App 开发的一款 AR 作品制作工具，易学易用是它的最大特色。用户可以使用ENTiTi平台上传图片和视频以及相应的动作指令， 并通过简单的逻辑串联，就可以轻松创建出包含3D图像、动画或者游戏的AR/VR 内容。该平台不需要任何编程、完全依靠鼠标拖放就能完成整个创建过程。EN-TiTi是基于云计算的平台，可以在线 3D 视角查看内容，并自动适配各种终端，比如，手机或平台电脑、三星 Gear VR 盒子、Vuzix 智能眼镜等。开发者通过它所发布出来的AR内容，只需要通过一个叫作 EN-TiTi View 软件的入口，就可以轻松访问。 这意味着全球所有开发者所开发出的成千上万的 AR 内容，只需要一个软件即可全部浏览。 Realmax公司是一个国际化AR生态级企业，在上海、香港、纽约、慕尼黑都设立了分公司，并建立了5个全球实验室，完成了硬件量产、软件算法、应用开发和内容制作的AR技术储备，AR操作系统“Realcast”也有可观的用户量，在工业、幼教、电商、旅游等领域积累了大量客户，是AR领域唯一的一家完成“平台+内容+终端+应用”生态链布局的企业。 根据以上分析，如果想完成某一个特定的AR或者VR应用，可以使用上述公司提供的SDK，在一定程度上会加快开发速度。 参考文献1． 增强现实公司类型：http://www.marxentlabs.com/augmented-reality-company-primer-5-types-augmented-reality-companies/2． Metaio公司主页：http://www.metaio.eu/3． AR技术举例以及现有公司介绍：http://www.marxentlabs.com/what-is-virtual-reality-definition-and-examples/。4． Layer公司AR开发SDK：https://www.layar.com/solutions/#sdk5． Wikitude官网：http://www.metaio.eu/index.html6． ENTiTi Creator 官网：http://www.wakingapp.com/7． Realmax公司官网：http://www.realmax.com/或者http://www.realmax.com.hk/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Markdown 学习]]></title>
      <url>%2Fposts%2Flearning-Markdown%2F</url>
      <content type="text"><![CDATA[本文涉及学习Markdown文本标记语言的一些练习笔记。 Note Tag 测试1234567891011121314/** * note.js | global hexo script. * * ATTENTION! No need to write this tag in 1 line if u don't want see probally bugs. * * Usage: * * &#123;% note [class] %&#125; * Any content (support inline tags too). * &#123;% endnote %&#125; * * [class] : default | primary | success | info | warning | danger. * May be not defined. */ Test note default昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。 Test note primary昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。 Test note success昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。 Test note info昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。 Test note warning昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。 Test note danger昏鴉盡，小立恨因誰 ?急雪乍翻香閣絮，輕風吹到膽瓶梅，心字已成灰。 Button 标签测试12Usage: &#123;% button /path/to/url/, text, icon [class], title %&#125;Alias: &#123;% btn /path/to/url/, text, icon [class], title %&#125; Button内嵌文字1&#123;% button #, Text %&#125;&#123;% button #插入不同颜色的字体, 插入不同颜色的字体,heart %&#125; Text插入不同颜色的字体 Button内嵌logo123&lt;div class="text-center"&gt;&lt;span&gt;&#123;% btn ##插入不同颜色的字体,, header %&#125;&#123;% btn #,, edge %&#125;&#123;% btn #,, times %&#125;&#123;% btn #,, circle-o %&#125;&lt;/span&gt;&lt;span&gt;&#123;% btn #,, italic %&#125;&#123;% btn #,, scribd %&#125;&lt;/span&gt;&lt;span&gt;&#123;% btn #,, google %&#125;&#123;% btn #,, chrome %&#125;&#123;% btn #,, opera %&#125;&#123;% btn #,, diamond fa-rotate-270 %&#125;&lt;/span&gt;&lt;/div&gt; 插入不同颜色的字体 Button Margin1&lt;div class="text-center"&gt;&#123;% btn #, Almost, adn fa-fw fa-lg %&#125; &#123;% btn #, Over, terminal fa-fw fa-lg %&#125;&lt;/div&gt; Almost Over 123&lt;div class="text-right"&gt;&#123;% btn #, Test is finished., check fa-fw fa-lg, Button tag test is finished. %&#125;&lt;/div&gt; Test is finished. Label Tag测试文中字体颜色1234567891011121314From &#123;% label @fairest creatures %&#125; we desire increase,That thereby &#123;% label primary@beauty's rose %&#125; might never die,But as the &#123;% label success@riper %&#125; should by time decease,His tender heir might &#123;% label info@bear his memory %&#125;:But thou contracted to thine own bright eyes,Feed'st thy light's flame with *&#123;% label warning @self-substantial fuel%&#125;*,Making a famine where ~~&#123;% label default @abundance lies %&#125;~~,Thy self thy foe, to thy &lt;mark&gt;sweet self too cruel&lt;/mark&gt;:Thou that art now the world's fresh ornament,And only herald to the gaudy spring,Within thine own bud buriest thy content,And &#123;% label danger@tender churl mak'st waste in niggarding %&#125;:Pity the world, or else this glutton be,&#123;% label warning@To eat the world's due, by the grave and thee %&#125;. From fairest creatures we desire increase,That thereby beauty's rose might never die,But as the riper should by time decease,His tender heir might bear his memory:But thou contracted to thine own bright eyes,Feed’st thy light’s flame with self-substantial fuel,Making a famine where abundance lies,Thy self thy foe, to thy sweet self too cruel:Thou that art now the world’s fresh ornament,And only herald to the gaudy spring,Within thine own bud buriest thy content,And tender churl mak'st waste in niggarding:Pity the world, or else this glutton be,To eat the world's due, by the grave and thee. 表格Tag测试1234567891011&#123;% tabs First unique name %&#125;&lt;!-- tab --&gt;**This is Tab 1.**&lt;!-- endtab --&gt;&lt;!-- tab --&gt;**This is Tab 2.**&lt;!-- endtab --&gt;&lt;!-- tab --&gt;**This is Tab 3.**&lt;!-- endtab --&gt;&#123;% endtabs %&#125; First unique name 1First unique name 2First unique name 3This is Tab 1. This is Tab 2. This is Tab 3. 插入不同颜色的字体1&lt;table&gt;&lt;tr&gt;&lt;td bgcolor=LimeGreen&gt;&lt;font color=white size=3&gt;我是白色的字体，背景是色的~&lt;/font&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt; 我是白色的字体，背景是色的~ 我是白色的字体，背景是深灰色的~ 我是白色的字体，背景是浅海绿的~ 我是白色的字体，背景是蓝色的~ 我是白色的字体，背景是银色的~ 我是白色的字体，背景是淡灰色的~ 我是白色的字体，背景是深灰色的~ 插入代码这里是代码区域 1234567891011121314151617181920212223242526% The following is the Matlab Code% I want to see the resultfunction demo()temp=zeros(5,6);for i=1:size(temp,1) for j=1:size(temp,2) temp(i,j)=rand(1); if temp(i,j)&gt;0.5 temp(i,j)=1; end endendreturn temp 插入标题1234567891011# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 列表123456- 文本1- 文本2- 文本31. 文本一2. 文本二3. 文本三 文本1 文本2 文本3 文本一 文本二 文本三 插入图像1![](http://pic1.win4000.com/wallpaper/0/54cae8e69ac8b.jpg) 或者：1&lt;center&gt;&lt;img src="http://i2.wp.com/posturemag.com/online/wp-content/uploads/2015/07/Kaz7.jpg" width="100%" &gt;&lt;/center&gt; 插入链接 segmentfault上的一个Markdown学习手册 有道云笔记的Markdown学习指南-基础篇 Git学习手册 插入公式1$$E=mc^2$$ E=mc^2Hexo文档使用Markdown语言对文档进行编辑，Hexo自身对公式可以进行渲染但是效果不佳，我们采用的是mathjax对Markdown中的公式进行渲染。首先修复Hexo与mathjax之间的渲染冲突，然后可以参考mathjax的说明文档编辑公式。 希腊字母对应表 字母名称 大写 markdown原文 小写 markdown原文 alpha $A$ A $\alpha$ \alpha beta $B$ B $\beta$ \beta gamma $\Gamma$ \Gamma $\gamma$ \gamma delta $\Delta$ \Delta $\delta$ \delta epsilon $E$ E $\epsilon$ \epsilon - - - $\varepsilon$ \varepsilon zeta $Z$ Z $\zeta$ \zeta eta $E$ E $\eta$ \eta theta $\Theta$ \Theta $\theta$ \theta iota $I$ I $\iota$ \iota kappa $K$ K $\kappa$ \kappa lambda $\Lambda$ \Lambda $\lambda$ \lambda Mu $M$ M $\mu$ \mu nu $N$ N $\nu$ \nu xi $\Xi$ \Xi $\xi$ \xi omicron $O$ O $\omicron$ \omicron pi $\Pi$ \Pi $\pi$ \pi rho $P$ P $\rho$ \rho sigma $\Sigma$ \Sigma $\sigma$ \sigma tau $T$ T $\tau$ \tau upsilon $\Upsilon$ \Upsilon $\upsilon$ \upsilon phi $\Phi$ \Phi $\phi$ \phi - - - $\varphi$ \varphi chi $X$ X $\chi$ \chi psi $\Psi$ \Psi $\psi$ \psi 参考 Hexo Theme Next Test Color map 一个关于Latex不短的介绍 Latex常用命令摘录]]></content>
    </entry>

    
  
  
</search>
